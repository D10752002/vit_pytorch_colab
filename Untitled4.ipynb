{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8uPeq4kcbmjoRfVQsR/be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c2f8fad19d140649f2a58f0200452ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b03a813bf94348d6b54bf0fc917c1c59",
              "IPY_MODEL_ddfd64923f504676afcce79e69ad79bf",
              "IPY_MODEL_97568a328ab542c592bf2b440c87f4ac"
            ],
            "layout": "IPY_MODEL_4f65511803054a92a1a4ce8509870e87"
          }
        },
        "b03a813bf94348d6b54bf0fc917c1c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581605d2071f4c91a0da72b82128c575",
            "placeholder": "​",
            "style": "IPY_MODEL_59df99d561294193ad7bd2b2a2b1850f",
            "value": "100%"
          }
        },
        "ddfd64923f504676afcce79e69ad79bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b6e03d371f4f26b9db12e661de1d61",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f1e957a42184616834267fdec0e8dfa",
            "value": 102530333
          }
        },
        "97568a328ab542c592bf2b440c87f4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_426b5f96806548b2848edb7f92ac65ee",
            "placeholder": "​",
            "style": "IPY_MODEL_b36705e0317b4b48bbf9a827e61f8490",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 168MB/s]"
          }
        },
        "4f65511803054a92a1a4ce8509870e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581605d2071f4c91a0da72b82128c575": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59df99d561294193ad7bd2b2a2b1850f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b6e03d371f4f26b9db12e661de1d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1e957a42184616834267fdec0e8dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "426b5f96806548b2848edb7f92ac65ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36705e0317b4b48bbf9a827e61f8490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D10752002/vit_pytorch_colab/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Eo0YwUudthWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccde2b5-7b34-40a4-f39b-e9ecdc0bb81a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUAqjadYkOM",
        "outputId": "826ccc4a-36fd-48ca-eee5-a8bcdaabd193"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 31.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 73.8 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import datasets\n",
        "import os\n",
        "import subprocess\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from packaging import version\n",
        "from typing import Optional, List\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops.boxes import box_area\n",
        "from PIL import Image\n",
        "import io\n",
        "import copy\n",
        "import torch.utils.data\n",
        "from pycocotools import mask as coco_mask\n",
        "import torchvision.transforms as T\n",
        "import sys\n",
        "from typing import Iterable\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "import contextlib\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import functools\n",
        "import traceback\n",
        "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "  from torchvision.ops import _new_empty_tensor\n",
        "  from torchvision.ops.misc import _output_size\n",
        "import PIL"
      ],
      "metadata": {
        "id": "zYFUt0znti2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_traceback(f):\n",
        "    @functools.wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print('Caught exception in worker thread:')\n",
        "            traceback.print_exc()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "class IdGenerator():\n",
        "    '''\n",
        "    The class is designed to generate unique IDs that have meaningful RGB encoding.\n",
        "    Given semantic category unique ID will be generated and its RGB encoding will\n",
        "    have color close to the predefined semantic category color.\n",
        "    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n",
        "    Class constructor takes dictionary {id: category_info}, where all semantic\n",
        "    class ids are presented and category_info record is a dict with fields\n",
        "    'isthing' and 'color'\n",
        "    '''\n",
        "    def __init__(self, categories):\n",
        "        self.taken_colors = set([0, 0, 0])\n",
        "        self.categories = categories\n",
        "        for category in self.categories.values():\n",
        "            if category['isthing'] == 0:\n",
        "                self.taken_colors.add(tuple(category['color']))\n",
        "\n",
        "    def get_color(self, cat_id):\n",
        "        def random_color(base, max_dist=30):\n",
        "            new_color = base + np.random.randint(low=-max_dist,\n",
        "                                                 high=max_dist+1,\n",
        "                                                 size=3)\n",
        "            return tuple(np.maximum(0, np.minimum(255, new_color)))\n",
        "\n",
        "        category = self.categories[cat_id]\n",
        "        if category['isthing'] == 0:\n",
        "            return category['color']\n",
        "        base_color_array = category['color']\n",
        "        base_color = tuple(base_color_array)\n",
        "        if base_color not in self.taken_colors:\n",
        "            self.taken_colors.add(base_color)\n",
        "            return base_color\n",
        "        else:\n",
        "            while True:\n",
        "                color = random_color(base_color_array)\n",
        "                if color not in self.taken_colors:\n",
        "                    self.taken_colors.add(color)\n",
        "                    return color\n",
        "\n",
        "    def get_id(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color)\n",
        "\n",
        "    def get_id_and_color(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color), color\n",
        "\n",
        "\n",
        "def rgb2id(color):\n",
        "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
        "        if color.dtype == np.uint8:\n",
        "            color = color.astype(np.int32)\n",
        "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
        "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n",
        "\n",
        "\n",
        "def id2rgb(id_map):\n",
        "    if isinstance(id_map, np.ndarray):\n",
        "        id_map_copy = id_map.copy()\n",
        "        rgb_shape = tuple(list(id_map.shape) + [3])\n",
        "        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n",
        "        for i in range(3):\n",
        "            rgb_map[..., i] = id_map_copy % 256\n",
        "            id_map_copy //= 256\n",
        "        return rgb_map\n",
        "    color = []\n",
        "    for _ in range(3):\n",
        "        color.append(id_map % 256)\n",
        "        id_map //= 256\n",
        "    return color\n",
        "\n",
        "\n",
        "def save_json(d, file):\n",
        "    with open(file, 'w') as f:\n",
        "        json.dump(d, f)"
      ],
      "metadata": {
        "id": "LEYJ4qCBdBbr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/datadetr.zip' -d '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUq3ABZzXu8-",
        "outputId": "cdd08919-5dfe-47e1-8407-d2190aa553c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datadetr.zip\n",
            "   creating: /content/data/annotations/\n",
            "  inflating: /content/data/annotations/instances_train.coco.json  \n",
            "  inflating: /content/data/annotations/instances_validation.coco.json  \n",
            "   creating: /content/data/train/\n",
            "  inflating: /content/data/train/aachen_000000_000019_leftImg8bit_png.rf.582961c9887d9dae3c8dd1610453db5f.jpg  \n",
            "  inflating: /content/data/train/aachen_000001_000019_leftImg8bit_png.rf.f91aa5821ab0425ddcbcd47a0cac899e.jpg  \n",
            "  inflating: /content/data/train/aachen_000004_000019_leftImg8bit_png.rf.142c5891e1df86382211343550db7023.jpg  \n",
            "  inflating: /content/data/train/aachen_000005_000019_leftImg8bit_png.rf.de26ff2bd0d7e188be5c08b6d989c2a2.jpg  \n",
            "  inflating: /content/data/train/aachen_000006_000019_leftImg8bit_png.rf.4fb5925d88109cf20d3504bbe9468322.jpg  \n",
            "  inflating: /content/data/train/aachen_000008_000019_leftImg8bit_png.rf.cad9c3ec56eded4864bdd989ace140f6.jpg  \n",
            "  inflating: /content/data/train/aachen_000009_000019_leftImg8bit_png.rf.a7e527f04f1d1c5facabd83b914a95ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000010_000019_leftImg8bit_png.rf.675f7c3c4a4f3cbe26c09e4b412cef5a.jpg  \n",
            "  inflating: /content/data/train/aachen_000011_000019_leftImg8bit_png.rf.dd610a26bd9e6be7f93f12f43aeb36aa.jpg  \n",
            "  inflating: /content/data/train/aachen_000013_000019_leftImg8bit_png.rf.5fc98df03ca46477aa96f62cface6fba.jpg  \n",
            "  inflating: /content/data/train/aachen_000016_000019_leftImg8bit_png.rf.2e35b5dad96ada244066499b12f2f6eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000018_000019_leftImg8bit_png.rf.77d20ae3a4d0e48af549a191bd33bad9.jpg  \n",
            "  inflating: /content/data/train/aachen_000020_000019_leftImg8bit_png.rf.6a9e74e6c1a5e5c677d3ca11d3d52f69.jpg  \n",
            "  inflating: /content/data/train/aachen_000021_000019_leftImg8bit_png.rf.0b389f3e83a0d9f0ce2f0d7fb273f034.jpg  \n",
            "  inflating: /content/data/train/aachen_000022_000019_leftImg8bit_png.rf.6a16d20bca3574c8277828adf4836134.jpg  \n",
            "  inflating: /content/data/train/aachen_000023_000019_leftImg8bit_png.rf.b6b31e1fa5ea04e9cf13e9a472b6f827.jpg  \n",
            "  inflating: /content/data/train/aachen_000024_000019_leftImg8bit_png.rf.8009b3386285391e44c5a21fd3e02eaa.jpg  \n",
            "  inflating: /content/data/train/aachen_000025_000019_leftImg8bit_png.rf.e687bdc864d15d67b649837c417d9a59.jpg  \n",
            "  inflating: /content/data/train/aachen_000026_000019_leftImg8bit_png.rf.8748f8bc57ee4d152e7fa5eaab0cd5d7.jpg  \n",
            "  inflating: /content/data/train/aachen_000027_000019_leftImg8bit_png.rf.1181733e113db61e61276b22526b3fc1.jpg  \n",
            "  inflating: /content/data/train/aachen_000028_000019_leftImg8bit_png.rf.ea4a138db348d6603161fea627a5278e.jpg  \n",
            "  inflating: /content/data/train/aachen_000030_000019_leftImg8bit_png.rf.65655c6374cb57a6675ecff27c9d6f35.jpg  \n",
            "  inflating: /content/data/train/aachen_000031_000019_leftImg8bit_png.rf.e2db2694efa79a571dfa721abb9fea52.jpg  \n",
            "  inflating: /content/data/train/aachen_000033_000019_leftImg8bit_png.rf.d205d12321923fc632b7ab25098fcad2.jpg  \n",
            "  inflating: /content/data/train/aachen_000034_000019_leftImg8bit_png.rf.f8c5d9982106ca613017c72024909a67.jpg  \n",
            "  inflating: /content/data/train/aachen_000036_000019_leftImg8bit_png.rf.db334a2a24a2faee8a0d665f3871c067.jpg  \n",
            "  inflating: /content/data/train/aachen_000037_000019_leftImg8bit_png.rf.72979012452111fba093cbc6582a753b.jpg  \n",
            "  inflating: /content/data/train/aachen_000038_000019_leftImg8bit_png.rf.d1361a986c4e094e54c934a5d6e96320.jpg  \n",
            "  inflating: /content/data/train/aachen_000039_000019_leftImg8bit_png.rf.0d7e6af1d670616da2dca51e52060f88.jpg  \n",
            "  inflating: /content/data/train/aachen_000042_000019_leftImg8bit_png.rf.8d4bbff500a1a12f8c726be8e961e564.jpg  \n",
            "  inflating: /content/data/train/aachen_000043_000019_leftImg8bit_png.rf.b2cd43d90e05ce870ca9baa3096e547e.jpg  \n",
            "  inflating: /content/data/train/aachen_000044_000019_leftImg8bit_png.rf.00fa4c7ce5b1518bef7a82301cdd6a05.jpg  \n",
            "  inflating: /content/data/train/aachen_000045_000019_leftImg8bit_png.rf.1e16d81e47f37dbd285a6eb7e448abc7.jpg  \n",
            "  inflating: /content/data/train/aachen_000046_000019_leftImg8bit_png.rf.2f2ec43d41e99f166c13ea0c1e0331d8.jpg  \n",
            "  inflating: /content/data/train/aachen_000047_000019_leftImg8bit_png.rf.17ce1c4c3dbc025982fa3cda51648a16.jpg  \n",
            "  inflating: /content/data/train/aachen_000050_000019_leftImg8bit_png.rf.f4709323a3f6dde6fc747d50054cc598.jpg  \n",
            "  inflating: /content/data/train/aachen_000051_000019_leftImg8bit_png.rf.547ab162758d1e43cd969afcecc6ec34.jpg  \n",
            "  inflating: /content/data/train/aachen_000052_000019_leftImg8bit_png.rf.d02bb848e044d2b2527c62637bc43e9f.jpg  \n",
            "  inflating: /content/data/train/aachen_000054_000019_leftImg8bit_png.rf.4d8a3479da146b6bed4d6c909234eed0.jpg  \n",
            "  inflating: /content/data/train/aachen_000055_000019_leftImg8bit_png.rf.bb998259571bbcbae4f82e08e3734e12.jpg  \n",
            "  inflating: /content/data/train/aachen_000057_000019_leftImg8bit_png.rf.13cfcba5e93781422271386003839a96.jpg  \n",
            "  inflating: /content/data/train/aachen_000058_000019_leftImg8bit_png.rf.d84b7b3b2df9e69f58cae9c4d5b360fd.jpg  \n",
            "  inflating: /content/data/train/aachen_000060_000019_leftImg8bit_png.rf.eff631cc3a3c23866ed331de3b3ffb3c.jpg  \n",
            "  inflating: /content/data/train/aachen_000063_000019_leftImg8bit_png.rf.6337ee59c0f594739be7b3eec4efc5e5.jpg  \n",
            "  inflating: /content/data/train/aachen_000065_000019_leftImg8bit_png.rf.266a2b60b1c86eb35e705bbccc7d4259.jpg  \n",
            "  inflating: /content/data/train/aachen_000067_000019_leftImg8bit_png.rf.2f948caed84ba80f408cd7bd033f1b73.jpg  \n",
            "  inflating: /content/data/train/aachen_000068_000019_leftImg8bit_png.rf.9280201c560cb90cb5a88da3e1a791ff.jpg  \n",
            "  inflating: /content/data/train/aachen_000069_000019_leftImg8bit_png.rf.d874145a3fba381eb78a867c54724ac5.jpg  \n",
            "  inflating: /content/data/train/aachen_000072_000019_leftImg8bit_png.rf.1530c551e18dae775c81c0f1286a8c64.jpg  \n",
            "  inflating: /content/data/train/aachen_000073_000019_leftImg8bit_png.rf.9a071eeb371fa4e82041e1279c448a02.jpg  \n",
            "  inflating: /content/data/train/aachen_000075_000019_leftImg8bit_png.rf.73cd658733f76de10456bcc0a0947a82.jpg  \n",
            "  inflating: /content/data/train/aachen_000076_000019_leftImg8bit_png.rf.6cf14dfe9991bec232596e2bf39ef568.jpg  \n",
            "  inflating: /content/data/train/aachen_000077_000019_leftImg8bit_png.rf.0aeda9d9faa9281cda0c0a6e9fd6b97d.jpg  \n",
            "  inflating: /content/data/train/aachen_000078_000019_leftImg8bit_png.rf.cc4adfa4551ca8d0d034f89526604295.jpg  \n",
            "  inflating: /content/data/train/aachen_000079_000019_leftImg8bit_png.rf.2ac3bba14dd21775a50bd617bc7be587.jpg  \n",
            "  inflating: /content/data/train/aachen_000080_000019_leftImg8bit_png.rf.2311c995e3abeda3b7c2ba2e90dddf80.jpg  \n",
            "  inflating: /content/data/train/aachen_000081_000019_leftImg8bit_png.rf.a90eed1c7a7545c6410037bb21ef5d36.jpg  \n",
            "  inflating: /content/data/train/aachen_000082_000019_leftImg8bit_png.rf.72a023062f990cdb18baea60b19e3d79.jpg  \n",
            "  inflating: /content/data/train/aachen_000083_000019_leftImg8bit_png.rf.30dc49787becc63a284e9c2a973806c4.jpg  \n",
            "  inflating: /content/data/train/aachen_000085_000019_leftImg8bit_png.rf.122d3e8ef5ad1c8aadde930be5e4b461.jpg  \n",
            "  inflating: /content/data/train/aachen_000087_000019_leftImg8bit_png.rf.6ca99b38326512df1416d294871720eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000088_000019_leftImg8bit_png.rf.c82e3c76de2fd92381659a0a82ee1405.jpg  \n",
            "  inflating: /content/data/train/aachen_000089_000019_leftImg8bit_png.rf.506c340b39d22561c43b3272e0bfe0a6.jpg  \n",
            "  inflating: /content/data/train/aachen_000090_000019_leftImg8bit_png.rf.4d935aa535df87c3733362904cc8778f.jpg  \n",
            "  inflating: /content/data/train/aachen_000092_000019_leftImg8bit_png.rf.b76779489351cd819dfa0d7c8d1484a4.jpg  \n",
            "  inflating: /content/data/train/aachen_000093_000019_leftImg8bit_png.rf.356e7e9aa28a0a91dc88d71ba98657d9.jpg  \n",
            "  inflating: /content/data/train/aachen_000094_000019_leftImg8bit_png.rf.6d24805d7ba12b81f792fdb481548448.jpg  \n",
            "  inflating: /content/data/train/aachen_000095_000019_leftImg8bit_png.rf.dc2989dcd19abb2433b9065388619310.jpg  \n",
            "  inflating: /content/data/train/aachen_000096_000019_leftImg8bit_png.rf.0255b08c88af8b9200593ced5d056a61.jpg  \n",
            "  inflating: /content/data/train/aachen_000097_000019_leftImg8bit_png.rf.aca03dc4416adfc2a64452821cda6bb1.jpg  \n",
            "  inflating: /content/data/train/aachen_000098_000019_leftImg8bit_png.rf.7fbd90ec7b2226ad8a178f7e9d9030a5.jpg  \n",
            "  inflating: /content/data/train/aachen_000099_000019_leftImg8bit_png.rf.02d4fa4ae52edebf96f044ba83ed66e0.jpg  \n",
            "  inflating: /content/data/train/aachen_000101_000019_leftImg8bit_png.rf.49c37139cf2433206154633375f28229.jpg  \n",
            "  inflating: /content/data/train/aachen_000103_000019_leftImg8bit_png.rf.1d0680f932c2ac46c90b75cf203e0bd8.jpg  \n",
            "  inflating: /content/data/train/aachen_000104_000019_leftImg8bit_png.rf.acc8cb7e50c8fbf34574646ec432f9eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000105_000019_leftImg8bit_png.rf.f7dca82179bbf100cdb5e8588dda6072.jpg  \n",
            "  inflating: /content/data/train/aachen_000106_000019_leftImg8bit_png.rf.4e69cfbc700b9b93f259ea5b5154fab8.jpg  \n",
            "  inflating: /content/data/train/aachen_000107_000019_leftImg8bit_png.rf.33cc496a658ab5442d351466769b29c5.jpg  \n",
            "  inflating: /content/data/train/aachen_000108_000019_leftImg8bit_png.rf.781c474237b83bb520170754abefc440.jpg  \n",
            "  inflating: /content/data/train/aachen_000109_000019_leftImg8bit_png.rf.14683461510891e9b39c1b6e455c98e4.jpg  \n",
            "  inflating: /content/data/train/aachen_000113_000019_leftImg8bit_png.rf.d0520d659de2a15f82958165a9211091.jpg  \n",
            "  inflating: /content/data/train/aachen_000114_000019_leftImg8bit_png.rf.f458aa9a3124ba0ed37b3d770b4281da.jpg  \n",
            "  inflating: /content/data/train/aachen_000115_000019_leftImg8bit_png.rf.3eac5ba7981b51c6f2b1c76b2acb52c8.jpg  \n",
            "  inflating: /content/data/train/aachen_000117_000019_leftImg8bit_png.rf.81ae718a167a947f37ac111f8e7b50f5.jpg  \n",
            "  inflating: /content/data/train/aachen_000118_000019_leftImg8bit_png.rf.20a269ca4d5f2f4396a58ce92b472de3.jpg  \n",
            "  inflating: /content/data/train/aachen_000119_000019_leftImg8bit_png.rf.91bb1b005bf551ab15f0c17cee5d3935.jpg  \n",
            "  inflating: /content/data/train/aachen_000124_000019_leftImg8bit_png.rf.0524efd0270f1cf4e053db2417193fee.jpg  \n",
            "  inflating: /content/data/train/aachen_000125_000019_leftImg8bit_png.rf.180b75c2204b7b051ed4e1d52da910fb.jpg  \n",
            "  inflating: /content/data/train/aachen_000126_000019_leftImg8bit_png.rf.e07728e3131d3c6043cadf083e77713f.jpg  \n",
            "  inflating: /content/data/train/aachen_000127_000019_leftImg8bit_png.rf.8f706816706f0e7f20ea71d3bf7d419f.jpg  \n",
            "  inflating: /content/data/train/aachen_000128_000019_leftImg8bit_png.rf.6ae359a7fd72496cb3b0d5fcc996d434.jpg  \n",
            "  inflating: /content/data/train/aachen_000129_000019_leftImg8bit_png.rf.631ca71349d4e6baf58833f8714d7851.jpg  \n",
            "  inflating: /content/data/train/aachen_000130_000019_leftImg8bit_png.rf.c7e5ac55a71ac12880822c051f2de163.jpg  \n",
            "  inflating: /content/data/train/aachen_000131_000019_leftImg8bit_png.rf.8e488844cce658f76733654af0e2cd7d.jpg  \n",
            "  inflating: /content/data/train/aachen_000132_000019_leftImg8bit_png.rf.afc8ad56b7a29d94906caadb16ac672c.jpg  \n",
            "  inflating: /content/data/train/aachen_000133_000019_leftImg8bit_png.rf.9fb3b29d309760ff05f0a972c034beaf.jpg  \n",
            "  inflating: /content/data/train/aachen_000134_000019_leftImg8bit_png.rf.ba30a4a6e827364d7e0f407eaad5b491.jpg  \n",
            "  inflating: /content/data/train/aachen_000135_000019_leftImg8bit_png.rf.bce0c7196e52636b2a82e1374d568de1.jpg  \n",
            "  inflating: /content/data/train/aachen_000136_000019_leftImg8bit_png.rf.a7e51c45a64d183f1699f2fd993e90a0.jpg  \n",
            "  inflating: /content/data/train/aachen_000137_000019_leftImg8bit_png.rf.ca65ac2cb350587c9ddff89514878f45.jpg  \n",
            "  inflating: /content/data/train/aachen_000140_000019_leftImg8bit_png.rf.96d744a6fd6b306f019f7d8b4fa8eea6.jpg  \n",
            "  inflating: /content/data/train/aachen_000141_000019_leftImg8bit_png.rf.ee35b8d44543bbedc4045d2378147f29.jpg  \n",
            "  inflating: /content/data/train/aachen_000142_000019_leftImg8bit_png.rf.d4e871c2194bb382986237ebba91e246.jpg  \n",
            "  inflating: /content/data/train/aachen_000143_000019_leftImg8bit_png.rf.d893d65b77e35d650e9a35fad263cb76.jpg  \n",
            "  inflating: /content/data/train/aachen_000145_000019_leftImg8bit_png.rf.71fee64855f3b61477e3f2edb72c1371.jpg  \n",
            "  inflating: /content/data/train/aachen_000146_000019_leftImg8bit_png.rf.34797ab10911181da932fdda12cc1bcb.jpg  \n",
            "  inflating: /content/data/train/aachen_000147_000019_leftImg8bit_png.rf.e274d8d63ba97a51f9e2903aec4af6ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000148_000019_leftImg8bit_png.rf.1623e2e6f4b5e9eb8e7d08f783c8b62a.jpg  \n",
            "  inflating: /content/data/train/aachen_000149_000019_leftImg8bit_png.rf.b2785c1ee31130aea2c1e30aa9b8ee72.jpg  \n",
            "  inflating: /content/data/train/aachen_000151_000019_leftImg8bit_png.rf.8a799370f596d09430b4ba3b4031a756.jpg  \n",
            "  inflating: /content/data/train/aachen_000152_000019_leftImg8bit_png.rf.4a46069771bc7e953460f23ee9aabb81.jpg  \n",
            "  inflating: /content/data/train/aachen_000153_000019_leftImg8bit_png.rf.f1cba172cf391b9dce19cf26e020e6a1.jpg  \n",
            "  inflating: /content/data/train/aachen_000154_000019_leftImg8bit_png.rf.882ceaaa629155c07f0e14365fc88592.jpg  \n",
            "  inflating: /content/data/train/aachen_000156_000019_leftImg8bit_png.rf.eb4161ee4b7897524001f8644c6395bc.jpg  \n",
            "  inflating: /content/data/train/aachen_000157_000019_leftImg8bit_png.rf.8f4bdf1192aed5b585586cfa3735fce6.jpg  \n",
            "  inflating: /content/data/train/aachen_000158_000019_leftImg8bit_png.rf.7e135350c7c101d65f4d416a18fb4164.jpg  \n",
            "  inflating: /content/data/train/aachen_000159_000019_leftImg8bit_png.rf.27211f1b4f231e82fa8245345ead8339.jpg  \n",
            "  inflating: /content/data/train/aachen_000161_000019_leftImg8bit_png.rf.560dc17d398c591ebbadcdbd32ca05de.jpg  \n",
            "  inflating: /content/data/train/aachen_000162_000019_leftImg8bit_png.rf.9d7e0d3df4ebbe261a35cd9f4b0d8c51.jpg  \n",
            "  inflating: /content/data/train/aachen_000166_000019_leftImg8bit_png.rf.a42129c63a156108e76d48b9d909f921.jpg  \n",
            "  inflating: /content/data/train/aachen_000170_000019_leftImg8bit_png.rf.3c7a97c13c6f6ed36fc73d977f2f5f61.jpg  \n",
            "  inflating: /content/data/train/aachen_000171_000019_leftImg8bit_png.rf.adb9a75ba618dd928a760402a18ab860.jpg  \n",
            "  inflating: /content/data/train/aachen_000172_000019_leftImg8bit_png.rf.0aa2934f298162c62156f4d873d82232.jpg  \n",
            "  inflating: /content/data/train/aachen_000173_000019_leftImg8bit_png.rf.b4845aed54386c2cc0129fcca1eae8a7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000313_leftImg8bit_png.rf.cdb54888334f20611ebcd00c9af1deec.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000600_leftImg8bit_png.rf.b4c24893839f2bcd9b06afde6f849e8a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000885_leftImg8bit_png.rf.01a02ae08b2bfb62d0e80947c9be8872.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001097_leftImg8bit_png.rf.c7e62a71951ab1ab5842b4ff5127325f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001519_leftImg8bit_png.rf.0ecd8b2735e614373916e1ceb3a1d7d6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001828_leftImg8bit_png.rf.bbd3318dc482c8ccf7945cf65fe6d951.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002293_leftImg8bit_png.rf.2017c0aedd3274782c88d2efb46f97df.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002562_leftImg8bit_png.rf.848ac562b826b2c581911d69053799a3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003005_leftImg8bit_png.rf.23b4c88d410a7bc0748b4cd8beb4e98b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003245_leftImg8bit_png.rf.5fe32ed5747882d0885e87ca4f5e8b21.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003674_leftImg8bit_png.rf.06dccce91b2cfb7485ad7e703350620b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004032_leftImg8bit_png.rf.7c84310fc88c12ddee1373ceba32cb36.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004229_leftImg8bit_png.rf.0a200538efdf332df0ed72d7fd83bb02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004748_leftImg8bit_png.rf.eefb5f1a511d74a60ba60eb0b1da8cf5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005537_leftImg8bit_png.rf.49cc4e08f14d7d86ded0ce4665fa5210.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005936_leftImg8bit_png.rf.56f7b0c22522c08edb995cdbb2e72c76.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006026_leftImg8bit_png.rf.ca46678b4a0b8c454ff2707273b5326f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006484_leftImg8bit_png.rf.a0b9382e02d60b014ca464b0b3877d57.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006746_leftImg8bit_png.rf.2bfe8191bd8d58f2bd3c9027e82e13c3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007150_leftImg8bit_png.rf.5d4767e7b9f38941d4d33a9afca6f0b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007651_leftImg8bit_png.rf.f3990a0ce5318f8a2543694c79dc677f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007950_leftImg8bit_png.rf.e74ba39da412ddbc4182b18334f2b4a6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008162_leftImg8bit_png.rf.ffa6fba76cc50401e9320689f9fa4e7e.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008448_leftImg8bit_png.rf.0b0d4699fed0f591b4320949ead1221f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008804_leftImg8bit_png.rf.2b7364087aa2828dd5ce1809af738f1d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009554_leftImg8bit_png.rf.a53264261013eb7a73b2da3dc815b5f7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009951_leftImg8bit_png.rf.f5bd1ba188821fc687b5784b6bbb9db4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010562_leftImg8bit_png.rf.1bc55b226f79b236b65caa5d31db800b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010700_leftImg8bit_png.rf.0d6d60bcc7834f21fa8e1b6f91f9b92a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011255_leftImg8bit_png.rf.ae9061f18745641e2eb326804cefe956.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011711_leftImg8bit_png.rf.ed5d78847f6960e7550b2aa598249c15.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013209_leftImg8bit_png.rf.ae4b5c6c11d4c9e5049e97b38fbc8879.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013705_leftImg8bit_png.rf.8d9bab8adc8b3defa4bcaca6965d7ab7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014332_leftImg8bit_png.rf.550c0517305e8ff5344fd234a6c0f97b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014658_leftImg8bit_png.rf.ab3930592d123c03c355f0eace7ff85a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014803_leftImg8bit_png.rf.caf45c9c363708685e260fc7b51be23b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015038_leftImg8bit_png.rf.62e139d2f97498715ef3fe8a6b6e1c35.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015321_leftImg8bit_png.rf.7f182a92115bb07442599a7db5ba172b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015645_leftImg8bit_png.rf.a3651463751f17ab64b54c52fc3bfd66.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015880_leftImg8bit_png.rf.158cbe1a9b498263d75e4f7a13219964.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016125_leftImg8bit_png.rf.e16afcadd8849ede0789ddc05384d331.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016260_leftImg8bit_png.rf.7f720d63a85187ae68bc78a48b497f62.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016591_leftImg8bit_png.rf.8db9f2f9eb6b552673739c6fc74e38f5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016758_leftImg8bit_png.rf.3346b9076cbbde3513e9c3a752d37ea1.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017216_leftImg8bit_png.rf.4cbea2ce893ddd2cfdaf22de929c2636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017453_leftImg8bit_png.rf.860c1c92b266e77ab84ef397473e95b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_018195_leftImg8bit_png.rf.cf6f05a76903407c272b24b22f6582f0.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_019188_leftImg8bit_png.rf.8557e31d399f843d310e76e65f0d7c80.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020673_leftImg8bit_png.rf.dbd3a86dcbd8805d713e5c4217080444.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020776_leftImg8bit_png.rf.369980b8627d9677e26ac8bdf83a7547.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020899_leftImg8bit_png.rf.6ce4b719674fe57df93bb54dc5bbb998.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021070_leftImg8bit_png.rf.8325c8ae110f28f6fe237a04f2e33636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021325_leftImg8bit_png.rf.b68052e0f3ee178253e75a0169bf49d5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021393_leftImg8bit_png.rf.2b23e701b246cc68999af0cb35d07e8f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021479_leftImg8bit_png.rf.d9aa04d2edc3394d394bfa35e16ae490.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021606_leftImg8bit_png.rf.6702f2e90e4555f7f1460db2c5e5850a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022210_leftImg8bit_png.rf.c1177d53e1dfd1ef8bf890b09aa67aee.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022414_leftImg8bit_png.rf.5d566b2d947f4b399ee2af9f3f5d1232.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023040_leftImg8bit_png.rf.8f9eebc460ecdc4bff9b56f530d9aa1c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023174_leftImg8bit_png.rf.42032eae5d1bb3c9541b1b7bc9a02cc2.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023435_leftImg8bit_png.rf.17cecf48854cef1e1e075a80c94e4153.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023648_leftImg8bit_png.rf.e95f5a8364203c3f5d24ffa4ba25879d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024196_leftImg8bit_png.rf.585d15dd2c92cab2f73b441ad4807b8b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024343_leftImg8bit_png.rf.edaa4c6ef8c708146cdc968949a20b55.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024524_leftImg8bit_png.rf.ba7c88de97f03a740dc6dc48254f0b68.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024717_leftImg8bit_png.rf.64e781abffc3371bb926895177aeccef.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024855_leftImg8bit_png.rf.35403fc1d9bc0a72c447e6858bac2a02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025746_leftImg8bit_png.rf.ec311dad296652b3ec4e81f6674af666.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025833_leftImg8bit_png.rf.9705275256673249d132f470ce6fb911.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026056_leftImg8bit_png.rf.344f8ddabad1025860710b809d278c20.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026634_leftImg8bit_png.rf.5a6ad1ef572d13fa578e36c86598e1bc.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027057_leftImg8bit_png.rf.5d389009d11e6c33bae12dc45b09c3d9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027699_leftImg8bit_png.rf.488e9485ee5f3ff55db89fa6522c63a4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027951_leftImg8bit_png.rf.283ec4d088ca18f8d32b44f9039eb5ad.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028297_leftImg8bit_png.rf.c83bce1734ea69e0227ad541015fb28c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028764_leftImg8bit_png.rf.a8e19726cd87755d1cc1bd4e22699652.jpg  \n",
            "   creating: /content/data/valid/\n",
            "  inflating: /content/data/valid/aachen_000003_000019_leftImg8bit_png.rf.9a379e9f7a303e744866b107930c4175.jpg  \n",
            "  inflating: /content/data/valid/aachen_000007_000019_leftImg8bit_png.rf.85a74f4a1e05e20637a9e3a2d730961d.jpg  \n",
            "  inflating: /content/data/valid/aachen_000012_000019_leftImg8bit_png.rf.b6ce34c36c3f2d3c7bff808609955786.jpg  \n",
            "  inflating: /content/data/valid/aachen_000017_000019_leftImg8bit_png.rf.9b1c3440df6680fabf3953aa1d99cb25.jpg  \n",
            "  inflating: /content/data/valid/aachen_000019_000019_leftImg8bit_png.rf.1d51a99266e69039ed2bc1582ecc07dd.jpg  \n",
            "  inflating: /content/data/valid/aachen_000035_000019_leftImg8bit_png.rf.c83cbc1ce05dee016a75643441216dea.jpg  \n",
            "  inflating: /content/data/valid/aachen_000040_000019_leftImg8bit_png.rf.4d9cafbeff64514512064ad44d1332b1.jpg  \n",
            "  inflating: /content/data/valid/aachen_000048_000019_leftImg8bit_png.rf.f2f7e73b675fbfdfba151fa975c3940c.jpg  \n",
            "  inflating: /content/data/valid/aachen_000053_000019_leftImg8bit_png.rf.0ca5cabb5369372c55c1af22ed5c2c3c.jpg  \n",
            "  inflating: /content/data/valid/aachen_000056_000019_leftImg8bit_png.rf.cf96e54a4c525d38da85cb78503c6ccf.jpg  \n",
            "  inflating: /content/data/valid/aachen_000064_000019_leftImg8bit_png.rf.4569531a2f19553b428b3b5523d9f0bd.jpg  \n",
            "  inflating: /content/data/valid/aachen_000066_000019_leftImg8bit_png.rf.08e6f4873424578357d5fd86a1ddd999.jpg  \n",
            "  inflating: /content/data/valid/aachen_000070_000019_leftImg8bit_png.rf.8ff4653e899a25eed2df080a7b2a2796.jpg  \n",
            "  inflating: /content/data/valid/aachen_000071_000019_leftImg8bit_png.rf.bc59a08bc6f7ed79eb33a6f05116f94b.jpg  \n",
            "  inflating: /content/data/valid/aachen_000074_000019_leftImg8bit_png.rf.504cd36b5cb0a79d74d906c8e23f6344.jpg  \n",
            "  inflating: /content/data/valid/aachen_000084_000019_leftImg8bit_png.rf.513e35b8bdc48bfe9ced0363aa17d06e.jpg  \n",
            "  inflating: /content/data/valid/aachen_000086_000019_leftImg8bit_png.rf.a213e85264ac5d8052232a0315ee3185.jpg  \n",
            "  inflating: /content/data/valid/aachen_000102_000019_leftImg8bit_png.rf.963ed9d6fd4cfff0c1dba932304b154c.jpg  \n",
            "  inflating: /content/data/valid/aachen_000112_000019_leftImg8bit_png.rf.7443071dee0f6d2a122d772212fadd29.jpg  \n",
            "  inflating: /content/data/valid/aachen_000116_000019_leftImg8bit_png.rf.713f786b2697c54726f5f5e86ac31b9f.jpg  \n",
            "  inflating: /content/data/valid/aachen_000120_000019_leftImg8bit_png.rf.52ba85851eac90172dec00d120150585.jpg  \n",
            "  inflating: /content/data/valid/aachen_000121_000019_leftImg8bit_png.rf.7034e6e2563e5b7bd769ae410290d90f.jpg  \n",
            "  inflating: /content/data/valid/aachen_000122_000019_leftImg8bit_png.rf.5dd22bed47e2f4b3156c5c4f77b677d7.jpg  \n",
            "  inflating: /content/data/valid/aachen_000139_000019_leftImg8bit_png.rf.7d4794ecb099f44ddab7cb34aad5f06e.jpg  \n",
            "  inflating: /content/data/valid/aachen_000144_000019_leftImg8bit_png.rf.12a3f8ad8bc5e3d32486b24e545f702c.jpg  \n",
            "  inflating: /content/data/valid/aachen_000160_000019_leftImg8bit_png.rf.ed4afa3d9454c16a61b38818546ceda0.jpg  \n",
            "  inflating: /content/data/valid/aachen_000165_000019_leftImg8bit_png.rf.55ccba25725d192d2bced4d243a49317.jpg  \n",
            "  inflating: /content/data/valid/aachen_000167_000019_leftImg8bit_png.rf.23fd3c37a7b39244a900ecff5ca6b73f.jpg  \n",
            "  inflating: /content/data/valid/aachen_000168_000019_leftImg8bit_png.rf.81d2baa96ed6469f3e14402ce34f8495.jpg  \n",
            "  inflating: /content/data/valid/aachen_000169_000019_leftImg8bit_png.rf.2def8edb587275ca4b99e858b36ed1fa.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                #util misc.py\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)"
      ],
      "metadata": {
        "id": "_sXoHHZpupKf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=256):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = torch.cat([\n",
        "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(args):\n",
        "    N_steps = args.hidden_dim // 2\n",
        "    if args.position_embedding in ('v2', 'sine'):\n",
        "        # TODO find a better way of exposing other arguments\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif args.position_embedding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"not supported {args.position_embedding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "z7tUSid5d-aL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "class BackboneBase(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
        "        super().__init__()\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "        if return_interm_layers:\n",
        "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
        "        else:\n",
        "            return_layers = {'layer4': \"0\"}\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self.body(tensor_list.tensors)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            assert m is not None\n",
        "            mask = torch.nn.functional.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            out[name] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Backbone(BackboneBase):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str,\n",
        "                 train_backbone: bool,\n",
        "                 return_interm_layers: bool,\n",
        "                 dilation: bool):\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "            replace_stride_with_dilation=[False, False, dilation],\n",
        "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
        "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
        "\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for name, x in xs.items():\n",
        "            out.append(x)\n",
        "            # position encoding\n",
        "            pos.append(self[1](x).to(x.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(args):\n",
        "    position_embedding = build_position_encoding(args)\n",
        "    train_backbone = args.lr_backbone > 0\n",
        "    return_interm_layers = args.masks\n",
        "    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "id": "iZbz9oRtdxbW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "# modified from torchvision to also return the union\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "\n",
        "def masks_to_boxes(masks):\n",
        "    \"\"\"Compute the bounding boxes around the provided masks\n",
        "    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n",
        "    Returns a [N, 4] tensors, with the boxes in xyxy format\n",
        "    \"\"\"\n",
        "    if masks.numel() == 0:\n",
        "        return torch.zeros((0, 4), device=masks.device)\n",
        "\n",
        "    h, w = masks.shape[-2:]\n",
        "\n",
        "    y = torch.arange(0, h, dtype=torch.float)\n",
        "    x = torch.arange(0, w, dtype=torch.float)\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    x_mask = (masks * x.unsqueeze(0))\n",
        "    x_max = x_mask.flatten(1).max(-1)[0]\n",
        "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    y_mask = (masks * y.unsqueeze(0))\n",
        "    y_max = y_mask.flatten(1).max(-1)[0]\n",
        "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
      ],
      "metadata": {
        "id": "_9wRuXfUebzm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "\n",
        "def build_matcher(args):\n",
        "    return HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)"
      ],
      "metadata": {
        "id": "r40lsWnJeNEc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from panopticapi.utils import id2rgb, rgb2id\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class DETRsegm(nn.Module):\n",
        "    def __init__(self, detr, freeze_detr=False):\n",
        "        super().__init__()\n",
        "        self.detr = detr\n",
        "\n",
        "        if freeze_detr:\n",
        "            for p in self.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
        "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)\n",
        "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.detr.backbone(samples)\n",
        "\n",
        "        bs = features[-1].tensors.shape[0]\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        src_proj = self.detr.input_proj(src)\n",
        "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
        "\n",
        "        outputs_class = self.detr.class_embed(hs)\n",
        "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
        "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
        "        if self.detr.aux_loss:\n",
        "            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)\n",
        "\n",
        "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
        "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
        "\n",
        "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
        "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
        "\n",
        "        out[\"pred_masks\"] = outputs_seg_masks\n",
        "        return out\n",
        "\n",
        "\n",
        "def _expand(tensor, length: int):\n",
        "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
        "\n",
        "\n",
        "class MaskHeadSmallConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple convolutional head, using group norm.\n",
        "    Upsampling is done using a FPN approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, fpn_dims, context_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
        "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
        "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
        "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
        "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
        "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
        "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
        "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
        "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
        "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
        "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
        "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
        "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
        "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
        "        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
        "\n",
        "        x = self.lay1(x)\n",
        "        x = self.gn1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.lay2(x)\n",
        "        x = self.gn2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter1(fpns[0])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay3(x)\n",
        "        x = self.gn3(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter2(fpns[1])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay4(x)\n",
        "        x = self.gn4(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter3(fpns[2])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay5(x)\n",
        "        x = self.gn5(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        x = self.out_lay(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MHAttentionMap(nn.Module):\n",
        "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
        "\n",
        "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "\n",
        "        nn.init.zeros_(self.k_linear.bias)\n",
        "        nn.init.zeros_(self.q_linear.bias)\n",
        "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
        "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
        "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
        "\n",
        "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
        "        q = self.q_linear(q)\n",
        "        k = torch.nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
        "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
        "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
        "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
        "\n",
        "        if mask is not None:\n",
        "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
        "        weights = torch.nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
        "        weights = self.dropout(weights)\n",
        "        return weights\n",
        "\n",
        "\n",
        "def dice_loss(inputs, targets, num_boxes):\n",
        "    \"\"\"\n",
        "    Compute the DICE loss, similar to generalized IOU for masks\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "    \"\"\"\n",
        "    inputs = inputs.sigmoid()\n",
        "    inputs = inputs.flatten(1)\n",
        "    numerator = 2 * (inputs * targets).sum(1)\n",
        "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
        "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
        "    return loss.sum() / num_boxes\n",
        "\n",
        "\n",
        "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
        "    \"\"\"\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "    prob = inputs.sigmoid()\n",
        "    ce_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    return loss.mean(1).sum() / num_boxes\n",
        "\n",
        "\n",
        "class PostProcessSegm(nn.Module):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
        "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
        "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
        "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
        "        outputs_masks = torch.nn.functional.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
        "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
        "\n",
        "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
        "            img_h, img_w = t[0], t[1]\n",
        "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
        "            results[i][\"masks\"] = torch.nn.functional.interpolate(\n",
        "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
        "            ).byte()\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class PostProcessPanoptic(nn.Module):\n",
        "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
        "    coco panoptic API \"\"\"\n",
        "\n",
        "    def __init__(self, is_thing_map, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
        "                          the class is  a thing (True) or a stuff (False) class\n",
        "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.is_thing_map = is_thing_map\n",
        "\n",
        "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
        "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
        "        Parameters:\n",
        "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
        "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
        "                             model, ie the size after data augmentation but before batching.\n",
        "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
        "                          of each prediction. If left to None, it will default to the processed_sizes\n",
        "            \"\"\"\n",
        "        if target_sizes is None:\n",
        "            target_sizes = processed_sizes\n",
        "        assert len(processed_sizes) == len(target_sizes)\n",
        "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
        "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
        "        preds = []\n",
        "\n",
        "        def to_tuple(tup):\n",
        "            if isinstance(tup, tuple):\n",
        "                return tup\n",
        "            return tuple(tup.cpu().tolist())\n",
        "\n",
        "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
        "            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
        "        ):\n",
        "            # we filter empty queries and detection below threshold\n",
        "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
        "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
        "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
        "            cur_scores = cur_scores[keep]\n",
        "            cur_classes = cur_classes[keep]\n",
        "            cur_masks = cur_masks[keep]\n",
        "            cur_masks = interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
        "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
        "\n",
        "            h, w = cur_masks.shape[-2:]\n",
        "            assert len(cur_boxes) == len(cur_classes)\n",
        "\n",
        "            # It may be that we have several predicted masks for the same stuff class.\n",
        "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
        "            cur_masks = cur_masks.flatten(1)\n",
        "            stuff_equiv_classes = defaultdict(lambda: [])\n",
        "            for k, label in enumerate(cur_classes):\n",
        "                if not self.is_thing_map[label.item()]:\n",
        "                    stuff_equiv_classes[label.item()].append(k)\n",
        "\n",
        "            def get_ids_area(masks, scores, dedup=False):\n",
        "                # This helper function creates the final panoptic segmentation image\n",
        "                # It also returns the area of the masks that appears on the image\n",
        "\n",
        "                m_id = masks.transpose(0, 1).softmax(-1)\n",
        "\n",
        "                if m_id.shape[-1] == 0:\n",
        "                    # We didn't detect any mask :(\n",
        "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
        "                else:\n",
        "                    m_id = m_id.argmax(-1).view(h, w)\n",
        "\n",
        "                if dedup:\n",
        "                    # Merge the masks corresponding to the same stuff class\n",
        "                    for equiv in stuff_equiv_classes.values():\n",
        "                        if len(equiv) > 1:\n",
        "                            for eq_id in equiv:\n",
        "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
        "\n",
        "                final_h, final_w = to_tuple(target_size)\n",
        "\n",
        "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
        "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
        "\n",
        "                np_seg_img = (\n",
        "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
        "                )\n",
        "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
        "\n",
        "                area = []\n",
        "                for i in range(len(scores)):\n",
        "                    area.append(m_id.eq(i).sum().item())\n",
        "                return area, seg_img\n",
        "\n",
        "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
        "            if cur_classes.numel() > 0:\n",
        "                # We know filter empty masks as long as we find some\n",
        "                while True:\n",
        "                    filtered_small = torch.as_tensor(\n",
        "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
        "                    )\n",
        "                    if filtered_small.any().item():\n",
        "                        cur_scores = cur_scores[~filtered_small]\n",
        "                        cur_classes = cur_classes[~filtered_small]\n",
        "                        cur_masks = cur_masks[~filtered_small]\n",
        "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            else:\n",
        "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
        "\n",
        "            segments_info = []\n",
        "            for i, a in enumerate(area):\n",
        "                cat = cur_classes[i].item()\n",
        "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
        "            del cur_classes\n",
        "\n",
        "            with io.BytesIO() as out:\n",
        "                seg_img.save(out, format=\"PNG\")\n",
        "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
        "            preds.append(predictions)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "s35pEGzafkaI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
        "                                          return_intermediate=return_intermediate_dec)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed, query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return torch.nn.functional.relu\n",
        "    if activation == \"gelu\":\n",
        "        return torch.nn.functional.gelu\n",
        "    if activation == \"glu\":\n",
        "        return torch.nn.functional.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
      ],
      "metadata": {
        "id": "iBVuKFedfv5a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "# Autograd Function objects are what record operation history on tensors,\n",
        "# and define formulas for the forward and backprop.\n",
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        # Store context for backprop\n",
        "        ctx.alpha = alpha\n",
        "        \n",
        "        # Forward pass is a no-op\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Backward pass is just to -alpha the gradient\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        # Must return same number as inputs to forward()\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "GgySY6tH3uGa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "        self.num_channels_backbone = backbone.num_channels\n",
        "        self.domain_classifier1 = nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(80*80, 100), nn.BatchNorm1d(100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 2),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "        return out\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "        return [{'pred_logits': a, 'pred_boxes': b}\n",
        "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = torch.nn.functional.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = torch.nn.functional.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = torch.nn.functional.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    if loss == 'masks':\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    if loss == 'labels':\n",
        "                        # Logging is enabled only for the last layer\n",
        "                        kwargs = {'log': False}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class PostProcess(nn.Module):\n",
        "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = torch.nn.functional.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = torch.nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "    num_classes = 20 if args.dataset_file != 'coco' else 91\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # for panoptic, we just add a num_classes that is large enough to hold\n",
        "        # max_obj_id + 1, but the exact value doesn't really matter\n",
        "        num_classes = 250\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "    )\n",
        "    if args.masks:\n",
        "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef}\n",
        "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
        "                             eos_coef=args.eos_coef, losses=losses)\n",
        "    criterion.to(device)\n",
        "    postprocessors = {'bbox': PostProcess()}\n",
        "    if args.masks:\n",
        "        postprocessors['segm'] = PostProcessSegm()\n",
        "        if args.dataset_file == \"coco_panoptic\":\n",
        "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
        "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
        "\n",
        "    return model, criterion, postprocessors"
      ],
      "metadata": {
        "id": "4krKVB3edRMJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(args):\n",
        "    return build(args)"
      ],
      "metadata": {
        "id": "JJbCcgswvAJx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def crop(image, target, region):\n",
        "    cropped_image = F.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        target[\"area\"] = area\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        # FIXME should we update the area here if there are no boxes?\n",
        "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
        "        fields.append(\"masks\")\n",
        "\n",
        "    # remove elements for which the boxes or masks that have zero area\n",
        "    if \"boxes\" in target or \"masks\" in target:\n",
        "        # favor boxes selection when defining which elements to keep\n",
        "        # this is compatible with previous implementation\n",
        "        if \"boxes\" in target:\n",
        "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "        else:\n",
        "            keep = target['masks'].flatten(1).any(1)\n",
        "\n",
        "        for field in fields:\n",
        "            target[field] = target[field][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = F.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # size can be min_size (scalar) or (w, h) tuple\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = F.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    if \"area\" in target:\n",
        "        area = target[\"area\"]\n",
        "        scaled_area = area * (ratio_width * ratio_height)\n",
        "        target[\"area\"] = scaled_area\n",
        "\n",
        "    h, w = size\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = interpolate(\n",
        "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "\n",
        "def pad(image, target, padding):\n",
        "    # assumes that we only pad on the bottom right corners\n",
        "    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n",
        "    if target is None:\n",
        "        return padded_image, None\n",
        "    target = target.copy()\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
        "    return padded_image, target\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        region = T.RandomCrop.get_params(img, self.size)\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        image_width, image_height = img.size\n",
        "        crop_height, crop_width = self.size\n",
        "        crop_top = int(round((image_height - crop_height) / 2.))\n",
        "        crop_left = int(round((image_width - crop_width) / 2.))\n",
        "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "\n",
        "class RandomPad(object):\n",
        "    def __init__(self, max_pad):\n",
        "        self.max_pad = max_pad\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        pad_x = random.randint(0, self.max_pad)\n",
        "        pad_y = random.randint(0, self.max_pad)\n",
        "        return pad(img, target, (pad_x, pad_y))\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Randomly selects between transforms1 and transforms2,\n",
        "    with probability p for transforms1 and (1 - p) for transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return F.to_tensor(img), target\n",
        "\n",
        "\n",
        "class RandomErasing(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        return self.eraser(img), target\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = box_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string"
      ],
      "metadata": {
        "id": "pGFF3M8cdneg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CocoPanoptic:\n",
        "    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n",
        "        with open(ann_file, 'r') as f:\n",
        "            self.coco = json.load(f)\n",
        "\n",
        "        # sort 'images' field so that they are aligned with 'annotations'\n",
        "        # i.e., in alphabetical order\n",
        "        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n",
        "        # sanity check\n",
        "        if \"annotations\" in self.coco:\n",
        "            for img, ann in zip(self.coco['images'], self.coco['annotations']):\n",
        "                assert img['file_name'][:-4] == ann['file_name'][:-4]\n",
        "\n",
        "        self.img_folder = img_folder\n",
        "        self.ann_folder = ann_folder\n",
        "        self.ann_file = ann_file\n",
        "        self.transforms = transforms\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann_info = self.coco['annotations'][idx] if \"annotations\" in self.coco else self.coco['images'][idx]\n",
        "        img_path = Path(self.img_folder) / ann_info['file_name'].replace('.png', '.jpg')\n",
        "        ann_path = Path(self.ann_folder) / ann_info['file_name']\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w, h = img.size\n",
        "        if \"segments_info\" in ann_info:\n",
        "            masks = np.asarray(Image.open(ann_path), dtype=np.uint32)\n",
        "            masks = rgb2id(masks)\n",
        "\n",
        "            ids = np.array([ann['id'] for ann in ann_info['segments_info']])\n",
        "            masks = masks == ids[:, None, None]\n",
        "\n",
        "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "            labels = torch.tensor([ann['category_id'] for ann in ann_info['segments_info']], dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['image_id'] = torch.tensor([ann_info['image_id'] if \"image_id\" in ann_info else ann_info[\"id\"]])\n",
        "        if self.return_masks:\n",
        "            target['masks'] = masks\n",
        "        target['labels'] = labels\n",
        "\n",
        "        target[\"boxes\"] = masks_to_boxes(masks)\n",
        "\n",
        "        target['size'] = torch.as_tensor([int(h), int(w)])\n",
        "        target['orig_size'] = torch.as_tensor([int(h), int(w)])\n",
        "        if \"segments_info\" in ann_info:\n",
        "            for name in ['iscrowd', 'area']:\n",
        "                target[name] = torch.tensor([ann[name] for ann in ann_info['segments_info']])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco['images'])\n",
        "\n",
        "    def get_height_and_width(self, idx):\n",
        "        img_info = self.coco['images'][idx]\n",
        "        height = img_info['height']\n",
        "        width = img_info['width']\n",
        "        return height, width\n",
        "\n",
        "\n",
        "def build_coco_panoptic(image_set, args):\n",
        "    img_folder_root = Path(args.coco_path)\n",
        "    ann_folder_root = Path(args.coco_panoptic_path)\n",
        "    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n",
        "    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n",
        "    mode = 'panoptic'\n",
        "    PATHS = {\n",
        "        \"train\": (\"train\", Path(\"annotations\") / f'{mode}_train.coco.json'),\n",
        "        \"val\": (\"valid\", Path(\"annotations\") / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder_path = img_folder_root / img_folder\n",
        "    ann_folder = ann_folder_root / f'{mode}_{img_folder}'\n",
        "    ann_file = ann_folder_root / ann_file\n",
        "\n",
        "    dataset = CocoPanoptic(img_folder_path, ann_folder, ann_file,\n",
        "                           transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "sIpvAUtOl2fx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            RandomSelect(\n",
        "                RandomResize(scales, max_size=1333),\n",
        "                Compose([\n",
        "                    RandomResize([400, 500, 600]),\n",
        "                    RandomSizeCrop(384, 600),\n",
        "                    RandomResize(scales, max_size=1333),\n",
        "                ])\n",
        "            ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return Compose([\n",
        "            RandomResize([800], max_size=1333),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build_coco(image_set, args):\n",
        "    root = Path(args.coco_path)\n",
        "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train\", root / \"annotations\" / f'{mode}_train.coco.json'),\n",
        "        \"val\": (root / \"valid\", root / \"annotations\" / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "9EDouCpOtaKZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        #     break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "\n",
        "\n",
        "def build_dataset(image_set, args):\n",
        "    if args.dataset_file == 'coco':\n",
        "        return build_coco(image_set, args)\n",
        "    if args.dataset_file == 'coco_panoptic':\n",
        "        # to avoid making panopticapi required for coco\n",
        "        # from .coco_panoptic import build as build_coco_panoptic\n",
        "        return build_coco_panoptic(image_set, args)\n",
        "    raise ValueError(f'dataset {args.dataset_file} not supported')"
      ],
      "metadata": {
        "id": "v02HZoMxkgib"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = _evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def _evaluate(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    # tic = time.time()\n",
        "    # print('Running per image evaluation...')\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "a2K6mtEoohni"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from evaluation import pq_compute\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PanopticEvaluator(object):\n",
        "    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n",
        "        self.gt_json = ann_file\n",
        "        self.gt_folder = ann_folder\n",
        "        if is_main_process():\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.mkdir(output_dir)\n",
        "        self.output_dir = output_dir\n",
        "        self.predictions = []\n",
        "\n",
        "    def update(self, predictions):\n",
        "        for p in predictions:\n",
        "            with open(os.path.join(self.output_dir, p[\"file_name\"]), \"wb\") as f:\n",
        "                f.write(p.pop(\"png_string\"))\n",
        "\n",
        "        self.predictions += predictions\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        all_predictions = all_gather(self.predictions)\n",
        "        merged_predictions = []\n",
        "        for p in all_predictions:\n",
        "            merged_predictions += p\n",
        "        self.predictions = merged_predictions\n",
        "\n",
        "    def summarize(self):\n",
        "        if is_main_process():\n",
        "            json_data = {\"annotations\": self.predictions}\n",
        "            predictions_json = os.path.join(self.output_dir, \"predictions.json\")\n",
        "            with open(predictions_json, \"w\") as f:\n",
        "                f.write(json.dumps(json_data))\n",
        "            return pq_compute(self.gt_json, predictions_json, gt_folder=self.gt_folder, pred_folder=self.output_dir)\n",
        "        return None"
      ],
      "metadata": {
        "id": "XnnXPGQno-wp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
        "\n",
        "    panoptic_evaluator = None\n",
        "    if 'panoptic' in postprocessors.keys():\n",
        "        panoptic_evaluator = PanopticEvaluator(\n",
        "            data_loader.dataset.ann_file,\n",
        "            data_loader.dataset.ann_folder,\n",
        "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
        "        )\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, 10, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "        if panoptic_evaluator is not None:\n",
        "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
        "            for i, target in enumerate(targets):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                file_name = f\"{image_id:012d}.png\"\n",
        "                res_pano[i][\"image_id\"] = image_id\n",
        "                res_pano[i][\"file_name\"] = file_name\n",
        "\n",
        "            panoptic_evaluator.update(res_pano)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "    panoptic_res = None\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_res = panoptic_evaluator.summarize()\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "    if coco_evaluator is not None:\n",
        "        if 'bbox' in postprocessors.keys():\n",
        "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
        "    if panoptic_res is not None:\n",
        "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
        "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
        "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
        "    return stats, coco_evaluator"
      ],
      "metadata": {
        "id": "bYNMLy8jpUIX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=2, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--lr_drop', default=200, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    # * Backbone\n",
        "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
        "                        help=\"Name of the convolutional backbone to use\")\n",
        "    parser.add_argument('--dilation', action='store_true',\n",
        "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
        "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
        "                        help=\"Type of positional embedding to use on top of the image features\")\n",
        "\n",
        "    # * Transformer\n",
        "    parser.add_argument('--enc_layers', default=6, type=int,\n",
        "                        help=\"Number of encoding layers in the transformer\")\n",
        "    parser.add_argument('--dec_layers', default=6, type=int,\n",
        "                        help=\"Number of decoding layers in the transformer\")\n",
        "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
        "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
        "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
        "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
        "    parser.add_argument('--dropout', default=0.1, type=float,\n",
        "                        help=\"Dropout applied in the transformer\")\n",
        "    parser.add_argument('--nheads', default=8, type=int,\n",
        "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
        "    parser.add_argument('--num_queries', default=100, type=int,\n",
        "                        help=\"Number of query slots\")\n",
        "    parser.add_argument('--pre_norm', action='store_true')\n",
        "\n",
        "    # * Segmentation\n",
        "    parser.add_argument('--masks', action='store_true',\n",
        "                        help=\"Train segmentation head if the flag is provided\")\n",
        "\n",
        "    # Loss\n",
        "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
        "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
        "    # * Matcher\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
        "                        help=\"L1 box coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
        "                        help=\"giou box coefficient in the matching cost\")\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
        "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
        "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
        "                        help=\"Relative classification weight of the no-object class\")\n",
        "\n",
        "    # dataset parameters\n",
        "    parser.add_argument('--dataset_file', default='coco')\n",
        "    parser.add_argument('--coco_path', default='/content/data', type=str)\n",
        "    parser.add_argument('--coco_panoptic_path', type=str)\n",
        "    parser.add_argument('--remove_difficult', action='store_true')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--num_workers', default=2, type=int)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--world_size', default=1, type=int,\n",
        "                        help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "    return parser\n",
        "\n",
        "def main_train(args):\n",
        "    init_distributed_mode(args)\n",
        "    # print(\"git:\\n  {}\\n\".format(get_sha()))\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    model, criterion, postprocessors = build_model(args)\n",
        "    model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
        "                                  weight_decay=args.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "    if args.distributed:\n",
        "        sampler_train = DistributedSampler(dataset_train)\n",
        "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, args.batch_size, drop_last=True)\n",
        "\n",
        "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                                   collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
        "                                 drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # We also evaluate AP during panoptic training, on original coco DS\n",
        "        coco_val = build_coco(\"val\", args)\n",
        "        base_ds = get_coco_api_from_dataset(coco_val)\n",
        "    else:\n",
        "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
        "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if args.resume:\n",
        "        if args.resume.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.resume, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "            args.start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if args.eval:\n",
        "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
        "                                              data_loader_val, base_ds, device, args.output_dir)\n",
        "        if args.output_dir:\n",
        "            save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "        return\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            sampler_train.set_epoch(epoch)\n",
        "        train_stats = train_one_epoch(\n",
        "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
        "            args.clip_max_norm)\n",
        "        lr_scheduler.step()\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "            # extra checkpoint before LR drop and every 100 epochs\n",
        "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
        "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "        test_stats, coco_evaluator = evaluate(\n",
        "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
        "        )\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "            # for evaluation logs\n",
        "            if coco_evaluator is not None:\n",
        "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "                if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                    filenames = ['latest.pth']\n",
        "                    if epoch % 50 == 0:\n",
        "                        filenames.append(f'{epoch:03}.pth')\n",
        "                    for name in filenames:\n",
        "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                   output_dir / \"eval\" / name)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "metadata": {
        "id": "_B1ZqWzSubXV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "args = parser.parse_args()\n",
        "if args.output_dir:\n",
        "  Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "main_train(args)"
      ],
      "metadata": {
        "id": "fFbD4iyPufcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c2f8fad19d140649f2a58f0200452ca",
            "b03a813bf94348d6b54bf0fc917c1c59",
            "ddfd64923f504676afcce79e69ad79bf",
            "97568a328ab542c592bf2b440c87f4ac",
            "4f65511803054a92a1a4ce8509870e87",
            "581605d2071f4c91a0da72b82128c575",
            "59df99d561294193ad7bd2b2a2b1850f",
            "78b6e03d371f4f26b9db12e661de1d61",
            "8f1e957a42184616834267fdec0e8dfa",
            "426b5f96806548b2848edb7f92ac65ee",
            "b36705e0317b4b48bbf9a827e61f8490"
          ]
        },
        "outputId": "67ee1019-caca-4b52-df8f-b3a31b7115f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aux_loss=True, backbone='resnet50', batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='/content/data', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=False, f='/root/.local/share/jupyter/runtime/kernel-9cb5bc45-5ef2-4a83-9f16-5a6201224c6e.json', frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c2f8fad19d140649f2a58f0200452ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of params: 44569431\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/100]  eta: 0:16:02  lr: 0.000100  class_error: 98.28  loss: 68.3365 (68.3365)  loss_ce: 4.7343 (4.7343)  loss_bbox: 5.0694 (5.0694)  loss_giou: 2.0256 (2.0256)  loss_ce_0: 4.2728 (4.2728)  loss_bbox_0: 4.9275 (4.9275)  loss_giou_0: 2.0213 (2.0213)  loss_ce_1: 3.9179 (3.9179)  loss_bbox_1: 4.9871 (4.9871)  loss_giou_1: 2.0257 (2.0257)  loss_ce_2: 3.9840 (3.9840)  loss_bbox_2: 5.0636 (5.0636)  loss_giou_2: 2.0236 (2.0236)  loss_ce_3: 4.4781 (4.4781)  loss_bbox_3: 5.1309 (5.1309)  loss_giou_3: 2.0231 (2.0231)  loss_ce_4: 4.5535 (4.5535)  loss_bbox_4: 5.0696 (5.0696)  loss_giou_4: 2.0284 (2.0284)  loss_ce_unscaled: 4.7343 (4.7343)  class_error_unscaled: 98.2759 (98.2759)  loss_bbox_unscaled: 1.0139 (1.0139)  loss_giou_unscaled: 1.0128 (1.0128)  cardinality_error_unscaled: 71.0000 (71.0000)  loss_ce_0_unscaled: 4.2728 (4.2728)  loss_bbox_0_unscaled: 0.9855 (0.9855)  loss_giou_0_unscaled: 1.0106 (1.0106)  cardinality_error_0_unscaled: 71.0000 (71.0000)  loss_ce_1_unscaled: 3.9179 (3.9179)  loss_bbox_1_unscaled: 0.9974 (0.9974)  loss_giou_1_unscaled: 1.0128 (1.0128)  cardinality_error_1_unscaled: 71.0000 (71.0000)  loss_ce_2_unscaled: 3.9840 (3.9840)  loss_bbox_2_unscaled: 1.0127 (1.0127)  loss_giou_2_unscaled: 1.0118 (1.0118)  cardinality_error_2_unscaled: 71.0000 (71.0000)  loss_ce_3_unscaled: 4.4781 (4.4781)  loss_bbox_3_unscaled: 1.0262 (1.0262)  loss_giou_3_unscaled: 1.0116 (1.0116)  cardinality_error_3_unscaled: 71.0000 (71.0000)  loss_ce_4_unscaled: 4.5535 (4.5535)  loss_bbox_4_unscaled: 1.0139 (1.0139)  loss_giou_4_unscaled: 1.0142 (1.0142)  cardinality_error_4_unscaled: 71.0000 (71.0000)  time: 9.6247  data: 0.2573  max mem: 855\n",
            "Epoch: [0]  [ 10/100]  eta: 0:01:44  lr: 0.000100  class_error: 50.98  loss: 53.0145 (56.1954)  loss_ce: 2.3031 (2.4720)  loss_bbox: 4.8939 (4.7999)  loss_giou: 2.0649 (2.1104)  loss_ce_0: 2.4610 (2.5699)  loss_bbox_0: 4.9143 (4.8038)  loss_giou_0: 2.0637 (2.1039)  loss_ce_1: 2.2033 (2.4426)  loss_bbox_1: 4.8791 (4.7284)  loss_giou_1: 2.0759 (2.1152)  loss_ce_2: 2.2181 (2.3928)  loss_bbox_2: 4.8731 (4.7462)  loss_giou_2: 2.0635 (2.1099)  loss_ce_3: 2.4969 (2.5521)  loss_bbox_3: 4.7828 (4.6869)  loss_giou_3: 2.0723 (2.1187)  loss_ce_4: 2.4485 (2.5382)  loss_bbox_4: 4.8222 (4.7786)  loss_giou_4: 2.0782 (2.1258)  loss_ce_unscaled: 2.3031 (2.4720)  class_error_unscaled: 50.0000 (46.3210)  loss_bbox_unscaled: 0.9788 (0.9600)  loss_giou_unscaled: 1.0324 (1.0552)  cardinality_error_unscaled: 72.0000 (64.0455)  loss_ce_0_unscaled: 2.4610 (2.5699)  loss_bbox_0_unscaled: 0.9829 (0.9608)  loss_giou_0_unscaled: 1.0318 (1.0519)  cardinality_error_0_unscaled: 79.0000 (76.8182)  loss_ce_1_unscaled: 2.2033 (2.4426)  loss_bbox_1_unscaled: 0.9758 (0.9457)  loss_giou_1_unscaled: 1.0379 (1.0576)  cardinality_error_1_unscaled: 78.5000 (76.1364)  loss_ce_2_unscaled: 2.2181 (2.3928)  loss_bbox_2_unscaled: 0.9746 (0.9492)  loss_giou_2_unscaled: 1.0318 (1.0550)  cardinality_error_2_unscaled: 80.5000 (68.6818)  loss_ce_3_unscaled: 2.4969 (2.5521)  loss_bbox_3_unscaled: 0.9566 (0.9374)  loss_giou_3_unscaled: 1.0361 (1.0594)  cardinality_error_3_unscaled: 72.0000 (59.9091)  loss_ce_4_unscaled: 2.4485 (2.5382)  loss_bbox_4_unscaled: 0.9644 (0.9557)  loss_giou_4_unscaled: 1.0391 (1.0629)  cardinality_error_4_unscaled: 71.5000 (63.2273)  time: 1.1576  data: 0.0338  max mem: 1922\n",
            "Epoch: [0]  [ 20/100]  eta: 0:01:00  lr: 0.000100  class_error: 50.00  loss: 44.8082 (49.2366)  loss_ce: 1.4750 (1.9631)  loss_bbox: 3.8567 (4.0878)  loss_giou: 2.1066 (2.1598)  loss_ce_0: 1.6387 (2.0139)  loss_bbox_0: 3.9384 (4.1192)  loss_giou_0: 2.1078 (2.1502)  loss_ce_1: 1.5857 (1.9485)  loss_bbox_1: 3.9137 (4.0700)  loss_giou_1: 2.1174 (2.1629)  loss_ce_2: 1.7623 (1.9398)  loss_bbox_2: 3.8304 (4.0306)  loss_giou_2: 2.1111 (2.1625)  loss_ce_3: 1.6720 (2.0342)  loss_bbox_3: 3.7760 (3.9945)  loss_giou_3: 2.1119 (2.1673)  loss_ce_4: 1.5255 (2.0199)  loss_bbox_4: 3.7674 (4.0366)  loss_giou_4: 2.1216 (2.1758)  loss_ce_unscaled: 1.4750 (1.9631)  class_error_unscaled: 29.1667 (45.4408)  loss_bbox_unscaled: 0.7713 (0.8176)  loss_giou_unscaled: 1.0533 (1.0799)  cardinality_error_unscaled: 63.0000 (58.2381)  loss_ce_0_unscaled: 1.6387 (2.0139)  loss_bbox_0_unscaled: 0.7877 (0.8238)  loss_giou_0_unscaled: 1.0539 (1.0751)  cardinality_error_0_unscaled: 72.5000 (63.3095)  loss_ce_1_unscaled: 1.5857 (1.9485)  loss_bbox_1_unscaled: 0.7827 (0.8140)  loss_giou_1_unscaled: 1.0587 (1.0815)  cardinality_error_1_unscaled: 78.5000 (65.5000)  loss_ce_2_unscaled: 1.7623 (1.9398)  loss_bbox_2_unscaled: 0.7661 (0.8061)  loss_giou_2_unscaled: 1.0556 (1.0812)  cardinality_error_2_unscaled: 78.0000 (62.1667)  loss_ce_3_unscaled: 1.6720 (2.0342)  loss_bbox_3_unscaled: 0.7552 (0.7989)  loss_giou_3_unscaled: 1.0559 (1.0837)  cardinality_error_3_unscaled: 67.5000 (56.1667)  loss_ce_4_unscaled: 1.5255 (2.0199)  loss_bbox_4_unscaled: 0.7535 (0.8073)  loss_giou_4_unscaled: 1.0608 (1.0879)  cardinality_error_4_unscaled: 66.0000 (57.9286)  time: 0.3125  data: 0.0136  max mem: 2041\n",
            "Epoch: [0]  [ 30/100]  eta: 0:00:41  lr: 0.000100  class_error: 97.30  loss: 40.5390 (46.0488)  loss_ce: 1.4360 (1.8537)  loss_bbox: 2.8133 (3.5496)  loss_giou: 2.2883 (2.2674)  loss_ce_0: 1.4028 (1.8897)  loss_bbox_0: 2.8448 (3.5788)  loss_giou_0: 2.2827 (2.2551)  loss_ce_1: 1.4310 (1.8515)  loss_bbox_1: 2.8326 (3.5392)  loss_giou_1: 2.2985 (2.2725)  loss_ce_2: 1.4778 (1.8562)  loss_bbox_2: 2.7238 (3.4960)  loss_giou_2: 2.3047 (2.2750)  loss_ce_3: 1.4995 (1.9105)  loss_bbox_3: 2.7799 (3.4897)  loss_giou_3: 2.3082 (2.2741)  loss_ce_4: 1.4578 (1.8959)  loss_bbox_4: 2.7844 (3.5141)  loss_giou_4: 2.2888 (2.2797)  loss_ce_unscaled: 1.4360 (1.8537)  class_error_unscaled: 65.2174 (57.9843)  loss_bbox_unscaled: 0.5627 (0.7099)  loss_giou_unscaled: 1.1442 (1.1337)  cardinality_error_unscaled: 17.5000 (44.8065)  loss_ce_0_unscaled: 1.4028 (1.8897)  loss_bbox_0_unscaled: 0.5690 (0.7158)  loss_giou_0_unscaled: 1.1413 (1.1276)  cardinality_error_0_unscaled: 17.5000 (46.8871)  loss_ce_1_unscaled: 1.4310 (1.8515)  loss_bbox_1_unscaled: 0.5665 (0.7078)  loss_giou_1_unscaled: 1.1492 (1.1362)  cardinality_error_1_unscaled: 17.0000 (49.3710)  loss_ce_2_unscaled: 1.4778 (1.8562)  loss_bbox_2_unscaled: 0.5448 (0.6992)  loss_giou_2_unscaled: 1.1523 (1.1375)  cardinality_error_2_unscaled: 18.5000 (48.4355)  loss_ce_3_unscaled: 1.4995 (1.9105)  loss_bbox_3_unscaled: 0.5560 (0.6979)  loss_giou_3_unscaled: 1.1541 (1.1370)  cardinality_error_3_unscaled: 18.0000 (43.6452)  loss_ce_4_unscaled: 1.4578 (1.8959)  loss_bbox_4_unscaled: 0.5569 (0.7028)  loss_giou_4_unscaled: 1.1444 (1.1399)  cardinality_error_4_unscaled: 17.5000 (44.5968)  time: 0.2803  data: 0.0132  max mem: 2236\n",
            "Epoch: [0]  [ 40/100]  eta: 0:00:30  lr: 0.000100  class_error: 4.26  loss: 37.4890 (43.8947)  loss_ce: 1.5404 (1.7604)  loss_bbox: 2.2904 (3.1730)  loss_giou: 2.6253 (2.3751)  loss_ce_0: 1.5580 (1.7959)  loss_bbox_0: 2.2899 (3.1945)  loss_giou_0: 2.6176 (2.3654)  loss_ce_1: 1.5195 (1.7600)  loss_bbox_1: 2.2956 (3.1723)  loss_giou_1: 2.6413 (2.3765)  loss_ce_2: 1.5454 (1.7600)  loss_bbox_2: 2.2848 (3.1348)  loss_giou_2: 2.6409 (2.3825)  loss_ce_3: 1.5235 (1.8046)  loss_bbox_3: 2.3020 (3.1332)  loss_giou_3: 2.6444 (2.3794)  loss_ce_4: 1.5386 (1.7924)  loss_bbox_4: 2.2905 (3.1502)  loss_giou_4: 2.6422 (2.3845)  loss_ce_unscaled: 1.5404 (1.7604)  class_error_unscaled: 70.8333 (57.2658)  loss_bbox_unscaled: 0.4581 (0.6346)  loss_giou_unscaled: 1.3126 (1.1876)  cardinality_error_unscaled: 17.5000 (41.2683)  loss_ce_0_unscaled: 1.5580 (1.7959)  loss_bbox_0_unscaled: 0.4580 (0.6389)  loss_giou_0_unscaled: 1.3088 (1.1827)  cardinality_error_0_unscaled: 17.5000 (44.1585)  loss_ce_1_unscaled: 1.5195 (1.7600)  loss_bbox_1_unscaled: 0.4591 (0.6345)  loss_giou_1_unscaled: 1.3207 (1.1883)  cardinality_error_1_unscaled: 18.5000 (49.7805)  loss_ce_2_unscaled: 1.5454 (1.7600)  loss_bbox_2_unscaled: 0.4570 (0.6270)  loss_giou_2_unscaled: 1.3204 (1.1912)  cardinality_error_2_unscaled: 20.0000 (46.8902)  loss_ce_3_unscaled: 1.5235 (1.8046)  loss_bbox_3_unscaled: 0.4604 (0.6266)  loss_giou_3_unscaled: 1.3222 (1.1897)  cardinality_error_3_unscaled: 19.0000 (45.5854)  loss_ce_4_unscaled: 1.5386 (1.7924)  loss_bbox_4_unscaled: 0.4581 (0.6300)  loss_giou_4_unscaled: 1.3211 (1.1922)  cardinality_error_4_unscaled: 20.0000 (41.8171)  time: 0.2648  data: 0.0117  max mem: 2254\n",
            "Epoch: [0]  [ 50/100]  eta: 0:00:23  lr: 0.000100  class_error: 54.05  loss: 36.1040 (42.6864)  loss_ce: 1.4670 (1.7117)  loss_bbox: 1.8815 (2.9252)  loss_giou: 2.7105 (2.4724)  loss_ce_0: 1.5050 (1.7401)  loss_bbox_0: 1.8948 (2.9333)  loss_giou_0: 2.7213 (2.4628)  loss_ce_1: 1.5034 (1.7150)  loss_bbox_1: 1.9295 (2.9307)  loss_giou_1: 2.7065 (2.4720)  loss_ce_2: 1.4870 (1.7095)  loss_bbox_2: 1.9024 (2.8942)  loss_giou_2: 2.7178 (2.4784)  loss_ce_3: 1.4982 (1.7482)  loss_bbox_3: 1.9019 (2.8940)  loss_giou_3: 2.7073 (2.4755)  loss_ce_4: 1.4762 (1.7338)  loss_bbox_4: 1.8949 (2.9082)  loss_giou_4: 2.7161 (2.4814)  loss_ce_unscaled: 1.4670 (1.7117)  class_error_unscaled: 46.6667 (53.3220)  loss_bbox_unscaled: 0.3763 (0.5850)  loss_giou_unscaled: 1.3553 (1.2362)  cardinality_error_unscaled: 54.0000 (48.1667)  loss_ce_0_unscaled: 1.5050 (1.7401)  loss_bbox_0_unscaled: 0.3790 (0.5867)  loss_giou_0_unscaled: 1.3606 (1.2314)  cardinality_error_0_unscaled: 65.5000 (51.3529)  loss_ce_1_unscaled: 1.5034 (1.7150)  loss_bbox_1_unscaled: 0.3859 (0.5861)  loss_giou_1_unscaled: 1.3532 (1.2360)  cardinality_error_1_unscaled: 65.5000 (55.5392)  loss_ce_2_unscaled: 1.4870 (1.7095)  loss_bbox_2_unscaled: 0.3805 (0.5788)  loss_giou_2_unscaled: 1.3589 (1.2392)  cardinality_error_2_unscaled: 65.5000 (52.8627)  loss_ce_3_unscaled: 1.4982 (1.7482)  loss_bbox_3_unscaled: 0.3804 (0.5788)  loss_giou_3_unscaled: 1.3537 (1.2378)  cardinality_error_3_unscaled: 65.5000 (51.8039)  loss_ce_4_unscaled: 1.4762 (1.7338)  loss_bbox_4_unscaled: 0.3790 (0.5816)  loss_giou_4_unscaled: 1.3581 (1.2407)  cardinality_error_4_unscaled: 51.0000 (48.6569)  time: 0.2788  data: 0.0123  max mem: 2485\n",
            "Epoch: [0]  [ 60/100]  eta: 0:00:17  lr: 0.000100  class_error: 41.18  loss: 36.7027 (41.7139)  loss_ce: 1.3179 (1.6527)  loss_bbox: 1.8946 (2.7537)  loss_giou: 2.8444 (2.5398)  loss_ce_0: 1.3430 (1.6781)  loss_bbox_0: 1.8917 (2.7586)  loss_giou_0: 2.8759 (2.5295)  loss_ce_1: 1.3757 (1.6600)  loss_bbox_1: 1.9235 (2.7627)  loss_giou_1: 2.8530 (2.5374)  loss_ce_2: 1.3415 (1.6522)  loss_bbox_2: 1.9024 (2.7281)  loss_giou_2: 2.8701 (2.5441)  loss_ce_3: 1.3668 (1.6874)  loss_bbox_3: 1.9409 (2.7276)  loss_giou_3: 2.8625 (2.5441)  loss_ce_4: 1.2484 (1.6685)  loss_bbox_4: 1.9472 (2.7395)  loss_giou_4: 2.8364 (2.5500)  loss_ce_unscaled: 1.3179 (1.6527)  class_error_unscaled: 71.4286 (58.8803)  loss_bbox_unscaled: 0.3789 (0.5507)  loss_giou_unscaled: 1.4222 (1.2699)  cardinality_error_unscaled: 37.0000 (42.9754)  loss_ce_0_unscaled: 1.3430 (1.6781)  loss_bbox_0_unscaled: 0.3783 (0.5517)  loss_giou_0_unscaled: 1.4379 (1.2648)  cardinality_error_0_unscaled: 63.5000 (46.3770)  loss_ce_1_unscaled: 1.3757 (1.6600)  loss_bbox_1_unscaled: 0.3847 (0.5525)  loss_giou_1_unscaled: 1.4265 (1.2687)  cardinality_error_1_unscaled: 26.0000 (49.1721)  loss_ce_2_unscaled: 1.3415 (1.6522)  loss_bbox_2_unscaled: 0.3805 (0.5456)  loss_giou_2_unscaled: 1.4350 (1.2721)  cardinality_error_2_unscaled: 34.5000 (46.8770)  loss_ce_3_unscaled: 1.3668 (1.6874)  loss_bbox_3_unscaled: 0.3882 (0.5455)  loss_giou_3_unscaled: 1.4313 (1.2720)  cardinality_error_3_unscaled: 28.5000 (45.8689)  loss_ce_4_unscaled: 1.2484 (1.6685)  loss_bbox_4_unscaled: 0.3894 (0.5479)  loss_giou_4_unscaled: 1.4182 (1.2750)  cardinality_error_4_unscaled: 48.5000 (43.9508)  time: 0.2673  data: 0.0116  max mem: 2485\n",
            "Epoch: [0]  [ 70/100]  eta: 0:00:12  lr: 0.000100  class_error: 22.22  loss: 36.7027 (41.0822)  loss_ce: 1.3293 (1.6098)  loss_bbox: 1.8946 (2.6296)  loss_giou: 2.8829 (2.6028)  loss_ce_0: 1.3430 (1.6323)  loss_bbox_0: 1.8698 (2.6296)  loss_giou_0: 2.8777 (2.5937)  loss_ce_1: 1.3757 (1.6183)  loss_bbox_1: 1.8970 (2.6362)  loss_giou_1: 2.9000 (2.6000)  loss_ce_2: 1.3415 (1.6120)  loss_bbox_2: 1.8683 (2.6047)  loss_giou_2: 2.9127 (2.6092)  loss_ce_3: 1.3668 (1.6412)  loss_bbox_3: 1.8662 (2.6025)  loss_giou_3: 2.8980 (2.6094)  loss_ce_4: 1.3433 (1.6225)  loss_bbox_4: 1.8846 (2.6146)  loss_giou_4: 2.8702 (2.6137)  loss_ce_unscaled: 1.3293 (1.6098)  class_error_unscaled: 45.7143 (54.9404)  loss_bbox_unscaled: 0.3789 (0.5259)  loss_giou_unscaled: 1.4415 (1.3014)  cardinality_error_unscaled: 42.0000 (48.5563)  loss_ce_0_unscaled: 1.3430 (1.6323)  loss_bbox_0_unscaled: 0.3740 (0.5259)  loss_giou_0_unscaled: 1.4388 (1.2969)  cardinality_error_0_unscaled: 33.5000 (49.5563)  loss_ce_1_unscaled: 1.3757 (1.6183)  loss_bbox_1_unscaled: 0.3794 (0.5272)  loss_giou_1_unscaled: 1.4500 (1.3000)  cardinality_error_1_unscaled: 26.0000 (53.7254)  loss_ce_2_unscaled: 1.3415 (1.6120)  loss_bbox_2_unscaled: 0.3737 (0.5209)  loss_giou_2_unscaled: 1.4564 (1.3046)  cardinality_error_2_unscaled: 34.5000 (51.8310)  loss_ce_3_unscaled: 1.3668 (1.6412)  loss_bbox_3_unscaled: 0.3732 (0.5205)  loss_giou_3_unscaled: 1.4490 (1.3047)  cardinality_error_3_unscaled: 28.5000 (50.6901)  loss_ce_4_unscaled: 1.3433 (1.6225)  loss_bbox_4_unscaled: 0.3769 (0.5229)  loss_giou_4_unscaled: 1.4351 (1.3068)  cardinality_error_4_unscaled: 73.0000 (49.1761)  time: 0.2577  data: 0.0108  max mem: 2485\n",
            "Epoch: [0]  [ 80/100]  eta: 0:00:07  lr: 0.000100  class_error: 46.88  loss: 35.5538 (40.2009)  loss_ce: 1.3562 (1.5791)  loss_bbox: 1.6781 (2.4967)  loss_giou: 2.8314 (2.6215)  loss_ce_0: 1.2966 (1.5969)  loss_bbox_0: 1.6013 (2.4929)  loss_giou_0: 2.8213 (2.6114)  loss_ce_1: 1.3492 (1.5868)  loss_bbox_1: 1.6530 (2.5021)  loss_giou_1: 2.8136 (2.6170)  loss_ce_2: 1.3388 (1.5800)  loss_bbox_2: 1.6834 (2.4751)  loss_giou_2: 2.8453 (2.6295)  loss_ce_3: 1.3428 (1.6037)  loss_bbox_3: 1.6433 (2.4721)  loss_giou_3: 2.8478 (2.6296)  loss_ce_4: 1.3477 (1.5899)  loss_bbox_4: 1.6763 (2.4833)  loss_giou_4: 2.8505 (2.6333)  loss_ce_unscaled: 1.3562 (1.5791)  class_error_unscaled: 46.8750 (58.8754)  loss_bbox_unscaled: 0.3356 (0.4993)  loss_giou_unscaled: 1.4157 (1.3107)  cardinality_error_unscaled: 54.5000 (44.9815)  loss_ce_0_unscaled: 1.2966 (1.5969)  loss_bbox_0_unscaled: 0.3203 (0.4986)  loss_giou_0_unscaled: 1.4106 (1.3057)  cardinality_error_0_unscaled: 51.5000 (46.8025)  loss_ce_1_unscaled: 1.3492 (1.5868)  loss_bbox_1_unscaled: 0.3306 (0.5004)  loss_giou_1_unscaled: 1.4068 (1.3085)  cardinality_error_1_unscaled: 60.0000 (49.9938)  loss_ce_2_unscaled: 1.3388 (1.5800)  loss_bbox_2_unscaled: 0.3367 (0.4950)  loss_giou_2_unscaled: 1.4226 (1.3148)  cardinality_error_2_unscaled: 67.0000 (48.0062)  loss_ce_3_unscaled: 1.3428 (1.6037)  loss_bbox_3_unscaled: 0.3287 (0.4944)  loss_giou_3_unscaled: 1.4239 (1.3148)  cardinality_error_3_unscaled: 57.5000 (47.1111)  loss_ce_4_unscaled: 1.3477 (1.5899)  loss_bbox_4_unscaled: 0.3353 (0.4967)  loss_giou_4_unscaled: 1.4252 (1.3167)  cardinality_error_4_unscaled: 45.5000 (45.4444)  time: 0.2617  data: 0.0112  max mem: 2485\n",
            "Epoch: [0]  [ 90/100]  eta: 0:00:03  lr: 0.000100  class_error: 76.67  loss: 33.0734 (39.4331)  loss_ce: 1.3337 (1.5586)  loss_bbox: 1.4376 (2.3832)  loss_giou: 2.7237 (2.6251)  loss_ce_0: 1.3085 (1.5729)  loss_bbox_0: 1.4182 (2.3791)  loss_giou_0: 2.6936 (2.6174)  loss_ce_1: 1.3329 (1.5652)  loss_bbox_1: 1.4376 (2.3880)  loss_giou_1: 2.7164 (2.6217)  loss_ce_2: 1.3287 (1.5596)  loss_bbox_2: 1.4096 (2.3631)  loss_giou_2: 2.7563 (2.6351)  loss_ce_3: 1.3424 (1.5797)  loss_bbox_3: 1.4189 (2.3629)  loss_giou_3: 2.7497 (2.6383)  loss_ce_4: 1.3494 (1.5701)  loss_bbox_4: 1.4317 (2.3725)  loss_giou_4: 2.7651 (2.6407)  loss_ce_unscaled: 1.3337 (1.5586)  class_error_unscaled: 60.0000 (57.3851)  loss_bbox_unscaled: 0.2875 (0.4766)  loss_giou_unscaled: 1.3619 (1.3126)  cardinality_error_unscaled: 28.0000 (47.7857)  loss_ce_0_unscaled: 1.3085 (1.5729)  loss_bbox_0_unscaled: 0.2836 (0.4758)  loss_giou_0_unscaled: 1.3468 (1.3087)  cardinality_error_0_unscaled: 52.5000 (49.7363)  loss_ce_1_unscaled: 1.3329 (1.5652)  loss_bbox_1_unscaled: 0.2875 (0.4776)  loss_giou_1_unscaled: 1.3582 (1.3108)  cardinality_error_1_unscaled: 43.5000 (53.0714)  loss_ce_2_unscaled: 1.3287 (1.5596)  loss_bbox_2_unscaled: 0.2819 (0.4726)  loss_giou_2_unscaled: 1.3782 (1.3175)  cardinality_error_2_unscaled: 57.0000 (51.2802)  loss_ce_3_unscaled: 1.3424 (1.5797)  loss_bbox_3_unscaled: 0.2838 (0.4726)  loss_giou_3_unscaled: 1.3748 (1.3192)  cardinality_error_3_unscaled: 28.5000 (50.0549)  loss_ce_4_unscaled: 1.3494 (1.5701)  loss_bbox_4_unscaled: 0.2863 (0.4745)  loss_giou_4_unscaled: 1.3825 (1.3203)  cardinality_error_4_unscaled: 28.0000 (48.6703)  time: 0.2593  data: 0.0114  max mem: 2485\n",
            "Epoch: [0]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 70.00  loss: 33.0734 (38.9181)  loss_ce: 1.2734 (1.5321)  loss_bbox: 1.5372 (2.3171)  loss_giou: 2.7530 (2.6329)  loss_ce_0: 1.3039 (1.5425)  loss_bbox_0: 1.4824 (2.3106)  loss_giou_0: 2.7526 (2.6246)  loss_ce_1: 1.3013 (1.5356)  loss_bbox_1: 1.5375 (2.3212)  loss_giou_1: 2.7434 (2.6302)  loss_ce_2: 1.2917 (1.5305)  loss_bbox_2: 1.5121 (2.2993)  loss_giou_2: 2.7756 (2.6449)  loss_ce_3: 1.2802 (1.5490)  loss_bbox_3: 1.5528 (2.3000)  loss_giou_3: 2.8018 (2.6482)  loss_ce_4: 1.2984 (1.5408)  loss_bbox_4: 1.5269 (2.3094)  loss_giou_4: 2.8014 (2.6493)  loss_ce_unscaled: 1.2734 (1.5321)  class_error_unscaled: 64.7059 (60.5053)  loss_bbox_unscaled: 0.3074 (0.4634)  loss_giou_unscaled: 1.3765 (1.3164)  cardinality_error_unscaled: 19.5000 (44.4900)  loss_ce_0_unscaled: 1.3039 (1.5425)  loss_bbox_0_unscaled: 0.2965 (0.4621)  loss_giou_0_unscaled: 1.3763 (1.3123)  cardinality_error_0_unscaled: 28.0000 (46.2950)  loss_ce_1_unscaled: 1.3013 (1.5356)  loss_bbox_1_unscaled: 0.3075 (0.4642)  loss_giou_1_unscaled: 1.3717 (1.3151)  cardinality_error_1_unscaled: 43.5000 (49.9450)  loss_ce_2_unscaled: 1.2917 (1.5305)  loss_bbox_2_unscaled: 0.3024 (0.4599)  loss_giou_2_unscaled: 1.3878 (1.3224)  cardinality_error_2_unscaled: 28.0000 (47.7100)  loss_ce_3_unscaled: 1.2802 (1.5490)  loss_bbox_3_unscaled: 0.3106 (0.4600)  loss_giou_3_unscaled: 1.4009 (1.3241)  cardinality_error_3_unscaled: 19.0000 (46.4150)  loss_ce_4_unscaled: 1.2984 (1.5408)  loss_bbox_4_unscaled: 0.3054 (0.4619)  loss_giou_4_unscaled: 1.4007 (1.3246)  cardinality_error_4_unscaled: 19.5000 (45.2000)  time: 0.2552  data: 0.0104  max mem: 2485\n",
            "Epoch: [0] Total time: 0:00:36 (0.3666 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 70.00  loss: 33.0734 (38.9181)  loss_ce: 1.2734 (1.5321)  loss_bbox: 1.5372 (2.3171)  loss_giou: 2.7530 (2.6329)  loss_ce_0: 1.3039 (1.5425)  loss_bbox_0: 1.4824 (2.3106)  loss_giou_0: 2.7526 (2.6246)  loss_ce_1: 1.3013 (1.5356)  loss_bbox_1: 1.5375 (2.3212)  loss_giou_1: 2.7434 (2.6302)  loss_ce_2: 1.2917 (1.5305)  loss_bbox_2: 1.5121 (2.2993)  loss_giou_2: 2.7756 (2.6449)  loss_ce_3: 1.2802 (1.5490)  loss_bbox_3: 1.5528 (2.3000)  loss_giou_3: 2.8018 (2.6482)  loss_ce_4: 1.2984 (1.5408)  loss_bbox_4: 1.5269 (2.3094)  loss_giou_4: 2.8014 (2.6493)  loss_ce_unscaled: 1.2734 (1.5321)  class_error_unscaled: 64.7059 (60.5053)  loss_bbox_unscaled: 0.3074 (0.4634)  loss_giou_unscaled: 1.3765 (1.3164)  cardinality_error_unscaled: 19.5000 (44.4900)  loss_ce_0_unscaled: 1.3039 (1.5425)  loss_bbox_0_unscaled: 0.2965 (0.4621)  loss_giou_0_unscaled: 1.3763 (1.3123)  cardinality_error_0_unscaled: 28.0000 (46.2950)  loss_ce_1_unscaled: 1.3013 (1.5356)  loss_bbox_1_unscaled: 0.3075 (0.4642)  loss_giou_1_unscaled: 1.3717 (1.3151)  cardinality_error_1_unscaled: 43.5000 (49.9450)  loss_ce_2_unscaled: 1.2917 (1.5305)  loss_bbox_2_unscaled: 0.3024 (0.4599)  loss_giou_2_unscaled: 1.3878 (1.3224)  cardinality_error_2_unscaled: 28.0000 (47.7100)  loss_ce_3_unscaled: 1.2802 (1.5490)  loss_bbox_3_unscaled: 0.3106 (0.4600)  loss_giou_3_unscaled: 1.4009 (1.3241)  cardinality_error_3_unscaled: 19.0000 (46.4150)  loss_ce_4_unscaled: 1.2984 (1.5408)  loss_bbox_4_unscaled: 0.3054 (0.4619)  loss_giou_4_unscaled: 1.4007 (1.3246)  cardinality_error_4_unscaled: 19.5000 (45.2000)\n",
            "Test:  [ 0/15]  eta: 0:00:06  class_error: 100.00  loss: 40.6044 (40.6044)  loss_ce: 1.6979 (1.6979)  loss_bbox: 1.8615 (1.8615)  loss_giou: 3.1864 (3.1864)  loss_ce_0: 1.7418 (1.7418)  loss_bbox_0: 1.8480 (1.8480)  loss_giou_0: 3.1554 (3.1554)  loss_ce_1: 1.6915 (1.6915)  loss_bbox_1: 1.8507 (1.8507)  loss_giou_1: 3.1263 (3.1263)  loss_ce_2: 1.6977 (1.6977)  loss_bbox_2: 1.8952 (1.8952)  loss_giou_2: 3.2415 (3.2415)  loss_ce_3: 1.7315 (1.7315)  loss_bbox_3: 1.8742 (1.8742)  loss_giou_3: 3.2103 (3.2103)  loss_ce_4: 1.6780 (1.6780)  loss_bbox_4: 1.8985 (1.8985)  loss_giou_4: 3.2181 (3.2181)  loss_ce_unscaled: 1.6979 (1.6979)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3723 (0.3723)  loss_giou_unscaled: 1.5932 (1.5932)  cardinality_error_unscaled: 22.0000 (22.0000)  loss_ce_0_unscaled: 1.7418 (1.7418)  loss_bbox_0_unscaled: 0.3696 (0.3696)  loss_giou_0_unscaled: 1.5777 (1.5777)  cardinality_error_0_unscaled: 22.0000 (22.0000)  loss_ce_1_unscaled: 1.6915 (1.6915)  loss_bbox_1_unscaled: 0.3701 (0.3701)  loss_giou_1_unscaled: 1.5631 (1.5631)  cardinality_error_1_unscaled: 22.0000 (22.0000)  loss_ce_2_unscaled: 1.6977 (1.6977)  loss_bbox_2_unscaled: 0.3790 (0.3790)  loss_giou_2_unscaled: 1.6207 (1.6207)  cardinality_error_2_unscaled: 22.0000 (22.0000)  loss_ce_3_unscaled: 1.7315 (1.7315)  loss_bbox_3_unscaled: 0.3748 (0.3748)  loss_giou_3_unscaled: 1.6052 (1.6052)  cardinality_error_3_unscaled: 22.0000 (22.0000)  loss_ce_4_unscaled: 1.6780 (1.6780)  loss_bbox_4_unscaled: 0.3797 (0.3797)  loss_giou_4_unscaled: 1.6091 (1.6091)  cardinality_error_4_unscaled: 22.0000 (22.0000)  time: 0.4187  data: 0.2240  max mem: 2485\n",
            "Test:  [10/15]  eta: 0:00:01  class_error: 100.00  loss: 37.7532 (38.1524)  loss_ce: 1.4209 (1.4466)  loss_bbox: 1.8162 (1.8440)  loss_giou: 3.0894 (3.0815)  loss_ce_0: 1.4464 (1.4638)  loss_bbox_0: 1.8387 (1.8450)  loss_giou_0: 3.0616 (3.0721)  loss_ce_1: 1.4631 (1.4593)  loss_bbox_1: 1.8506 (1.8449)  loss_giou_1: 3.0308 (3.0436)  loss_ce_2: 1.4511 (1.4552)  loss_bbox_2: 1.7478 (1.8252)  loss_giou_2: 3.0641 (3.0836)  loss_ce_3: 1.4374 (1.4583)  loss_bbox_3: 1.7749 (1.8225)  loss_giou_3: 3.0686 (3.0720)  loss_ce_4: 1.4349 (1.4476)  loss_bbox_4: 1.7590 (1.8272)  loss_giou_4: 3.0361 (3.0601)  loss_ce_unscaled: 1.4209 (1.4466)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3632 (0.3688)  loss_giou_unscaled: 1.5447 (1.5407)  cardinality_error_unscaled: 17.5000 (18.7273)  loss_ce_0_unscaled: 1.4464 (1.4638)  loss_bbox_0_unscaled: 0.3677 (0.3690)  loss_giou_0_unscaled: 1.5308 (1.5361)  cardinality_error_0_unscaled: 17.5000 (18.7273)  loss_ce_1_unscaled: 1.4631 (1.4593)  loss_bbox_1_unscaled: 0.3701 (0.3690)  loss_giou_1_unscaled: 1.5154 (1.5218)  cardinality_error_1_unscaled: 17.5000 (18.7273)  loss_ce_2_unscaled: 1.4511 (1.4552)  loss_bbox_2_unscaled: 0.3496 (0.3650)  loss_giou_2_unscaled: 1.5320 (1.5418)  cardinality_error_2_unscaled: 17.5000 (18.7273)  loss_ce_3_unscaled: 1.4374 (1.4583)  loss_bbox_3_unscaled: 0.3550 (0.3645)  loss_giou_3_unscaled: 1.5343 (1.5360)  cardinality_error_3_unscaled: 17.5000 (18.7273)  loss_ce_4_unscaled: 1.4349 (1.4476)  loss_bbox_4_unscaled: 0.3518 (0.3654)  loss_giou_4_unscaled: 1.5180 (1.5300)  cardinality_error_4_unscaled: 17.5000 (18.7273)  time: 0.2053  data: 0.0307  max mem: 2485\n",
            "Test:  [14/15]  eta: 0:00:00  class_error: 100.00  loss: 37.7532 (37.7497)  loss_ce: 1.4209 (1.4588)  loss_bbox: 1.7930 (1.7868)  loss_giou: 3.0579 (3.0629)  loss_ce_0: 1.4464 (1.4786)  loss_bbox_0: 1.8060 (1.7890)  loss_giou_0: 3.0616 (3.0553)  loss_ce_1: 1.4631 (1.4741)  loss_bbox_1: 1.8098 (1.7886)  loss_giou_1: 3.0288 (3.0250)  loss_ce_2: 1.4511 (1.4694)  loss_bbox_2: 1.7432 (1.7629)  loss_giou_2: 2.9973 (3.0573)  loss_ce_3: 1.4374 (1.4736)  loss_bbox_3: 1.7534 (1.7614)  loss_giou_3: 3.0062 (3.0473)  loss_ce_4: 1.4349 (1.4617)  loss_bbox_4: 1.7491 (1.7649)  loss_giou_4: 2.9703 (3.0322)  loss_ce_unscaled: 1.4209 (1.4588)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3586 (0.3574)  loss_giou_unscaled: 1.5290 (1.5314)  cardinality_error_unscaled: 17.5000 (20.4000)  loss_ce_0_unscaled: 1.4464 (1.4786)  loss_bbox_0_unscaled: 0.3612 (0.3578)  loss_giou_0_unscaled: 1.5308 (1.5277)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.4631 (1.4741)  loss_bbox_1_unscaled: 0.3620 (0.3577)  loss_giou_1_unscaled: 1.5144 (1.5125)  cardinality_error_1_unscaled: 17.5000 (20.4000)  loss_ce_2_unscaled: 1.4511 (1.4694)  loss_bbox_2_unscaled: 0.3486 (0.3526)  loss_giou_2_unscaled: 1.4987 (1.5286)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.4374 (1.4736)  loss_bbox_3_unscaled: 0.3507 (0.3523)  loss_giou_3_unscaled: 1.5031 (1.5236)  cardinality_error_3_unscaled: 17.5000 (20.4000)  loss_ce_4_unscaled: 1.4349 (1.4617)  loss_bbox_4_unscaled: 0.3498 (0.3530)  loss_giou_4_unscaled: 1.4851 (1.5161)  cardinality_error_4_unscaled: 17.5000 (20.4000)  time: 0.2000  data: 0.0251  max mem: 2485\n",
            "Test: Total time: 0:00:03 (0.2037 s / it)\n",
            "Averaged stats: class_error: 100.00  loss: 37.7532 (37.7497)  loss_ce: 1.4209 (1.4588)  loss_bbox: 1.7930 (1.7868)  loss_giou: 3.0579 (3.0629)  loss_ce_0: 1.4464 (1.4786)  loss_bbox_0: 1.8060 (1.7890)  loss_giou_0: 3.0616 (3.0553)  loss_ce_1: 1.4631 (1.4741)  loss_bbox_1: 1.8098 (1.7886)  loss_giou_1: 3.0288 (3.0250)  loss_ce_2: 1.4511 (1.4694)  loss_bbox_2: 1.7432 (1.7629)  loss_giou_2: 2.9973 (3.0573)  loss_ce_3: 1.4374 (1.4736)  loss_bbox_3: 1.7534 (1.7614)  loss_giou_3: 3.0062 (3.0473)  loss_ce_4: 1.4349 (1.4617)  loss_bbox_4: 1.7491 (1.7649)  loss_giou_4: 2.9703 (3.0322)  loss_ce_unscaled: 1.4209 (1.4588)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3586 (0.3574)  loss_giou_unscaled: 1.5290 (1.5314)  cardinality_error_unscaled: 17.5000 (20.4000)  loss_ce_0_unscaled: 1.4464 (1.4786)  loss_bbox_0_unscaled: 0.3612 (0.3578)  loss_giou_0_unscaled: 1.5308 (1.5277)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.4631 (1.4741)  loss_bbox_1_unscaled: 0.3620 (0.3577)  loss_giou_1_unscaled: 1.5144 (1.5125)  cardinality_error_1_unscaled: 17.5000 (20.4000)  loss_ce_2_unscaled: 1.4511 (1.4694)  loss_bbox_2_unscaled: 0.3486 (0.3526)  loss_giou_2_unscaled: 1.4987 (1.5286)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.4374 (1.4736)  loss_bbox_3_unscaled: 0.3507 (0.3523)  loss_giou_3_unscaled: 1.5031 (1.5236)  cardinality_error_3_unscaled: 17.5000 (20.4000)  loss_ce_4_unscaled: 1.4349 (1.4617)  loss_bbox_4_unscaled: 0.3498 (0.3530)  loss_giou_4_unscaled: 1.4851 (1.5161)  cardinality_error_4_unscaled: 17.5000 (20.4000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [1]  [  0/100]  eta: 0:00:49  lr: 0.000100  class_error: 46.15  loss: 35.9810 (35.9810)  loss_ce: 1.0395 (1.0395)  loss_bbox: 2.1315 (2.1315)  loss_giou: 2.7379 (2.7379)  loss_ce_0: 1.0453 (1.0453)  loss_bbox_0: 2.1010 (2.1010)  loss_giou_0: 2.6625 (2.6625)  loss_ce_1: 1.0313 (1.0313)  loss_bbox_1: 2.1804 (2.1804)  loss_giou_1: 2.7739 (2.7739)  loss_ce_2: 1.0299 (1.0299)  loss_bbox_2: 2.1967 (2.1967)  loss_giou_2: 2.8437 (2.8437)  loss_ce_3: 1.0408 (1.0408)  loss_bbox_3: 2.2226 (2.2226)  loss_giou_3: 2.8616 (2.8616)  loss_ce_4: 1.0217 (1.0217)  loss_bbox_4: 2.2056 (2.2056)  loss_giou_4: 2.8552 (2.8552)  loss_ce_unscaled: 1.0395 (1.0395)  class_error_unscaled: 46.1538 (46.1538)  loss_bbox_unscaled: 0.4263 (0.4263)  loss_giou_unscaled: 1.3690 (1.3690)  cardinality_error_unscaled: 39.0000 (39.0000)  loss_ce_0_unscaled: 1.0453 (1.0453)  loss_bbox_0_unscaled: 0.4202 (0.4202)  loss_giou_0_unscaled: 1.3313 (1.3313)  cardinality_error_0_unscaled: 24.0000 (24.0000)  loss_ce_1_unscaled: 1.0313 (1.0313)  loss_bbox_1_unscaled: 0.4361 (0.4361)  loss_giou_1_unscaled: 1.3869 (1.3869)  cardinality_error_1_unscaled: 13.0000 (13.0000)  loss_ce_2_unscaled: 1.0299 (1.0299)  loss_bbox_2_unscaled: 0.4393 (0.4393)  loss_giou_2_unscaled: 1.4219 (1.4219)  cardinality_error_2_unscaled: 17.0000 (17.0000)  loss_ce_3_unscaled: 1.0408 (1.0408)  loss_bbox_3_unscaled: 0.4445 (0.4445)  loss_giou_3_unscaled: 1.4308 (1.4308)  cardinality_error_3_unscaled: 22.0000 (22.0000)  loss_ce_4_unscaled: 1.0217 (1.0217)  loss_bbox_4_unscaled: 0.4411 (0.4411)  loss_giou_4_unscaled: 1.4276 (1.4276)  cardinality_error_4_unscaled: 27.0000 (27.0000)  time: 0.4967  data: 0.1983  max mem: 2485\n",
            "Epoch: [1]  [ 10/100]  eta: 0:00:25  lr: 0.000100  class_error: 73.47  loss: 34.1826 (33.3858)  loss_ce: 1.1396 (1.3074)  loss_bbox: 1.6380 (1.5157)  loss_giou: 2.7379 (2.6211)  loss_ce_0: 1.1297 (1.2857)  loss_bbox_0: 1.5094 (1.6300)  loss_giou_0: 2.8132 (2.7517)  loss_ce_1: 1.1404 (1.2990)  loss_bbox_1: 1.5772 (1.6043)  loss_giou_1: 2.8403 (2.7219)  loss_ce_2: 1.1103 (1.2936)  loss_bbox_2: 1.6714 (1.5389)  loss_giou_2: 2.8437 (2.6713)  loss_ce_3: 1.1125 (1.2767)  loss_bbox_3: 1.7104 (1.5868)  loss_giou_3: 2.8392 (2.7015)  loss_ce_4: 1.1574 (1.3015)  loss_bbox_4: 1.6841 (1.5890)  loss_giou_4: 2.8335 (2.6897)  loss_ce_unscaled: 1.1396 (1.3074)  class_error_unscaled: 46.1538 (48.2659)  loss_bbox_unscaled: 0.3276 (0.3031)  loss_giou_unscaled: 1.3690 (1.3105)  cardinality_error_unscaled: 43.0000 (40.5909)  loss_ce_0_unscaled: 1.1297 (1.2857)  loss_bbox_0_unscaled: 0.3019 (0.3260)  loss_giou_0_unscaled: 1.4066 (1.3759)  cardinality_error_0_unscaled: 18.5000 (23.0455)  loss_ce_1_unscaled: 1.1404 (1.2990)  loss_bbox_1_unscaled: 0.3154 (0.3209)  loss_giou_1_unscaled: 1.4201 (1.3609)  cardinality_error_1_unscaled: 10.0000 (18.8636)  loss_ce_2_unscaled: 1.1103 (1.2936)  loss_bbox_2_unscaled: 0.3343 (0.3078)  loss_giou_2_unscaled: 1.4219 (1.3357)  cardinality_error_2_unscaled: 17.5000 (24.1364)  loss_ce_3_unscaled: 1.1125 (1.2767)  loss_bbox_3_unscaled: 0.3421 (0.3174)  loss_giou_3_unscaled: 1.4196 (1.3508)  cardinality_error_3_unscaled: 34.0000 (38.2273)  loss_ce_4_unscaled: 1.1574 (1.3015)  loss_bbox_4_unscaled: 0.3368 (0.3178)  loss_giou_4_unscaled: 1.4167 (1.3448)  cardinality_error_4_unscaled: 27.0000 (30.5909)  time: 0.2822  data: 0.0274  max mem: 2485\n",
            "Epoch: [1]  [ 20/100]  eta: 0:00:22  lr: 0.000100  class_error: 35.71  loss: 33.2715 (33.5800)  loss_ce: 1.3279 (1.3721)  loss_bbox: 1.4279 (1.4363)  loss_giou: 2.6246 (2.6494)  loss_ce_0: 1.3117 (1.3554)  loss_bbox_0: 1.4721 (1.5853)  loss_giou_0: 2.8132 (2.7844)  loss_ce_1: 1.3337 (1.3612)  loss_bbox_1: 1.4551 (1.5345)  loss_giou_1: 2.7650 (2.7370)  loss_ce_2: 1.2933 (1.3581)  loss_bbox_2: 1.4801 (1.4804)  loss_giou_2: 2.6710 (2.7032)  loss_ce_3: 1.3367 (1.3500)  loss_bbox_3: 1.4442 (1.5261)  loss_giou_3: 2.7103 (2.7350)  loss_ce_4: 1.3197 (1.3634)  loss_bbox_4: 1.4362 (1.5196)  loss_giou_4: 2.7184 (2.7285)  loss_ce_unscaled: 1.3279 (1.3721)  class_error_unscaled: 47.3684 (52.2613)  loss_bbox_unscaled: 0.2856 (0.2873)  loss_giou_unscaled: 1.3123 (1.3247)  cardinality_error_unscaled: 43.0000 (41.9524)  loss_ce_0_unscaled: 1.3117 (1.3554)  loss_bbox_0_unscaled: 0.2944 (0.3171)  loss_giou_0_unscaled: 1.4066 (1.3922)  cardinality_error_0_unscaled: 34.0000 (34.0714)  loss_ce_1_unscaled: 1.3337 (1.3612)  loss_bbox_1_unscaled: 0.2910 (0.3069)  loss_giou_1_unscaled: 1.3825 (1.3685)  cardinality_error_1_unscaled: 36.5000 (38.0476)  loss_ce_2_unscaled: 1.2933 (1.3581)  loss_bbox_2_unscaled: 0.2960 (0.2961)  loss_giou_2_unscaled: 1.3355 (1.3516)  cardinality_error_2_unscaled: 39.5000 (34.7619)  loss_ce_3_unscaled: 1.3367 (1.3500)  loss_bbox_3_unscaled: 0.2888 (0.3052)  loss_giou_3_unscaled: 1.3552 (1.3675)  cardinality_error_3_unscaled: 42.5000 (44.6905)  loss_ce_4_unscaled: 1.3197 (1.3634)  loss_bbox_4_unscaled: 0.2872 (0.3039)  loss_giou_4_unscaled: 1.3592 (1.3643)  cardinality_error_4_unscaled: 42.0000 (41.2857)  time: 0.2693  data: 0.0106  max mem: 2485\n",
            "Epoch: [1]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 25.00  loss: 31.9227 (32.7077)  loss_ce: 1.3179 (1.3520)  loss_bbox: 1.1938 (1.3448)  loss_giou: 2.4839 (2.5564)  loss_ce_0: 1.2992 (1.3204)  loss_bbox_0: 1.4922 (1.5633)  loss_giou_0: 2.7745 (2.7909)  loss_ce_1: 1.2879 (1.3316)  loss_bbox_1: 1.3358 (1.4607)  loss_giou_1: 2.5980 (2.6744)  loss_ce_2: 1.2914 (1.3341)  loss_bbox_2: 1.3320 (1.4251)  loss_giou_2: 2.6227 (2.6402)  loss_ce_3: 1.2878 (1.3208)  loss_bbox_3: 1.3845 (1.4616)  loss_giou_3: 2.6792 (2.6859)  loss_ce_4: 1.2793 (1.3359)  loss_bbox_4: 1.3300 (1.4433)  loss_giou_4: 2.6426 (2.6661)  loss_ce_unscaled: 1.3179 (1.3520)  class_error_unscaled: 45.4545 (49.9175)  loss_bbox_unscaled: 0.2388 (0.2690)  loss_giou_unscaled: 1.2419 (1.2782)  cardinality_error_unscaled: 66.0000 (53.1613)  loss_ce_0_unscaled: 1.2992 (1.3204)  loss_bbox_0_unscaled: 0.2984 (0.3127)  loss_giou_0_unscaled: 1.3872 (1.3954)  cardinality_error_0_unscaled: 67.5000 (47.1452)  loss_ce_1_unscaled: 1.2879 (1.3316)  loss_bbox_1_unscaled: 0.2672 (0.2921)  loss_giou_1_unscaled: 1.2990 (1.3372)  cardinality_error_1_unscaled: 72.0000 (51.1129)  loss_ce_2_unscaled: 1.2914 (1.3341)  loss_bbox_2_unscaled: 0.2664 (0.2850)  loss_giou_2_unscaled: 1.3114 (1.3201)  cardinality_error_2_unscaled: 69.5000 (48.8065)  loss_ce_3_unscaled: 1.2878 (1.3208)  loss_bbox_3_unscaled: 0.2769 (0.2923)  loss_giou_3_unscaled: 1.3396 (1.3430)  cardinality_error_3_unscaled: 62.0000 (53.5968)  loss_ce_4_unscaled: 1.2793 (1.3359)  loss_bbox_4_unscaled: 0.2660 (0.2887)  loss_giou_4_unscaled: 1.3213 (1.3331)  cardinality_error_4_unscaled: 71.0000 (51.2581)  time: 0.2738  data: 0.0106  max mem: 2485\n",
            "Epoch: [1]  [ 40/100]  eta: 0:00:17  lr: 0.000100  class_error: 72.97  loss: 31.4579 (32.8484)  loss_ce: 1.2467 (1.3296)  loss_bbox: 1.1938 (1.3480)  loss_giou: 2.3613 (2.5385)  loss_ce_0: 1.1636 (1.3045)  loss_bbox_0: 1.5466 (1.5801)  loss_giou_0: 2.8614 (2.7919)  loss_ce_1: 1.1836 (1.3097)  loss_bbox_1: 1.3552 (1.4871)  loss_giou_1: 2.5958 (2.6806)  loss_ce_2: 1.2380 (1.3099)  loss_bbox_2: 1.3320 (1.4695)  loss_giou_2: 2.5805 (2.6637)  loss_ce_3: 1.1872 (1.3006)  loss_bbox_3: 1.3891 (1.5194)  loss_giou_3: 2.6226 (2.7226)  loss_ce_4: 1.2167 (1.3119)  loss_bbox_4: 1.3385 (1.4862)  loss_giou_4: 2.6030 (2.6945)  loss_ce_unscaled: 1.2467 (1.3296)  class_error_unscaled: 32.6531 (46.2989)  loss_bbox_unscaled: 0.2388 (0.2696)  loss_giou_unscaled: 1.1806 (1.2692)  cardinality_error_unscaled: 73.5000 (55.5244)  loss_ce_0_unscaled: 1.1636 (1.3045)  loss_bbox_0_unscaled: 0.3093 (0.3160)  loss_giou_0_unscaled: 1.4307 (1.3959)  cardinality_error_0_unscaled: 76.0000 (55.6098)  loss_ce_1_unscaled: 1.1836 (1.3097)  loss_bbox_1_unscaled: 0.2710 (0.2974)  loss_giou_1_unscaled: 1.2979 (1.3403)  cardinality_error_1_unscaled: 78.5000 (56.5000)  loss_ce_2_unscaled: 1.2380 (1.3099)  loss_bbox_2_unscaled: 0.2664 (0.2939)  loss_giou_2_unscaled: 1.2903 (1.3319)  cardinality_error_2_unscaled: 77.0000 (54.6951)  loss_ce_3_unscaled: 1.1872 (1.3006)  loss_bbox_3_unscaled: 0.2778 (0.3039)  loss_giou_3_unscaled: 1.3113 (1.3613)  cardinality_error_3_unscaled: 78.5000 (60.0610)  loss_ce_4_unscaled: 1.2167 (1.3119)  loss_bbox_4_unscaled: 0.2677 (0.2972)  loss_giou_4_unscaled: 1.3015 (1.3473)  cardinality_error_4_unscaled: 73.5000 (56.0488)  time: 0.2879  data: 0.0115  max mem: 2666\n",
            "Epoch: [1]  [ 50/100]  eta: 0:00:14  lr: 0.000100  class_error: 66.67  loss: 30.9154 (32.1934)  loss_ce: 1.1403 (1.3212)  loss_bbox: 1.1446 (1.2979)  loss_giou: 2.2422 (2.4604)  loss_ce_0: 1.1542 (1.2958)  loss_bbox_0: 1.4416 (1.5300)  loss_giou_0: 2.4972 (2.7321)  loss_ce_1: 1.1439 (1.2977)  loss_bbox_1: 1.3239 (1.4255)  loss_giou_1: 2.5487 (2.5998)  loss_ce_2: 1.1388 (1.3018)  loss_bbox_2: 1.4592 (1.4388)  loss_giou_2: 2.5018 (2.6080)  loss_ce_3: 1.1031 (1.2898)  loss_bbox_3: 1.5181 (1.5092)  loss_giou_3: 2.5883 (2.6930)  loss_ce_4: 1.1263 (1.2986)  loss_bbox_4: 1.5068 (1.4533)  loss_giou_4: 2.6134 (2.6405)  loss_ce_unscaled: 1.1403 (1.3212)  class_error_unscaled: 36.8421 (48.4447)  loss_bbox_unscaled: 0.2289 (0.2596)  loss_giou_unscaled: 1.1211 (1.2302)  cardinality_error_unscaled: 62.5000 (51.3529)  loss_ce_0_unscaled: 1.1542 (1.2958)  loss_bbox_0_unscaled: 0.2883 (0.3060)  loss_giou_0_unscaled: 1.2486 (1.3660)  cardinality_error_0_unscaled: 65.0000 (53.3333)  loss_ce_1_unscaled: 1.1439 (1.2977)  loss_bbox_1_unscaled: 0.2648 (0.2851)  loss_giou_1_unscaled: 1.2744 (1.2999)  cardinality_error_1_unscaled: 35.0000 (48.5196)  loss_ce_2_unscaled: 1.1388 (1.3018)  loss_bbox_2_unscaled: 0.2918 (0.2878)  loss_giou_2_unscaled: 1.2509 (1.3040)  cardinality_error_2_unscaled: 61.0000 (52.8235)  loss_ce_3_unscaled: 1.1031 (1.2898)  loss_bbox_3_unscaled: 0.3036 (0.3018)  loss_giou_3_unscaled: 1.2942 (1.3465)  cardinality_error_3_unscaled: 62.5000 (58.2941)  loss_ce_4_unscaled: 1.1263 (1.2986)  loss_bbox_4_unscaled: 0.3014 (0.2907)  loss_giou_4_unscaled: 1.3067 (1.3203)  cardinality_error_4_unscaled: 60.5000 (53.6078)  time: 0.2888  data: 0.0123  max mem: 2666\n",
            "Epoch: [1]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 50.00  loss: 28.2026 (31.4707)  loss_ce: 1.2832 (1.3255)  loss_bbox: 1.1107 (1.2529)  loss_giou: 2.2145 (2.4181)  loss_ce_0: 1.1860 (1.2936)  loss_bbox_0: 1.2134 (1.4611)  loss_giou_0: 2.4195 (2.6614)  loss_ce_1: 1.1601 (1.2982)  loss_bbox_1: 1.0819 (1.3587)  loss_giou_1: 2.2587 (2.5403)  loss_ce_2: 1.1996 (1.3022)  loss_bbox_2: 1.0660 (1.3627)  loss_giou_2: 2.2554 (2.5445)  loss_ce_3: 1.1751 (1.2906)  loss_bbox_3: 1.2943 (1.4620)  loss_giou_3: 2.4069 (2.6553)  loss_ce_4: 1.2075 (1.2977)  loss_bbox_4: 1.1267 (1.3793)  loss_giou_4: 2.2994 (2.5665)  loss_ce_unscaled: 1.2832 (1.3255)  class_error_unscaled: 45.8333 (48.1302)  loss_bbox_unscaled: 0.2221 (0.2506)  loss_giou_unscaled: 1.1072 (1.2091)  cardinality_error_unscaled: 58.5000 (53.0164)  loss_ce_0_unscaled: 1.1860 (1.2936)  loss_bbox_0_unscaled: 0.2427 (0.2922)  loss_giou_0_unscaled: 1.2098 (1.3307)  cardinality_error_0_unscaled: 49.0000 (54.2377)  loss_ce_1_unscaled: 1.1601 (1.2982)  loss_bbox_1_unscaled: 0.2164 (0.2717)  loss_giou_1_unscaled: 1.1294 (1.2701)  cardinality_error_1_unscaled: 35.0000 (50.9590)  loss_ce_2_unscaled: 1.1996 (1.3022)  loss_bbox_2_unscaled: 0.2132 (0.2725)  loss_giou_2_unscaled: 1.1277 (1.2723)  cardinality_error_2_unscaled: 41.5000 (51.3279)  loss_ce_3_unscaled: 1.1751 (1.2906)  loss_bbox_3_unscaled: 0.2589 (0.2924)  loss_giou_3_unscaled: 1.2034 (1.3276)  cardinality_error_3_unscaled: 47.0000 (54.6311)  loss_ce_4_unscaled: 1.2075 (1.2977)  loss_bbox_4_unscaled: 0.2253 (0.2759)  loss_giou_4_unscaled: 1.1497 (1.2833)  cardinality_error_4_unscaled: 33.5000 (52.1803)  time: 0.2694  data: 0.0118  max mem: 2666\n",
            "Epoch: [1]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 65.12  loss: 28.1183 (30.7879)  loss_ce: 1.3512 (1.3388)  loss_bbox: 0.9671 (1.1968)  loss_giou: 2.1854 (2.3550)  loss_ce_0: 1.3250 (1.3044)  loss_bbox_0: 1.0637 (1.3928)  loss_giou_0: 2.2536 (2.5834)  loss_ce_1: 1.3313 (1.3114)  loss_bbox_1: 1.0777 (1.3117)  loss_giou_1: 2.2587 (2.4914)  loss_ce_2: 1.2995 (1.3148)  loss_bbox_2: 0.9624 (1.3033)  loss_giou_2: 2.1508 (2.4754)  loss_ce_3: 1.2896 (1.3023)  loss_bbox_3: 1.1372 (1.4015)  loss_giou_3: 2.3152 (2.5952)  loss_ce_4: 1.2866 (1.3099)  loss_bbox_4: 0.9853 (1.3105)  loss_giou_4: 2.1438 (2.4893)  loss_ce_unscaled: 1.3512 (1.3388)  class_error_unscaled: 45.8333 (47.9035)  loss_bbox_unscaled: 0.1934 (0.2394)  loss_giou_unscaled: 1.0927 (1.1775)  cardinality_error_unscaled: 59.5000 (54.0070)  loss_ce_0_unscaled: 1.3250 (1.3044)  loss_bbox_0_unscaled: 0.2127 (0.2786)  loss_giou_0_unscaled: 1.1268 (1.2917)  cardinality_error_0_unscaled: 73.5000 (56.2324)  loss_ce_1_unscaled: 1.3313 (1.3114)  loss_bbox_1_unscaled: 0.2155 (0.2623)  loss_giou_1_unscaled: 1.1294 (1.2457)  cardinality_error_1_unscaled: 64.0000 (51.1197)  loss_ce_2_unscaled: 1.2995 (1.3148)  loss_bbox_2_unscaled: 0.1925 (0.2607)  loss_giou_2_unscaled: 1.0754 (1.2377)  cardinality_error_2_unscaled: 58.5000 (53.2394)  loss_ce_3_unscaled: 1.2896 (1.3023)  loss_bbox_3_unscaled: 0.2274 (0.2803)  loss_giou_3_unscaled: 1.1576 (1.2976)  cardinality_error_3_unscaled: 66.5000 (57.3028)  loss_ce_4_unscaled: 1.2866 (1.3099)  loss_bbox_4_unscaled: 0.1971 (0.2621)  loss_giou_4_unscaled: 1.0719 (1.2447)  cardinality_error_4_unscaled: 54.5000 (53.3380)  time: 0.2748  data: 0.0114  max mem: 2666\n",
            "Epoch: [1]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 92.06  loss: 26.5145 (30.2619)  loss_ce: 1.3567 (1.3442)  loss_bbox: 0.8795 (1.1742)  loss_giou: 1.9253 (2.3179)  loss_ce_0: 1.3250 (1.3061)  loss_bbox_0: 0.9216 (1.3449)  loss_giou_0: 1.9615 (2.5092)  loss_ce_1: 1.3313 (1.3181)  loss_bbox_1: 1.1020 (1.2850)  loss_giou_1: 2.0122 (2.4380)  loss_ce_2: 1.3602 (1.3225)  loss_bbox_2: 0.9479 (1.2680)  loss_giou_2: 1.9254 (2.4107)  loss_ce_3: 1.2961 (1.3052)  loss_bbox_3: 1.0088 (1.3598)  loss_giou_3: 2.0819 (2.5325)  loss_ce_4: 1.3050 (1.3163)  loss_bbox_4: 0.9072 (1.2707)  loss_giou_4: 2.0435 (2.4387)  loss_ce_unscaled: 1.3567 (1.3442)  class_error_unscaled: 65.2174 (52.3063)  loss_bbox_unscaled: 0.1759 (0.2348)  loss_giou_unscaled: 0.9627 (1.1590)  cardinality_error_unscaled: 29.0000 (49.1975)  loss_ce_0_unscaled: 1.3250 (1.3061)  loss_bbox_0_unscaled: 0.1843 (0.2690)  loss_giou_0_unscaled: 0.9808 (1.2546)  cardinality_error_0_unscaled: 20.0000 (50.7284)  loss_ce_1_unscaled: 1.3313 (1.3181)  loss_bbox_1_unscaled: 0.2204 (0.2570)  loss_giou_1_unscaled: 1.0061 (1.2190)  cardinality_error_1_unscaled: 19.0000 (45.9630)  loss_ce_2_unscaled: 1.3602 (1.3225)  loss_bbox_2_unscaled: 0.1896 (0.2536)  loss_giou_2_unscaled: 0.9627 (1.2054)  cardinality_error_2_unscaled: 13.5000 (47.9074)  loss_ce_3_unscaled: 1.2961 (1.3052)  loss_bbox_3_unscaled: 0.2018 (0.2720)  loss_giou_3_unscaled: 1.0410 (1.2662)  cardinality_error_3_unscaled: 21.5000 (51.5802)  loss_ce_4_unscaled: 1.3050 (1.3163)  loss_bbox_4_unscaled: 0.1814 (0.2541)  loss_giou_4_unscaled: 1.0217 (1.2193)  cardinality_error_4_unscaled: 18.5000 (48.1481)  time: 0.2689  data: 0.0115  max mem: 2666\n",
            "Epoch: [1]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 44.00  loss: 25.1844 (29.6853)  loss_ce: 1.2597 (1.3363)  loss_bbox: 0.9107 (1.1455)  loss_giou: 2.0131 (2.2893)  loss_ce_0: 1.2421 (1.3048)  loss_bbox_0: 0.8834 (1.2974)  loss_giou_0: 1.9615 (2.4546)  loss_ce_1: 1.2727 (1.3149)  loss_bbox_1: 0.9151 (1.2379)  loss_giou_1: 2.0122 (2.3846)  loss_ce_2: 1.2829 (1.3229)  loss_bbox_2: 0.9474 (1.2281)  loss_giou_2: 1.8513 (2.3598)  loss_ce_3: 1.2159 (1.3007)  loss_bbox_3: 0.9232 (1.3063)  loss_giou_3: 2.0677 (2.4718)  loss_ce_4: 1.2436 (1.3114)  loss_bbox_4: 0.8813 (1.2264)  loss_giou_4: 2.0435 (2.3928)  loss_ce_unscaled: 1.2597 (1.3363)  class_error_unscaled: 73.6842 (53.0261)  loss_bbox_unscaled: 0.1821 (0.2291)  loss_giou_unscaled: 1.0065 (1.1446)  cardinality_error_unscaled: 15.0000 (46.2692)  loss_ce_0_unscaled: 1.2421 (1.3048)  loss_bbox_0_unscaled: 0.1767 (0.2595)  loss_giou_0_unscaled: 0.9808 (1.2273)  cardinality_error_0_unscaled: 12.5000 (48.0385)  loss_ce_1_unscaled: 1.2727 (1.3149)  loss_bbox_1_unscaled: 0.1830 (0.2476)  loss_giou_1_unscaled: 1.0061 (1.1923)  cardinality_error_1_unscaled: 9.0000 (43.1758)  loss_ce_2_unscaled: 1.2829 (1.3229)  loss_bbox_2_unscaled: 0.1895 (0.2456)  loss_giou_2_unscaled: 0.9257 (1.1799)  cardinality_error_2_unscaled: 13.5000 (48.4451)  loss_ce_3_unscaled: 1.2159 (1.3007)  loss_bbox_3_unscaled: 0.1846 (0.2613)  loss_giou_3_unscaled: 1.0338 (1.2359)  cardinality_error_3_unscaled: 12.5000 (49.3681)  loss_ce_4_unscaled: 1.2436 (1.3114)  loss_bbox_4_unscaled: 0.1763 (0.2453)  loss_giou_4_unscaled: 1.0217 (1.1964)  cardinality_error_4_unscaled: 13.0000 (46.9615)  time: 0.2533  data: 0.0114  max mem: 2666\n",
            "Epoch: [1]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 44.44  loss: 25.1844 (29.2714)  loss_ce: 1.2152 (1.3333)  loss_bbox: 0.8635 (1.1187)  loss_giou: 2.0456 (2.2747)  loss_ce_0: 1.2442 (1.3060)  loss_bbox_0: 0.7763 (1.2505)  loss_giou_0: 2.0167 (2.4176)  loss_ce_1: 1.2433 (1.3178)  loss_bbox_1: 0.7526 (1.1992)  loss_giou_1: 2.0371 (2.3518)  loss_ce_2: 1.2897 (1.3259)  loss_bbox_2: 0.8342 (1.1913)  loss_giou_2: 1.8513 (2.3272)  loss_ce_3: 1.2038 (1.3005)  loss_bbox_3: 0.8364 (1.2604)  loss_giou_3: 1.9783 (2.4294)  loss_ce_4: 1.2159 (1.3112)  loss_bbox_4: 0.8809 (1.1917)  loss_giou_4: 2.0072 (2.3643)  loss_ce_unscaled: 1.2152 (1.3333)  class_error_unscaled: 50.0000 (52.5489)  loss_bbox_unscaled: 0.1727 (0.2237)  loss_giou_unscaled: 1.0228 (1.1373)  cardinality_error_unscaled: 38.0000 (47.5550)  loss_ce_0_unscaled: 1.2442 (1.3060)  loss_bbox_0_unscaled: 0.1553 (0.2501)  loss_giou_0_unscaled: 1.0083 (1.2088)  cardinality_error_0_unscaled: 34.0000 (48.3300)  loss_ce_1_unscaled: 1.2433 (1.3178)  loss_bbox_1_unscaled: 0.1505 (0.2398)  loss_giou_1_unscaled: 1.0185 (1.1759)  cardinality_error_1_unscaled: 35.5000 (44.0400)  loss_ce_2_unscaled: 1.2897 (1.3259)  loss_bbox_2_unscaled: 0.1668 (0.2383)  loss_giou_2_unscaled: 0.9257 (1.1636)  cardinality_error_2_unscaled: 65.0000 (49.8550)  loss_ce_3_unscaled: 1.2038 (1.3005)  loss_bbox_3_unscaled: 0.1673 (0.2521)  loss_giou_3_unscaled: 0.9892 (1.2147)  cardinality_error_3_unscaled: 46.5000 (49.8350)  loss_ce_4_unscaled: 1.2159 (1.3112)  loss_bbox_4_unscaled: 0.1762 (0.2383)  loss_giou_4_unscaled: 1.0036 (1.1822)  cardinality_error_4_unscaled: 60.0000 (48.5350)  time: 0.2563  data: 0.0109  max mem: 2666\n",
            "Epoch: [1] Total time: 0:00:27 (0.2739 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 44.44  loss: 25.1844 (29.2714)  loss_ce: 1.2152 (1.3333)  loss_bbox: 0.8635 (1.1187)  loss_giou: 2.0456 (2.2747)  loss_ce_0: 1.2442 (1.3060)  loss_bbox_0: 0.7763 (1.2505)  loss_giou_0: 2.0167 (2.4176)  loss_ce_1: 1.2433 (1.3178)  loss_bbox_1: 0.7526 (1.1992)  loss_giou_1: 2.0371 (2.3518)  loss_ce_2: 1.2897 (1.3259)  loss_bbox_2: 0.8342 (1.1913)  loss_giou_2: 1.8513 (2.3272)  loss_ce_3: 1.2038 (1.3005)  loss_bbox_3: 0.8364 (1.2604)  loss_giou_3: 1.9783 (2.4294)  loss_ce_4: 1.2159 (1.3112)  loss_bbox_4: 0.8809 (1.1917)  loss_giou_4: 2.0072 (2.3643)  loss_ce_unscaled: 1.2152 (1.3333)  class_error_unscaled: 50.0000 (52.5489)  loss_bbox_unscaled: 0.1727 (0.2237)  loss_giou_unscaled: 1.0228 (1.1373)  cardinality_error_unscaled: 38.0000 (47.5550)  loss_ce_0_unscaled: 1.2442 (1.3060)  loss_bbox_0_unscaled: 0.1553 (0.2501)  loss_giou_0_unscaled: 1.0083 (1.2088)  cardinality_error_0_unscaled: 34.0000 (48.3300)  loss_ce_1_unscaled: 1.2433 (1.3178)  loss_bbox_1_unscaled: 0.1505 (0.2398)  loss_giou_1_unscaled: 1.0185 (1.1759)  cardinality_error_1_unscaled: 35.5000 (44.0400)  loss_ce_2_unscaled: 1.2897 (1.3259)  loss_bbox_2_unscaled: 0.1668 (0.2383)  loss_giou_2_unscaled: 0.9257 (1.1636)  cardinality_error_2_unscaled: 65.0000 (49.8550)  loss_ce_3_unscaled: 1.2038 (1.3005)  loss_bbox_3_unscaled: 0.1673 (0.2521)  loss_giou_3_unscaled: 0.9892 (1.2147)  cardinality_error_3_unscaled: 46.5000 (49.8350)  loss_ce_4_unscaled: 1.2159 (1.3112)  loss_bbox_4_unscaled: 0.1762 (0.2383)  loss_giou_4_unscaled: 1.0036 (1.1822)  cardinality_error_4_unscaled: 60.0000 (48.5350)\n",
            "Test:  [ 0/15]  eta: 0:00:06  class_error: 100.00  loss: 41.1433 (41.1433)  loss_ce: 1.5086 (1.5086)  loss_bbox: 1.9406 (1.9406)  loss_giou: 3.5178 (3.5178)  loss_ce_0: 1.5224 (1.5224)  loss_bbox_0: 2.0529 (2.0529)  loss_giou_0: 3.4462 (3.4462)  loss_ce_1: 1.5540 (1.5540)  loss_bbox_1: 1.9344 (1.9344)  loss_giou_1: 3.4555 (3.4555)  loss_ce_2: 1.4484 (1.4484)  loss_bbox_2: 1.7192 (1.7192)  loss_giou_2: 3.4500 (3.4500)  loss_ce_3: 1.4591 (1.4591)  loss_bbox_3: 1.6368 (1.6368)  loss_giou_3: 3.3018 (3.3018)  loss_ce_4: 1.4985 (1.4985)  loss_bbox_4: 2.2611 (2.2611)  loss_giou_4: 3.4361 (3.4361)  loss_ce_unscaled: 1.5086 (1.5086)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3881 (0.3881)  loss_giou_unscaled: 1.7589 (1.7589)  cardinality_error_unscaled: 22.0000 (22.0000)  loss_ce_0_unscaled: 1.5224 (1.5224)  loss_bbox_0_unscaled: 0.4106 (0.4106)  loss_giou_0_unscaled: 1.7231 (1.7231)  cardinality_error_0_unscaled: 22.0000 (22.0000)  loss_ce_1_unscaled: 1.5540 (1.5540)  loss_bbox_1_unscaled: 0.3869 (0.3869)  loss_giou_1_unscaled: 1.7278 (1.7278)  cardinality_error_1_unscaled: 22.0000 (22.0000)  loss_ce_2_unscaled: 1.4484 (1.4484)  loss_bbox_2_unscaled: 0.3438 (0.3438)  loss_giou_2_unscaled: 1.7250 (1.7250)  cardinality_error_2_unscaled: 22.0000 (22.0000)  loss_ce_3_unscaled: 1.4591 (1.4591)  loss_bbox_3_unscaled: 0.3274 (0.3274)  loss_giou_3_unscaled: 1.6509 (1.6509)  cardinality_error_3_unscaled: 22.0000 (22.0000)  loss_ce_4_unscaled: 1.4985 (1.4985)  loss_bbox_4_unscaled: 0.4522 (0.4522)  loss_giou_4_unscaled: 1.7180 (1.7180)  cardinality_error_4_unscaled: 22.0000 (22.0000)  time: 0.4013  data: 0.2276  max mem: 2666\n",
            "Test:  [10/15]  eta: 0:00:01  class_error: 100.00  loss: 40.2852 (39.5032)  loss_ce: 1.4088 (1.4087)  loss_bbox: 1.6381 (1.7168)  loss_giou: 3.2788 (3.3390)  loss_ce_0: 1.4155 (1.4096)  loss_bbox_0: 1.7619 (1.7901)  loss_giou_0: 3.3352 (3.3360)  loss_ce_1: 1.4234 (1.4205)  loss_bbox_1: 1.6303 (1.7054)  loss_giou_1: 3.2164 (3.2941)  loss_ce_2: 1.4477 (1.4430)  loss_bbox_2: 1.6234 (1.6862)  loss_giou_2: 3.3455 (3.3498)  loss_ce_3: 1.4275 (1.4394)  loss_bbox_3: 1.9899 (1.9810)  loss_giou_3: 3.3463 (3.3920)  loss_ce_4: 1.4129 (1.4198)  loss_bbox_4: 1.9194 (1.9424)  loss_giou_4: 3.4361 (3.4294)  loss_ce_unscaled: 1.4088 (1.4087)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3276 (0.3434)  loss_giou_unscaled: 1.6394 (1.6695)  cardinality_error_unscaled: 17.5000 (18.7273)  loss_ce_0_unscaled: 1.4155 (1.4096)  loss_bbox_0_unscaled: 0.3524 (0.3580)  loss_giou_0_unscaled: 1.6676 (1.6680)  cardinality_error_0_unscaled: 17.5000 (18.7273)  loss_ce_1_unscaled: 1.4234 (1.4205)  loss_bbox_1_unscaled: 0.3261 (0.3411)  loss_giou_1_unscaled: 1.6082 (1.6470)  cardinality_error_1_unscaled: 17.5000 (18.7273)  loss_ce_2_unscaled: 1.4477 (1.4430)  loss_bbox_2_unscaled: 0.3247 (0.3372)  loss_giou_2_unscaled: 1.6727 (1.6749)  cardinality_error_2_unscaled: 17.5000 (18.7273)  loss_ce_3_unscaled: 1.4275 (1.4394)  loss_bbox_3_unscaled: 0.3980 (0.3962)  loss_giou_3_unscaled: 1.6731 (1.6960)  cardinality_error_3_unscaled: 17.5000 (18.7273)  loss_ce_4_unscaled: 1.4129 (1.4198)  loss_bbox_4_unscaled: 0.3839 (0.3885)  loss_giou_4_unscaled: 1.7180 (1.7147)  cardinality_error_4_unscaled: 17.5000 (18.7273)  time: 0.2061  data: 0.0306  max mem: 2666\n",
            "Test:  [14/15]  eta: 0:00:00  class_error: 100.00  loss: 39.0219 (39.0290)  loss_ce: 1.4088 (1.4147)  loss_bbox: 1.5707 (1.6408)  loss_giou: 3.2788 (3.3180)  loss_ce_0: 1.4155 (1.4171)  loss_bbox_0: 1.6306 (1.7098)  loss_giou_0: 3.2992 (3.3085)  loss_ce_1: 1.4234 (1.4283)  loss_bbox_1: 1.5858 (1.6278)  loss_giou_1: 3.2164 (3.2695)  loss_ce_2: 1.4477 (1.4499)  loss_bbox_2: 1.6175 (1.6243)  loss_giou_2: 3.3297 (3.3292)  loss_ce_3: 1.4275 (1.4509)  loss_bbox_3: 1.9685 (1.9558)  loss_giou_3: 3.3886 (3.4044)  loss_ce_4: 1.4129 (1.4286)  loss_bbox_4: 1.7969 (1.8600)  loss_giou_4: 3.4130 (3.3913)  loss_ce_unscaled: 1.4088 (1.4147)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3141 (0.3282)  loss_giou_unscaled: 1.6394 (1.6590)  cardinality_error_unscaled: 17.5000 (20.4000)  loss_ce_0_unscaled: 1.4155 (1.4171)  loss_bbox_0_unscaled: 0.3261 (0.3420)  loss_giou_0_unscaled: 1.6496 (1.6543)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.4234 (1.4283)  loss_bbox_1_unscaled: 0.3172 (0.3256)  loss_giou_1_unscaled: 1.6082 (1.6348)  cardinality_error_1_unscaled: 17.5000 (20.4000)  loss_ce_2_unscaled: 1.4477 (1.4499)  loss_bbox_2_unscaled: 0.3235 (0.3249)  loss_giou_2_unscaled: 1.6649 (1.6646)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.4275 (1.4509)  loss_bbox_3_unscaled: 0.3937 (0.3912)  loss_giou_3_unscaled: 1.6943 (1.7022)  cardinality_error_3_unscaled: 17.5000 (20.4000)  loss_ce_4_unscaled: 1.4129 (1.4286)  loss_bbox_4_unscaled: 0.3594 (0.3720)  loss_giou_4_unscaled: 1.7065 (1.6957)  cardinality_error_4_unscaled: 17.5000 (20.4000)  time: 0.2090  data: 0.0251  max mem: 2666\n",
            "Test: Total time: 0:00:03 (0.2134 s / it)\n",
            "Averaged stats: class_error: 100.00  loss: 39.0219 (39.0290)  loss_ce: 1.4088 (1.4147)  loss_bbox: 1.5707 (1.6408)  loss_giou: 3.2788 (3.3180)  loss_ce_0: 1.4155 (1.4171)  loss_bbox_0: 1.6306 (1.7098)  loss_giou_0: 3.2992 (3.3085)  loss_ce_1: 1.4234 (1.4283)  loss_bbox_1: 1.5858 (1.6278)  loss_giou_1: 3.2164 (3.2695)  loss_ce_2: 1.4477 (1.4499)  loss_bbox_2: 1.6175 (1.6243)  loss_giou_2: 3.3297 (3.3292)  loss_ce_3: 1.4275 (1.4509)  loss_bbox_3: 1.9685 (1.9558)  loss_giou_3: 3.3886 (3.4044)  loss_ce_4: 1.4129 (1.4286)  loss_bbox_4: 1.7969 (1.8600)  loss_giou_4: 3.4130 (3.3913)  loss_ce_unscaled: 1.4088 (1.4147)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3141 (0.3282)  loss_giou_unscaled: 1.6394 (1.6590)  cardinality_error_unscaled: 17.5000 (20.4000)  loss_ce_0_unscaled: 1.4155 (1.4171)  loss_bbox_0_unscaled: 0.3261 (0.3420)  loss_giou_0_unscaled: 1.6496 (1.6543)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.4234 (1.4283)  loss_bbox_1_unscaled: 0.3172 (0.3256)  loss_giou_1_unscaled: 1.6082 (1.6348)  cardinality_error_1_unscaled: 17.5000 (20.4000)  loss_ce_2_unscaled: 1.4477 (1.4499)  loss_bbox_2_unscaled: 0.3235 (0.3249)  loss_giou_2_unscaled: 1.6649 (1.6646)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.4275 (1.4509)  loss_bbox_3_unscaled: 0.3937 (0.3912)  loss_giou_3_unscaled: 1.6943 (1.7022)  cardinality_error_3_unscaled: 17.5000 (20.4000)  loss_ce_4_unscaled: 1.4129 (1.4286)  loss_bbox_4_unscaled: 0.3594 (0.3720)  loss_giou_4_unscaled: 1.7065 (1.6957)  cardinality_error_4_unscaled: 17.5000 (20.4000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [2]  [  0/100]  eta: 0:00:46  lr: 0.000100  class_error: 50.00  loss: 23.8710 (23.8710)  loss_ce: 1.0977 (1.0977)  loss_bbox: 0.9926 (0.9926)  loss_giou: 1.6395 (1.6395)  loss_ce_0: 1.1014 (1.1014)  loss_bbox_0: 0.9905 (0.9905)  loss_giou_0: 1.7603 (1.7603)  loss_ce_1: 1.1338 (1.1338)  loss_bbox_1: 1.0215 (1.0215)  loss_giou_1: 1.7060 (1.7060)  loss_ce_2: 1.1260 (1.1260)  loss_bbox_2: 1.0096 (1.0096)  loss_giou_2: 1.7030 (1.7030)  loss_ce_3: 1.0853 (1.0853)  loss_bbox_3: 1.2559 (1.2559)  loss_giou_3: 2.3212 (2.3212)  loss_ce_4: 1.1462 (1.1462)  loss_bbox_4: 1.0239 (1.0239)  loss_giou_4: 1.7568 (1.7568)  loss_ce_unscaled: 1.0977 (1.0977)  class_error_unscaled: 50.0000 (50.0000)  loss_bbox_unscaled: 0.1985 (0.1985)  loss_giou_unscaled: 0.8197 (0.8197)  cardinality_error_unscaled: 25.0000 (25.0000)  loss_ce_0_unscaled: 1.1014 (1.1014)  loss_bbox_0_unscaled: 0.1981 (0.1981)  loss_giou_0_unscaled: 0.8802 (0.8802)  cardinality_error_0_unscaled: 26.0000 (26.0000)  loss_ce_1_unscaled: 1.1338 (1.1338)  loss_bbox_1_unscaled: 0.2043 (0.2043)  loss_giou_1_unscaled: 0.8530 (0.8530)  cardinality_error_1_unscaled: 21.0000 (21.0000)  loss_ce_2_unscaled: 1.1260 (1.1260)  loss_bbox_2_unscaled: 0.2019 (0.2019)  loss_giou_2_unscaled: 0.8515 (0.8515)  cardinality_error_2_unscaled: 10.0000 (10.0000)  loss_ce_3_unscaled: 1.0853 (1.0853)  loss_bbox_3_unscaled: 0.2512 (0.2512)  loss_giou_3_unscaled: 1.1606 (1.1606)  cardinality_error_3_unscaled: 5.5000 (5.5000)  loss_ce_4_unscaled: 1.1462 (1.1462)  loss_bbox_4_unscaled: 0.2048 (0.2048)  loss_giou_4_unscaled: 0.8784 (0.8784)  cardinality_error_4_unscaled: 9.5000 (9.5000)  time: 0.4683  data: 0.2197  max mem: 2666\n",
            "Epoch: [2]  [ 10/100]  eta: 0:00:25  lr: 0.000100  class_error: 34.00  loss: 24.4359 (24.5198)  loss_ce: 1.3301 (1.3387)  loss_bbox: 0.8220 (0.8623)  loss_giou: 1.9907 (1.9457)  loss_ce_0: 1.3315 (1.3159)  loss_bbox_0: 0.7976 (0.8170)  loss_giou_0: 1.8539 (1.8744)  loss_ce_1: 1.3593 (1.3420)  loss_bbox_1: 0.7751 (0.8035)  loss_giou_1: 1.9161 (1.9146)  loss_ce_2: 1.3532 (1.3497)  loss_bbox_2: 0.7293 (0.8144)  loss_giou_2: 1.9166 (1.9247)  loss_ce_3: 1.3231 (1.3209)  loss_bbox_3: 0.7836 (0.8487)  loss_giou_3: 1.9782 (2.0448)  loss_ce_4: 1.3719 (1.3356)  loss_bbox_4: 0.7719 (0.7894)  loss_giou_4: 1.9059 (1.8773)  loss_ce_unscaled: 1.3301 (1.3387)  class_error_unscaled: 85.7143 (73.1143)  loss_bbox_unscaled: 0.1644 (0.1725)  loss_giou_unscaled: 0.9954 (0.9729)  cardinality_error_unscaled: 15.0000 (26.1818)  loss_ce_0_unscaled: 1.3315 (1.3159)  loss_bbox_0_unscaled: 0.1595 (0.1634)  loss_giou_0_unscaled: 0.9269 (0.9372)  cardinality_error_0_unscaled: 17.5000 (28.5000)  loss_ce_1_unscaled: 1.3593 (1.3420)  loss_bbox_1_unscaled: 0.1550 (0.1607)  loss_giou_1_unscaled: 0.9581 (0.9573)  cardinality_error_1_unscaled: 14.5000 (24.9545)  loss_ce_2_unscaled: 1.3532 (1.3497)  loss_bbox_2_unscaled: 0.1459 (0.1629)  loss_giou_2_unscaled: 0.9583 (0.9624)  cardinality_error_2_unscaled: 10.0000 (25.0455)  loss_ce_3_unscaled: 1.3231 (1.3209)  loss_bbox_3_unscaled: 0.1567 (0.1697)  loss_giou_3_unscaled: 0.9891 (1.0224)  cardinality_error_3_unscaled: 12.0000 (26.7273)  loss_ce_4_unscaled: 1.3719 (1.3356)  loss_bbox_4_unscaled: 0.1544 (0.1579)  loss_giou_4_unscaled: 0.9529 (0.9386)  cardinality_error_4_unscaled: 12.0000 (19.6818)  time: 0.2844  data: 0.0298  max mem: 2666\n",
            "Epoch: [2]  [ 20/100]  eta: 0:00:22  lr: 0.000100  class_error: 51.85  loss: 24.4576 (24.5146)  loss_ce: 1.3301 (1.3296)  loss_bbox: 0.7856 (0.8346)  loss_giou: 1.8326 (1.9172)  loss_ce_0: 1.3315 (1.3047)  loss_bbox_0: 0.8002 (0.8561)  loss_giou_0: 1.9598 (1.9796)  loss_ce_1: 1.3593 (1.3425)  loss_bbox_1: 0.7641 (0.8058)  loss_giou_1: 1.9161 (1.8938)  loss_ce_2: 1.3532 (1.3258)  loss_bbox_2: 0.7974 (0.8545)  loss_giou_2: 2.0069 (1.9820)  loss_ce_3: 1.3231 (1.3329)  loss_bbox_3: 0.7615 (0.8212)  loss_giou_3: 1.8952 (1.9493)  loss_ce_4: 1.3719 (1.3380)  loss_bbox_4: 0.7359 (0.7782)  loss_giou_4: 1.8483 (1.8688)  loss_ce_unscaled: 1.3301 (1.3296)  class_error_unscaled: 52.0000 (55.9725)  loss_bbox_unscaled: 0.1571 (0.1669)  loss_giou_unscaled: 0.9163 (0.9586)  cardinality_error_unscaled: 64.0000 (49.9048)  loss_ce_0_unscaled: 1.3315 (1.3047)  loss_bbox_0_unscaled: 0.1600 (0.1712)  loss_giou_0_unscaled: 0.9799 (0.9898)  cardinality_error_0_unscaled: 70.5000 (53.0238)  loss_ce_1_unscaled: 1.3593 (1.3425)  loss_bbox_1_unscaled: 0.1528 (0.1612)  loss_giou_1_unscaled: 0.9581 (0.9469)  cardinality_error_1_unscaled: 55.5000 (48.4524)  loss_ce_2_unscaled: 1.3532 (1.3258)  loss_bbox_2_unscaled: 0.1595 (0.1709)  loss_giou_2_unscaled: 1.0034 (0.9910)  cardinality_error_2_unscaled: 49.5000 (44.4524)  loss_ce_3_unscaled: 1.3231 (1.3329)  loss_bbox_3_unscaled: 0.1523 (0.1642)  loss_giou_3_unscaled: 0.9476 (0.9746)  cardinality_error_3_unscaled: 67.5000 (52.2381)  loss_ce_4_unscaled: 1.3719 (1.3380)  loss_bbox_4_unscaled: 0.1472 (0.1556)  loss_giou_4_unscaled: 0.9242 (0.9344)  cardinality_error_4_unscaled: 61.0000 (48.4048)  time: 0.2667  data: 0.0111  max mem: 2666\n",
            "Epoch: [2]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 86.49  loss: 24.5511 (24.2984)  loss_ce: 1.4078 (1.3579)  loss_bbox: 0.6743 (0.7735)  loss_giou: 1.7513 (1.8648)  loss_ce_0: 1.3596 (1.3345)  loss_bbox_0: 0.8002 (0.8216)  loss_giou_0: 2.0341 (1.9600)  loss_ce_1: 1.4149 (1.3637)  loss_bbox_1: 0.7641 (0.7901)  loss_giou_1: 1.9094 (1.8986)  loss_ce_2: 1.4390 (1.3522)  loss_bbox_2: 0.7911 (0.8099)  loss_giou_2: 1.8168 (1.9119)  loss_ce_3: 1.4095 (1.3577)  loss_bbox_3: 0.7282 (0.8157)  loss_giou_3: 1.8066 (1.9389)  loss_ce_4: 1.4077 (1.3662)  loss_bbox_4: 0.7073 (0.7421)  loss_giou_4: 1.7904 (1.8392)  loss_ce_unscaled: 1.4078 (1.3579)  class_error_unscaled: 60.0000 (66.9579)  loss_bbox_unscaled: 0.1349 (0.1547)  loss_giou_unscaled: 0.8757 (0.9324)  cardinality_error_unscaled: 19.0000 (37.5161)  loss_ce_0_unscaled: 1.3596 (1.3345)  loss_bbox_0_unscaled: 0.1600 (0.1643)  loss_giou_0_unscaled: 1.0171 (0.9800)  cardinality_error_0_unscaled: 49.0000 (41.9839)  loss_ce_1_unscaled: 1.4149 (1.3637)  loss_bbox_1_unscaled: 0.1528 (0.1580)  loss_giou_1_unscaled: 0.9547 (0.9493)  cardinality_error_1_unscaled: 20.0000 (36.3065)  loss_ce_2_unscaled: 1.4390 (1.3522)  loss_bbox_2_unscaled: 0.1582 (0.1620)  loss_giou_2_unscaled: 0.9084 (0.9560)  cardinality_error_2_unscaled: 33.5000 (35.3387)  loss_ce_3_unscaled: 1.4095 (1.3577)  loss_bbox_3_unscaled: 0.1456 (0.1631)  loss_giou_3_unscaled: 0.9033 (0.9694)  cardinality_error_3_unscaled: 35.0000 (40.0968)  loss_ce_4_unscaled: 1.4077 (1.3662)  loss_bbox_4_unscaled: 0.1415 (0.1484)  loss_giou_4_unscaled: 0.8952 (0.9196)  cardinality_error_4_unscaled: 36.5000 (37.5323)  time: 0.2780  data: 0.0115  max mem: 2666\n",
            "Epoch: [2]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 35.71  loss: 23.7626 (24.3171)  loss_ce: 1.3402 (1.3525)  loss_bbox: 0.7143 (0.8144)  loss_giou: 1.8586 (1.9484)  loss_ce_0: 1.3405 (1.3335)  loss_bbox_0: 0.7153 (0.8078)  loss_giou_0: 1.9343 (1.9526)  loss_ce_1: 1.4076 (1.3640)  loss_bbox_1: 0.7086 (0.7799)  loss_giou_1: 1.8132 (1.8960)  loss_ce_2: 1.4390 (1.3651)  loss_bbox_2: 0.7019 (0.7881)  loss_giou_2: 1.7981 (1.9050)  loss_ce_3: 1.3853 (1.3625)  loss_bbox_3: 0.6826 (0.8000)  loss_giou_3: 1.8066 (1.9369)  loss_ce_4: 1.3936 (1.3660)  loss_bbox_4: 0.6407 (0.7248)  loss_giou_4: 1.7714 (1.8197)  loss_ce_unscaled: 1.3402 (1.3525)  class_error_unscaled: 50.0000 (60.2470)  loss_bbox_unscaled: 0.1429 (0.1629)  loss_giou_unscaled: 0.9293 (0.9742)  cardinality_error_unscaled: 19.0000 (45.0000)  loss_ce_0_unscaled: 1.3405 (1.3335)  loss_bbox_0_unscaled: 0.1431 (0.1616)  loss_giou_0_unscaled: 0.9672 (0.9763)  cardinality_error_0_unscaled: 25.5000 (42.7805)  loss_ce_1_unscaled: 1.4076 (1.3640)  loss_bbox_1_unscaled: 0.1417 (0.1560)  loss_giou_1_unscaled: 0.9066 (0.9480)  cardinality_error_1_unscaled: 17.5000 (41.9268)  loss_ce_2_unscaled: 1.4390 (1.3651)  loss_bbox_2_unscaled: 0.1404 (0.1576)  loss_giou_2_unscaled: 0.8991 (0.9525)  cardinality_error_2_unscaled: 55.0000 (46.5366)  loss_ce_3_unscaled: 1.3853 (1.3625)  loss_bbox_3_unscaled: 0.1365 (0.1600)  loss_giou_3_unscaled: 0.9033 (0.9685)  cardinality_error_3_unscaled: 35.0000 (48.8537)  loss_ce_4_unscaled: 1.3936 (1.3660)  loss_bbox_4_unscaled: 0.1281 (0.1450)  loss_giou_4_unscaled: 0.8857 (0.9098)  cardinality_error_4_unscaled: 35.0000 (46.4268)  time: 0.2699  data: 0.0111  max mem: 2666\n",
            "Epoch: [2]  [ 50/100]  eta: 0:00:13  lr: 0.000100  class_error: 62.50  loss: 23.7532 (24.5737)  loss_ce: 1.2659 (1.3579)  loss_bbox: 0.7143 (0.8124)  loss_giou: 1.8586 (1.9162)  loss_ce_0: 1.2809 (1.3390)  loss_bbox_0: 0.7042 (0.8273)  loss_giou_0: 1.9058 (1.9387)  loss_ce_1: 1.3002 (1.3718)  loss_bbox_1: 0.6546 (0.8080)  loss_giou_1: 1.7865 (1.8929)  loss_ce_2: 1.3540 (1.3671)  loss_bbox_2: 0.7908 (0.8477)  loss_giou_2: 1.8951 (1.9656)  loss_ce_3: 1.3050 (1.3678)  loss_bbox_3: 0.6469 (0.8207)  loss_giou_3: 1.8231 (1.9402)  loss_ce_4: 1.3274 (1.3587)  loss_bbox_4: 0.7850 (0.7744)  loss_giou_4: 1.8846 (1.8675)  loss_ce_unscaled: 1.2659 (1.3579)  class_error_unscaled: 45.7143 (58.2259)  loss_bbox_unscaled: 0.1429 (0.1625)  loss_giou_unscaled: 0.9293 (0.9581)  cardinality_error_unscaled: 72.5000 (49.3824)  loss_ce_0_unscaled: 1.2809 (1.3390)  loss_bbox_0_unscaled: 0.1408 (0.1655)  loss_giou_0_unscaled: 0.9529 (0.9694)  cardinality_error_0_unscaled: 61.5000 (47.8627)  loss_ce_1_unscaled: 1.3002 (1.3718)  loss_bbox_1_unscaled: 0.1309 (0.1616)  loss_giou_1_unscaled: 0.8932 (0.9465)  cardinality_error_1_unscaled: 67.5000 (45.2647)  loss_ce_2_unscaled: 1.3540 (1.3671)  loss_bbox_2_unscaled: 0.1582 (0.1695)  loss_giou_2_unscaled: 0.9475 (0.9828)  cardinality_error_2_unscaled: 75.5000 (43.6863)  loss_ce_3_unscaled: 1.3050 (1.3678)  loss_bbox_3_unscaled: 0.1294 (0.1641)  loss_giou_3_unscaled: 0.9116 (0.9701)  cardinality_error_3_unscaled: 64.5000 (45.3824)  loss_ce_4_unscaled: 1.3274 (1.3587)  loss_bbox_4_unscaled: 0.1570 (0.1549)  loss_giou_4_unscaled: 0.9423 (0.9337)  cardinality_error_4_unscaled: 65.5000 (43.7647)  time: 0.2645  data: 0.0108  max mem: 2666\n",
            "Epoch: [2]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 32.14  loss: 24.2971 (24.4492)  loss_ce: 1.2768 (1.3533)  loss_bbox: 0.6436 (0.7909)  loss_giou: 1.7437 (1.9094)  loss_ce_0: 1.2818 (1.3375)  loss_bbox_0: 0.6806 (0.8127)  loss_giou_0: 1.8221 (1.9383)  loss_ce_1: 1.3002 (1.3690)  loss_bbox_1: 0.6877 (0.7905)  loss_giou_1: 1.8018 (1.8902)  loss_ce_2: 1.2567 (1.3570)  loss_bbox_2: 0.7915 (0.8400)  loss_giou_2: 2.1305 (1.9888)  loss_ce_3: 1.2941 (1.3634)  loss_bbox_3: 0.6492 (0.7995)  loss_giou_3: 1.8550 (1.9296)  loss_ce_4: 1.2439 (1.3556)  loss_bbox_4: 0.7754 (0.7584)  loss_giou_4: 1.9192 (1.8655)  loss_ce_unscaled: 1.2768 (1.3533)  class_error_unscaled: 62.5000 (59.0615)  loss_bbox_unscaled: 0.1287 (0.1582)  loss_giou_unscaled: 0.8718 (0.9547)  cardinality_error_unscaled: 47.5000 (46.8033)  loss_ce_0_unscaled: 1.2818 (1.3375)  loss_bbox_0_unscaled: 0.1361 (0.1625)  loss_giou_0_unscaled: 0.9110 (0.9691)  cardinality_error_0_unscaled: 49.5000 (47.0328)  loss_ce_1_unscaled: 1.3002 (1.3690)  loss_bbox_1_unscaled: 0.1375 (0.1581)  loss_giou_1_unscaled: 0.9009 (0.9451)  cardinality_error_1_unscaled: 55.0000 (45.5738)  loss_ce_2_unscaled: 1.2567 (1.3570)  loss_bbox_2_unscaled: 0.1583 (0.1680)  loss_giou_2_unscaled: 1.0652 (0.9944)  cardinality_error_2_unscaled: 27.5000 (42.8443)  loss_ce_3_unscaled: 1.2941 (1.3634)  loss_bbox_3_unscaled: 0.1298 (0.1599)  loss_giou_3_unscaled: 0.9275 (0.9648)  cardinality_error_3_unscaled: 49.5000 (47.5656)  loss_ce_4_unscaled: 1.2439 (1.3556)  loss_bbox_4_unscaled: 0.1551 (0.1517)  loss_giou_4_unscaled: 0.9596 (0.9327)  cardinality_error_4_unscaled: 34.5000 (43.5574)  time: 0.2811  data: 0.0116  max mem: 2666\n",
            "Epoch: [2]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 42.86  loss: 24.8434 (24.5314)  loss_ce: 1.2613 (1.3411)  loss_bbox: 0.6356 (0.7825)  loss_giou: 1.7995 (1.9068)  loss_ce_0: 1.2461 (1.3235)  loss_bbox_0: 0.7427 (0.8246)  loss_giou_0: 1.9295 (1.9772)  loss_ce_1: 1.3177 (1.3633)  loss_bbox_1: 0.6877 (0.7837)  loss_giou_1: 1.8685 (1.8837)  loss_ce_2: 1.3512 (1.3576)  loss_bbox_2: 0.7915 (0.8562)  loss_giou_2: 2.0784 (2.0145)  loss_ce_3: 1.3245 (1.3572)  loss_bbox_3: 0.7819 (0.8134)  loss_giou_3: 1.9830 (1.9556)  loss_ce_4: 1.2639 (1.3448)  loss_bbox_4: 0.6793 (0.7640)  loss_giou_4: 1.8818 (1.8818)  loss_ce_unscaled: 1.2613 (1.3411)  class_error_unscaled: 56.0000 (58.1890)  loss_bbox_unscaled: 0.1271 (0.1565)  loss_giou_unscaled: 0.8997 (0.9534)  cardinality_error_unscaled: 34.5000 (46.0352)  loss_ce_0_unscaled: 1.2461 (1.3235)  loss_bbox_0_unscaled: 0.1485 (0.1649)  loss_giou_0_unscaled: 0.9647 (0.9886)  cardinality_error_0_unscaled: 37.0000 (43.1197)  loss_ce_1_unscaled: 1.3177 (1.3633)  loss_bbox_1_unscaled: 0.1375 (0.1567)  loss_giou_1_unscaled: 0.9343 (0.9418)  cardinality_error_1_unscaled: 41.5000 (45.0000)  loss_ce_2_unscaled: 1.3512 (1.3576)  loss_bbox_2_unscaled: 0.1583 (0.1712)  loss_giou_2_unscaled: 1.0392 (1.0073)  cardinality_error_2_unscaled: 48.5000 (46.2324)  loss_ce_3_unscaled: 1.3245 (1.3572)  loss_bbox_3_unscaled: 0.1564 (0.1627)  loss_giou_3_unscaled: 0.9915 (0.9778)  cardinality_error_3_unscaled: 65.5000 (49.9155)  loss_ce_4_unscaled: 1.2639 (1.3448)  loss_bbox_4_unscaled: 0.1359 (0.1528)  loss_giou_4_unscaled: 0.9409 (0.9409)  cardinality_error_4_unscaled: 35.0000 (42.3592)  time: 0.2815  data: 0.0115  max mem: 2666\n",
            "Epoch: [2]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 48.39  loss: 24.8434 (24.7155)  loss_ce: 1.2547 (1.3359)  loss_bbox: 0.7539 (0.7922)  loss_giou: 1.9627 (1.9387)  loss_ce_0: 1.2908 (1.3210)  loss_bbox_0: 0.8155 (0.8345)  loss_giou_0: 2.1364 (2.0074)  loss_ce_1: 1.3177 (1.3638)  loss_bbox_1: 0.6938 (0.7856)  loss_giou_1: 1.8685 (1.9020)  loss_ce_2: 1.3657 (1.3530)  loss_bbox_2: 0.8337 (0.8559)  loss_giou_2: 2.0350 (2.0249)  loss_ce_3: 1.2744 (1.3474)  loss_bbox_3: 0.8112 (0.8251)  loss_giou_3: 2.1001 (1.9882)  loss_ce_4: 1.2639 (1.3372)  loss_bbox_4: 0.7771 (0.7805)  loss_giou_4: 2.0498 (1.9223)  loss_ce_unscaled: 1.2547 (1.3359)  class_error_unscaled: 43.7500 (55.7607)  loss_bbox_unscaled: 0.1508 (0.1584)  loss_giou_unscaled: 0.9814 (0.9693)  cardinality_error_unscaled: 59.0000 (50.0494)  loss_ce_0_unscaled: 1.2908 (1.3210)  loss_bbox_0_unscaled: 0.1631 (0.1669)  loss_giou_0_unscaled: 1.0682 (1.0037)  cardinality_error_0_unscaled: 37.0000 (46.7346)  loss_ce_1_unscaled: 1.3177 (1.3638)  loss_bbox_1_unscaled: 0.1388 (0.1571)  loss_giou_1_unscaled: 0.9343 (0.9510)  cardinality_error_1_unscaled: 56.5000 (48.4321)  loss_ce_2_unscaled: 1.3657 (1.3530)  loss_bbox_2_unscaled: 0.1667 (0.1712)  loss_giou_2_unscaled: 1.0175 (1.0125)  cardinality_error_2_unscaled: 66.5000 (48.6296)  loss_ce_3_unscaled: 1.2744 (1.3474)  loss_bbox_3_unscaled: 0.1622 (0.1650)  loss_giou_3_unscaled: 1.0500 (0.9941)  cardinality_error_3_unscaled: 71.5000 (52.3025)  loss_ce_4_unscaled: 1.2639 (1.3372)  loss_bbox_4_unscaled: 0.1554 (0.1561)  loss_giou_4_unscaled: 1.0249 (0.9612)  cardinality_error_4_unscaled: 64.5000 (46.8395)  time: 0.2727  data: 0.0112  max mem: 2666\n",
            "Epoch: [2]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 100.00  loss: 25.8838 (24.9658)  loss_ce: 1.2547 (1.3310)  loss_bbox: 0.8214 (0.8142)  loss_giou: 2.0604 (1.9587)  loss_ce_0: 1.2950 (1.3177)  loss_bbox_0: 0.9039 (0.8662)  loss_giou_0: 2.1679 (2.0404)  loss_ce_1: 1.3257 (1.3574)  loss_bbox_1: 0.8240 (0.8179)  loss_giou_1: 2.0406 (1.9422)  loss_ce_2: 1.2568 (1.3475)  loss_bbox_2: 0.8060 (0.8713)  loss_giou_2: 1.9367 (2.0334)  loss_ce_3: 1.2708 (1.3364)  loss_bbox_3: 0.8616 (0.8457)  loss_giou_3: 2.1101 (2.0047)  loss_ce_4: 1.2705 (1.3294)  loss_bbox_4: 0.8088 (0.8048)  loss_giou_4: 2.0498 (1.9470)  loss_ce_unscaled: 1.2547 (1.3310)  class_error_unscaled: 50.0000 (57.4134)  loss_bbox_unscaled: 0.1643 (0.1628)  loss_giou_unscaled: 1.0302 (0.9793)  cardinality_error_unscaled: 51.0000 (46.6813)  loss_ce_0_unscaled: 1.2950 (1.3177)  loss_bbox_0_unscaled: 0.1808 (0.1732)  loss_giou_0_unscaled: 1.0839 (1.0202)  cardinality_error_0_unscaled: 63.5000 (45.6319)  loss_ce_1_unscaled: 1.3257 (1.3574)  loss_bbox_1_unscaled: 0.1648 (0.1636)  loss_giou_1_unscaled: 1.0203 (0.9711)  cardinality_error_1_unscaled: 56.0000 (46.8681)  loss_ce_2_unscaled: 1.2568 (1.3475)  loss_bbox_2_unscaled: 0.1612 (0.1743)  loss_giou_2_unscaled: 0.9683 (1.0167)  cardinality_error_2_unscaled: 46.5000 (47.1319)  loss_ce_3_unscaled: 1.2708 (1.3364)  loss_bbox_3_unscaled: 0.1723 (0.1691)  loss_giou_3_unscaled: 1.0550 (1.0024)  cardinality_error_3_unscaled: 58.0000 (51.5659)  loss_ce_4_unscaled: 1.2705 (1.3294)  loss_bbox_4_unscaled: 0.1618 (0.1610)  loss_giou_4_unscaled: 1.0249 (0.9735)  cardinality_error_4_unscaled: 69.0000 (46.5165)  time: 0.2851  data: 0.0121  max mem: 2666\n",
            "Epoch: [2]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 59.26  loss: 24.6522 (24.8679)  loss_ce: 1.2825 (1.3227)  loss_bbox: 0.8729 (0.8203)  loss_giou: 1.8373 (1.9434)  loss_ce_0: 1.2737 (1.3112)  loss_bbox_0: 0.9167 (0.8698)  loss_giou_0: 2.0920 (2.0251)  loss_ce_1: 1.3064 (1.3497)  loss_bbox_1: 0.9666 (0.8281)  loss_giou_1: 2.0489 (1.9304)  loss_ce_2: 1.2781 (1.3395)  loss_bbox_2: 0.8779 (0.8738)  loss_giou_2: 1.8269 (2.0111)  loss_ce_3: 1.2710 (1.3276)  loss_bbox_3: 0.9394 (0.8540)  loss_giou_3: 1.8564 (1.9917)  loss_ce_4: 1.2482 (1.3187)  loss_bbox_4: 0.9071 (0.8143)  loss_giou_4: 1.8498 (1.9366)  loss_ce_unscaled: 1.2825 (1.3227)  class_error_unscaled: 65.2174 (57.5329)  loss_bbox_unscaled: 0.1746 (0.1641)  loss_giou_unscaled: 0.9187 (0.9717)  cardinality_error_unscaled: 27.0000 (46.2100)  loss_ce_0_unscaled: 1.2737 (1.3112)  loss_bbox_0_unscaled: 0.1833 (0.1740)  loss_giou_0_unscaled: 1.0460 (1.0126)  cardinality_error_0_unscaled: 29.0000 (44.3650)  loss_ce_1_unscaled: 1.3064 (1.3497)  loss_bbox_1_unscaled: 0.1933 (0.1656)  loss_giou_1_unscaled: 1.0245 (0.9652)  cardinality_error_1_unscaled: 34.5000 (45.6650)  loss_ce_2_unscaled: 1.2781 (1.3395)  loss_bbox_2_unscaled: 0.1756 (0.1748)  loss_giou_2_unscaled: 0.9135 (1.0055)  cardinality_error_2_unscaled: 41.0000 (47.9050)  loss_ce_3_unscaled: 1.2710 (1.3276)  loss_bbox_3_unscaled: 0.1879 (0.1708)  loss_giou_3_unscaled: 0.9282 (0.9958)  cardinality_error_3_unscaled: 46.5000 (51.8450)  loss_ce_4_unscaled: 1.2482 (1.3187)  loss_bbox_4_unscaled: 0.1814 (0.1629)  loss_giou_4_unscaled: 0.9249 (0.9683)  cardinality_error_4_unscaled: 23.5000 (43.7700)  time: 0.3088  data: 0.0144  max mem: 2666\n",
            "Epoch: [2] Total time: 0:00:28 (0.2833 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 59.26  loss: 24.6522 (24.8679)  loss_ce: 1.2825 (1.3227)  loss_bbox: 0.8729 (0.8203)  loss_giou: 1.8373 (1.9434)  loss_ce_0: 1.2737 (1.3112)  loss_bbox_0: 0.9167 (0.8698)  loss_giou_0: 2.0920 (2.0251)  loss_ce_1: 1.3064 (1.3497)  loss_bbox_1: 0.9666 (0.8281)  loss_giou_1: 2.0489 (1.9304)  loss_ce_2: 1.2781 (1.3395)  loss_bbox_2: 0.8779 (0.8738)  loss_giou_2: 1.8269 (2.0111)  loss_ce_3: 1.2710 (1.3276)  loss_bbox_3: 0.9394 (0.8540)  loss_giou_3: 1.8564 (1.9917)  loss_ce_4: 1.2482 (1.3187)  loss_bbox_4: 0.9071 (0.8143)  loss_giou_4: 1.8498 (1.9366)  loss_ce_unscaled: 1.2825 (1.3227)  class_error_unscaled: 65.2174 (57.5329)  loss_bbox_unscaled: 0.1746 (0.1641)  loss_giou_unscaled: 0.9187 (0.9717)  cardinality_error_unscaled: 27.0000 (46.2100)  loss_ce_0_unscaled: 1.2737 (1.3112)  loss_bbox_0_unscaled: 0.1833 (0.1740)  loss_giou_0_unscaled: 1.0460 (1.0126)  cardinality_error_0_unscaled: 29.0000 (44.3650)  loss_ce_1_unscaled: 1.3064 (1.3497)  loss_bbox_1_unscaled: 0.1933 (0.1656)  loss_giou_1_unscaled: 1.0245 (0.9652)  cardinality_error_1_unscaled: 34.5000 (45.6650)  loss_ce_2_unscaled: 1.2781 (1.3395)  loss_bbox_2_unscaled: 0.1756 (0.1748)  loss_giou_2_unscaled: 0.9135 (1.0055)  cardinality_error_2_unscaled: 41.0000 (47.9050)  loss_ce_3_unscaled: 1.2710 (1.3276)  loss_bbox_3_unscaled: 0.1879 (0.1708)  loss_giou_3_unscaled: 0.9282 (0.9958)  cardinality_error_3_unscaled: 46.5000 (51.8450)  loss_ce_4_unscaled: 1.2482 (1.3187)  loss_bbox_4_unscaled: 0.1814 (0.1629)  loss_giou_4_unscaled: 0.9249 (0.9683)  cardinality_error_4_unscaled: 23.5000 (43.7700)\n",
            "Test:  [ 0/15]  eta: 0:00:09  class_error: 86.36  loss: 46.8481 (46.8481)  loss_ce: 1.7447 (1.7447)  loss_bbox: 2.2077 (2.2077)  loss_giou: 3.3555 (3.3555)  loss_ce_0: 1.7295 (1.7295)  loss_bbox_0: 2.4761 (2.4761)  loss_giou_0: 3.4871 (3.4871)  loss_ce_1: 1.7180 (1.7180)  loss_bbox_1: 2.3121 (2.3121)  loss_giou_1: 3.4256 (3.4256)  loss_ce_2: 1.7278 (1.7278)  loss_bbox_2: 3.2375 (3.2375)  loss_giou_2: 3.7223 (3.7223)  loss_ce_3: 1.6939 (1.6939)  loss_bbox_3: 3.3938 (3.3938)  loss_giou_3: 3.7082 (3.7082)  loss_ce_4: 1.6093 (1.6093)  loss_bbox_4: 1.8884 (1.8884)  loss_giou_4: 3.4107 (3.4107)  loss_ce_unscaled: 1.7447 (1.7447)  class_error_unscaled: 86.3636 (86.3636)  loss_bbox_unscaled: 0.4415 (0.4415)  loss_giou_unscaled: 1.6777 (1.6777)  cardinality_error_unscaled: 78.0000 (78.0000)  loss_ce_0_unscaled: 1.7295 (1.7295)  loss_bbox_0_unscaled: 0.4952 (0.4952)  loss_giou_0_unscaled: 1.7435 (1.7435)  cardinality_error_0_unscaled: 78.0000 (78.0000)  loss_ce_1_unscaled: 1.7180 (1.7180)  loss_bbox_1_unscaled: 0.4624 (0.4624)  loss_giou_1_unscaled: 1.7128 (1.7128)  cardinality_error_1_unscaled: 78.0000 (78.0000)  loss_ce_2_unscaled: 1.7278 (1.7278)  loss_bbox_2_unscaled: 0.6475 (0.6475)  loss_giou_2_unscaled: 1.8612 (1.8612)  cardinality_error_2_unscaled: 78.0000 (78.0000)  loss_ce_3_unscaled: 1.6939 (1.6939)  loss_bbox_3_unscaled: 0.6788 (0.6788)  loss_giou_3_unscaled: 1.8541 (1.8541)  cardinality_error_3_unscaled: 78.0000 (78.0000)  loss_ce_4_unscaled: 1.6093 (1.6093)  loss_bbox_4_unscaled: 0.3777 (0.3777)  loss_giou_4_unscaled: 1.7054 (1.7054)  cardinality_error_4_unscaled: 22.0000 (22.0000)  time: 0.6480  data: 0.3609  max mem: 2666\n",
            "Test:  [10/15]  eta: 0:00:01  class_error: 25.71  loss: 43.2063 (42.4127)  loss_ce: 1.3807 (1.4239)  loss_bbox: 1.9076 (1.9254)  loss_giou: 3.3555 (3.3795)  loss_ce_0: 1.3750 (1.4206)  loss_bbox_0: 2.1455 (2.1085)  loss_giou_0: 3.3722 (3.4023)  loss_ce_1: 1.3950 (1.4261)  loss_bbox_1: 1.9828 (1.9954)  loss_giou_1: 3.4256 (3.3941)  loss_ce_2: 1.4024 (1.4311)  loss_bbox_2: 2.8847 (2.7321)  loss_giou_2: 3.5322 (3.5581)  loss_ce_3: 1.4167 (1.4238)  loss_bbox_3: 3.0435 (2.8777)  loss_giou_3: 3.5145 (3.5454)  loss_ce_4: 1.4316 (1.4181)  loss_bbox_4: 1.6741 (1.7076)  loss_giou_4: 3.1661 (3.2432)  loss_ce_unscaled: 1.3807 (1.4239)  class_error_unscaled: 32.6531 (45.1222)  loss_bbox_unscaled: 0.3815 (0.3851)  loss_giou_unscaled: 1.6777 (1.6897)  cardinality_error_unscaled: 82.5000 (81.2727)  loss_ce_0_unscaled: 1.3750 (1.4206)  loss_bbox_0_unscaled: 0.4291 (0.4217)  loss_giou_0_unscaled: 1.6861 (1.7012)  cardinality_error_0_unscaled: 82.5000 (81.2727)  loss_ce_1_unscaled: 1.3950 (1.4261)  loss_bbox_1_unscaled: 0.3966 (0.3991)  loss_giou_1_unscaled: 1.7128 (1.6970)  cardinality_error_1_unscaled: 82.5000 (81.2727)  loss_ce_2_unscaled: 1.4024 (1.4311)  loss_bbox_2_unscaled: 0.5769 (0.5464)  loss_giou_2_unscaled: 1.7661 (1.7790)  cardinality_error_2_unscaled: 82.5000 (81.2727)  loss_ce_3_unscaled: 1.4167 (1.4238)  loss_bbox_3_unscaled: 0.6087 (0.5755)  loss_giou_3_unscaled: 1.7573 (1.7727)  cardinality_error_3_unscaled: 82.5000 (81.2727)  loss_ce_4_unscaled: 1.4316 (1.4181)  loss_bbox_4_unscaled: 0.3348 (0.3415)  loss_giou_4_unscaled: 1.5830 (1.6216)  cardinality_error_4_unscaled: 17.5000 (18.7273)  time: 0.2305  data: 0.0427  max mem: 2666\n",
            "Test:  [14/15]  eta: 0:00:00  class_error: 72.86  loss: 41.8136 (41.9473)  loss_ce: 1.3807 (1.4334)  loss_bbox: 1.8150 (1.8473)  loss_giou: 3.3433 (3.3401)  loss_ce_0: 1.3750 (1.4314)  loss_bbox_0: 1.8771 (2.0357)  loss_giou_0: 3.3722 (3.3861)  loss_ce_1: 1.3950 (1.4351)  loss_bbox_1: 1.8784 (1.9176)  loss_giou_1: 3.3913 (3.3587)  loss_ce_2: 1.4024 (1.4372)  loss_bbox_2: 2.5979 (2.6769)  loss_giou_2: 3.5642 (3.5624)  loss_ce_3: 1.4167 (1.4275)  loss_bbox_3: 2.7391 (2.8262)  loss_giou_3: 3.5492 (3.5497)  loss_ce_4: 1.4316 (1.4272)  loss_bbox_4: 1.6054 (1.6365)  loss_giou_4: 3.1661 (3.2182)  loss_ce_unscaled: 1.3807 (1.4334)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.3630 (0.3695)  loss_giou_unscaled: 1.6716 (1.6701)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.3750 (1.4314)  loss_bbox_0_unscaled: 0.3754 (0.4071)  loss_giou_0_unscaled: 1.6861 (1.6931)  cardinality_error_0_unscaled: 82.5000 (79.6000)  loss_ce_1_unscaled: 1.3950 (1.4351)  loss_bbox_1_unscaled: 0.3757 (0.3835)  loss_giou_1_unscaled: 1.6956 (1.6794)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.4024 (1.4372)  loss_bbox_2_unscaled: 0.5196 (0.5354)  loss_giou_2_unscaled: 1.7821 (1.7812)  cardinality_error_2_unscaled: 82.5000 (79.6000)  loss_ce_3_unscaled: 1.4167 (1.4275)  loss_bbox_3_unscaled: 0.5478 (0.5652)  loss_giou_3_unscaled: 1.7746 (1.7748)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.4316 (1.4272)  loss_bbox_4_unscaled: 0.3211 (0.3273)  loss_giou_4_unscaled: 1.5830 (1.6091)  cardinality_error_4_unscaled: 17.5000 (20.4000)  time: 0.2199  data: 0.0342  max mem: 2666\n",
            "Test: Total time: 0:00:03 (0.2238 s / it)\n",
            "Averaged stats: class_error: 72.86  loss: 41.8136 (41.9473)  loss_ce: 1.3807 (1.4334)  loss_bbox: 1.8150 (1.8473)  loss_giou: 3.3433 (3.3401)  loss_ce_0: 1.3750 (1.4314)  loss_bbox_0: 1.8771 (2.0357)  loss_giou_0: 3.3722 (3.3861)  loss_ce_1: 1.3950 (1.4351)  loss_bbox_1: 1.8784 (1.9176)  loss_giou_1: 3.3913 (3.3587)  loss_ce_2: 1.4024 (1.4372)  loss_bbox_2: 2.5979 (2.6769)  loss_giou_2: 3.5642 (3.5624)  loss_ce_3: 1.4167 (1.4275)  loss_bbox_3: 2.7391 (2.8262)  loss_giou_3: 3.5492 (3.5497)  loss_ce_4: 1.4316 (1.4272)  loss_bbox_4: 1.6054 (1.6365)  loss_giou_4: 3.1661 (3.2182)  loss_ce_unscaled: 1.3807 (1.4334)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.3630 (0.3695)  loss_giou_unscaled: 1.6716 (1.6701)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.3750 (1.4314)  loss_bbox_0_unscaled: 0.3754 (0.4071)  loss_giou_0_unscaled: 1.6861 (1.6931)  cardinality_error_0_unscaled: 82.5000 (79.6000)  loss_ce_1_unscaled: 1.3950 (1.4351)  loss_bbox_1_unscaled: 0.3757 (0.3835)  loss_giou_1_unscaled: 1.6956 (1.6794)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.4024 (1.4372)  loss_bbox_2_unscaled: 0.5196 (0.5354)  loss_giou_2_unscaled: 1.7821 (1.7812)  cardinality_error_2_unscaled: 82.5000 (79.6000)  loss_ce_3_unscaled: 1.4167 (1.4275)  loss_bbox_3_unscaled: 0.5478 (0.5652)  loss_giou_3_unscaled: 1.7746 (1.7748)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.4316 (1.4272)  loss_bbox_4_unscaled: 0.3211 (0.3273)  loss_giou_4_unscaled: 1.5830 (1.6091)  cardinality_error_4_unscaled: 17.5000 (20.4000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [3]  [  0/100]  eta: 0:01:02  lr: 0.000100  class_error: 56.82  loss: 32.5085 (32.5085)  loss_ce: 1.3254 (1.3254)  loss_bbox: 1.2568 (1.2568)  loss_giou: 2.2783 (2.2783)  loss_ce_0: 1.2866 (1.2866)  loss_bbox_0: 1.3912 (1.3912)  loss_giou_0: 2.5569 (2.5569)  loss_ce_1: 1.2688 (1.2688)  loss_bbox_1: 1.2296 (1.2296)  loss_giou_1: 2.3092 (2.3092)  loss_ce_2: 1.3387 (1.3387)  loss_bbox_2: 1.6848 (1.6848)  loss_giou_2: 2.8217 (2.8217)  loss_ce_3: 1.2811 (1.2811)  loss_bbox_3: 2.5342 (2.5342)  loss_giou_3: 3.3230 (3.3230)  loss_ce_4: 1.3377 (1.3377)  loss_bbox_4: 1.1577 (1.1577)  loss_giou_4: 2.1269 (2.1269)  loss_ce_unscaled: 1.3254 (1.3254)  class_error_unscaled: 56.8182 (56.8182)  loss_bbox_unscaled: 0.2514 (0.2514)  loss_giou_unscaled: 1.1391 (1.1391)  cardinality_error_unscaled: 57.0000 (57.0000)  loss_ce_0_unscaled: 1.2866 (1.2866)  loss_bbox_0_unscaled: 0.2782 (0.2782)  loss_giou_0_unscaled: 1.2785 (1.2785)  cardinality_error_0_unscaled: 46.0000 (46.0000)  loss_ce_1_unscaled: 1.2688 (1.2688)  loss_bbox_1_unscaled: 0.2459 (0.2459)  loss_giou_1_unscaled: 1.1546 (1.1546)  cardinality_error_1_unscaled: 33.0000 (33.0000)  loss_ce_2_unscaled: 1.3387 (1.3387)  loss_bbox_2_unscaled: 0.3370 (0.3370)  loss_giou_2_unscaled: 1.4109 (1.4109)  cardinality_error_2_unscaled: 62.0000 (62.0000)  loss_ce_3_unscaled: 1.2811 (1.2811)  loss_bbox_3_unscaled: 0.5068 (0.5068)  loss_giou_3_unscaled: 1.6615 (1.6615)  cardinality_error_3_unscaled: 74.0000 (74.0000)  loss_ce_4_unscaled: 1.3377 (1.3377)  loss_bbox_4_unscaled: 0.2315 (0.2315)  loss_giou_4_unscaled: 1.0634 (1.0634)  cardinality_error_4_unscaled: 20.0000 (20.0000)  time: 0.6244  data: 0.2672  max mem: 2666\n",
            "Epoch: [3]  [ 10/100]  eta: 0:00:26  lr: 0.000100  class_error: 90.00  loss: 26.7743 (26.5133)  loss_ce: 1.3451 (1.3488)  loss_bbox: 0.8035 (0.8612)  loss_giou: 1.8772 (1.9549)  loss_ce_0: 1.3440 (1.3385)  loss_bbox_0: 0.8278 (0.8986)  loss_giou_0: 2.0172 (2.0401)  loss_ce_1: 1.3478 (1.3418)  loss_bbox_1: 0.8453 (0.9162)  loss_giou_1: 2.0691 (2.0181)  loss_ce_2: 1.3387 (1.3468)  loss_bbox_2: 0.9650 (1.0327)  loss_giou_2: 2.1025 (2.1269)  loss_ce_3: 1.3412 (1.3575)  loss_bbox_3: 0.9604 (1.1509)  loss_giou_3: 2.0624 (2.2329)  loss_ce_4: 1.2922 (1.2843)  loss_bbox_4: 0.9232 (1.0924)  loss_giou_4: 2.0167 (2.1707)  loss_ce_unscaled: 1.3451 (1.3488)  class_error_unscaled: 77.0833 (70.9186)  loss_bbox_unscaled: 0.1607 (0.1722)  loss_giou_unscaled: 0.9386 (0.9775)  cardinality_error_unscaled: 29.5000 (32.4091)  loss_ce_0_unscaled: 1.3440 (1.3385)  loss_bbox_0_unscaled: 0.1656 (0.1797)  loss_giou_0_unscaled: 1.0086 (1.0200)  cardinality_error_0_unscaled: 32.0000 (30.9091)  loss_ce_1_unscaled: 1.3478 (1.3418)  loss_bbox_1_unscaled: 0.1691 (0.1832)  loss_giou_1_unscaled: 1.0346 (1.0090)  cardinality_error_1_unscaled: 18.5000 (21.8182)  loss_ce_2_unscaled: 1.3387 (1.3468)  loss_bbox_2_unscaled: 0.1930 (0.2065)  loss_giou_2_unscaled: 1.0512 (1.0635)  cardinality_error_2_unscaled: 13.5000 (24.7727)  loss_ce_3_unscaled: 1.3412 (1.3575)  loss_bbox_3_unscaled: 0.1921 (0.2302)  loss_giou_3_unscaled: 1.0312 (1.1165)  cardinality_error_3_unscaled: 44.0000 (41.3636)  loss_ce_4_unscaled: 1.2922 (1.2843)  loss_bbox_4_unscaled: 0.1846 (0.2185)  loss_giou_4_unscaled: 1.0084 (1.0853)  cardinality_error_4_unscaled: 10.0000 (11.0000)  time: 0.2995  data: 0.0355  max mem: 2666\n",
            "Epoch: [3]  [ 20/100]  eta: 0:00:22  lr: 0.000100  class_error: 60.00  loss: 25.8246 (25.9517)  loss_ce: 1.3451 (1.3330)  loss_bbox: 0.8811 (0.8788)  loss_giou: 1.8644 (1.9450)  loss_ce_0: 1.3440 (1.3122)  loss_bbox_0: 0.8768 (0.9127)  loss_giou_0: 1.8963 (1.9798)  loss_ce_1: 1.3478 (1.3485)  loss_bbox_1: 0.9434 (0.9649)  loss_giou_1: 1.9600 (2.0183)  loss_ce_2: 1.3166 (1.3476)  loss_bbox_2: 0.9650 (1.0302)  loss_giou_2: 2.0343 (2.0724)  loss_ce_3: 1.3412 (1.3471)  loss_bbox_3: 0.9214 (1.0467)  loss_giou_3: 1.9652 (2.0704)  loss_ce_4: 1.2634 (1.2759)  loss_bbox_4: 0.9370 (1.0124)  loss_giou_4: 1.8925 (2.0561)  loss_ce_unscaled: 1.3451 (1.3330)  class_error_unscaled: 84.2105 (76.8686)  loss_bbox_unscaled: 0.1762 (0.1758)  loss_giou_unscaled: 0.9322 (0.9725)  cardinality_error_unscaled: 20.5000 (28.0000)  loss_ce_0_unscaled: 1.3440 (1.3122)  loss_bbox_0_unscaled: 0.1754 (0.1825)  loss_giou_0_unscaled: 0.9481 (0.9899)  cardinality_error_0_unscaled: 12.0000 (21.4524)  loss_ce_1_unscaled: 1.3478 (1.3485)  loss_bbox_1_unscaled: 0.1887 (0.1930)  loss_giou_1_unscaled: 0.9800 (1.0092)  cardinality_error_1_unscaled: 22.5000 (26.8571)  loss_ce_2_unscaled: 1.3166 (1.3476)  loss_bbox_2_unscaled: 0.1930 (0.2060)  loss_giou_2_unscaled: 1.0172 (1.0362)  cardinality_error_2_unscaled: 11.0000 (18.1667)  loss_ce_3_unscaled: 1.3412 (1.3471)  loss_bbox_3_unscaled: 0.1843 (0.2093)  loss_giou_3_unscaled: 0.9826 (1.0352)  cardinality_error_3_unscaled: 26.5000 (33.8333)  loss_ce_4_unscaled: 1.2634 (1.2759)  loss_bbox_4_unscaled: 0.1874 (0.2025)  loss_giou_4_unscaled: 0.9463 (1.0280)  cardinality_error_4_unscaled: 10.0000 (14.3810)  time: 0.2689  data: 0.0117  max mem: 2666\n",
            "Epoch: [3]  [ 30/100]  eta: 0:00:20  lr: 0.000100  class_error: 64.29  loss: 24.2531 (25.0923)  loss_ce: 1.2323 (1.3044)  loss_bbox: 0.7780 (0.8276)  loss_giou: 1.8600 (1.9205)  loss_ce_0: 1.2126 (1.2927)  loss_bbox_0: 0.8023 (0.8467)  loss_giou_0: 1.7309 (1.9219)  loss_ce_1: 1.2457 (1.3244)  loss_bbox_1: 0.9434 (0.9525)  loss_giou_1: 2.0305 (2.0684)  loss_ce_2: 1.2279 (1.3096)  loss_bbox_2: 0.8595 (0.9442)  loss_giou_2: 1.9171 (2.0029)  loss_ce_3: 1.2231 (1.3116)  loss_bbox_3: 0.8072 (0.9357)  loss_giou_3: 1.7783 (1.9712)  loss_ce_4: 1.1891 (1.2566)  loss_bbox_4: 0.8421 (0.9250)  loss_giou_4: 1.8456 (1.9763)  loss_ce_unscaled: 1.2323 (1.3044)  class_error_unscaled: 64.2857 (70.2133)  loss_bbox_unscaled: 0.1556 (0.1655)  loss_giou_unscaled: 0.9300 (0.9603)  cardinality_error_unscaled: 22.5000 (30.3226)  loss_ce_0_unscaled: 1.2126 (1.2927)  loss_bbox_0_unscaled: 0.1605 (0.1693)  loss_giou_0_unscaled: 0.8654 (0.9610)  cardinality_error_0_unscaled: 14.0000 (25.9194)  loss_ce_1_unscaled: 1.2457 (1.3244)  loss_bbox_1_unscaled: 0.1887 (0.1905)  loss_giou_1_unscaled: 1.0153 (1.0342)  cardinality_error_1_unscaled: 51.0000 (38.6129)  loss_ce_2_unscaled: 1.2279 (1.3096)  loss_bbox_2_unscaled: 0.1719 (0.1888)  loss_giou_2_unscaled: 0.9585 (1.0015)  cardinality_error_2_unscaled: 17.5000 (31.9516)  loss_ce_3_unscaled: 1.2231 (1.3116)  loss_bbox_3_unscaled: 0.1614 (0.1871)  loss_giou_3_unscaled: 0.8891 (0.9856)  cardinality_error_3_unscaled: 28.5000 (40.3710)  loss_ce_4_unscaled: 1.1891 (1.2566)  loss_bbox_4_unscaled: 0.1684 (0.1850)  loss_giou_4_unscaled: 0.9228 (0.9882)  cardinality_error_4_unscaled: 13.0000 (17.9194)  time: 0.2832  data: 0.0109  max mem: 2666\n",
            "Epoch: [3]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 15.38  loss: 22.1346 (24.7131)  loss_ce: 1.2323 (1.2961)  loss_bbox: 0.7406 (0.8151)  loss_giou: 1.8304 (1.9177)  loss_ce_0: 1.2147 (1.2899)  loss_bbox_0: 0.7249 (0.8138)  loss_giou_0: 1.7006 (1.9024)  loss_ce_1: 1.2502 (1.3118)  loss_bbox_1: 0.7981 (0.9019)  loss_giou_1: 1.9504 (2.0291)  loss_ce_2: 1.2363 (1.3040)  loss_bbox_2: 0.7328 (0.8938)  loss_giou_2: 1.7911 (1.9730)  loss_ce_3: 1.2232 (1.3018)  loss_bbox_3: 0.7271 (0.8971)  loss_giou_3: 1.7806 (1.9636)  loss_ce_4: 1.1960 (1.2605)  loss_bbox_4: 0.7256 (0.8877)  loss_giou_4: 1.7874 (1.9536)  loss_ce_unscaled: 1.2323 (1.2961)  class_error_unscaled: 45.4545 (61.5434)  loss_bbox_unscaled: 0.1481 (0.1630)  loss_giou_unscaled: 0.9152 (0.9589)  cardinality_error_unscaled: 53.5000 (41.2683)  loss_ce_0_unscaled: 1.2147 (1.2899)  loss_bbox_0_unscaled: 0.1450 (0.1628)  loss_giou_0_unscaled: 0.8503 (0.9512)  cardinality_error_0_unscaled: 56.5000 (38.4024)  loss_ce_1_unscaled: 1.2502 (1.3118)  loss_bbox_1_unscaled: 0.1596 (0.1804)  loss_giou_1_unscaled: 0.9752 (1.0146)  cardinality_error_1_unscaled: 62.5000 (43.9878)  loss_ce_2_unscaled: 1.2363 (1.3040)  loss_bbox_2_unscaled: 0.1466 (0.1788)  loss_giou_2_unscaled: 0.8955 (0.9865)  cardinality_error_2_unscaled: 74.0000 (43.5610)  loss_ce_3_unscaled: 1.2232 (1.3018)  loss_bbox_3_unscaled: 0.1454 (0.1794)  loss_giou_3_unscaled: 0.8903 (0.9818)  cardinality_error_3_unscaled: 73.5000 (50.4390)  loss_ce_4_unscaled: 1.1960 (1.2605)  loss_bbox_4_unscaled: 0.1451 (0.1775)  loss_giou_4_unscaled: 0.8937 (0.9768)  cardinality_error_4_unscaled: 50.0000 (31.4146)  time: 0.2797  data: 0.0108  max mem: 2666\n",
            "Epoch: [3]  [ 50/100]  eta: 0:00:14  lr: 0.000100  class_error: 76.67  loss: 23.4522 (24.8094)  loss_ce: 1.3040 (1.3160)  loss_bbox: 0.8118 (0.8178)  loss_giou: 1.8376 (1.9397)  loss_ce_0: 1.2832 (1.3075)  loss_bbox_0: 0.7299 (0.8147)  loss_giou_0: 1.8552 (1.9360)  loss_ce_1: 1.3210 (1.3293)  loss_bbox_1: 0.7297 (0.8844)  loss_giou_1: 1.8714 (2.0444)  loss_ce_2: 1.3297 (1.3192)  loss_bbox_2: 0.7074 (0.8633)  loss_giou_2: 1.8553 (1.9714)  loss_ce_3: 1.3007 (1.3157)  loss_bbox_3: 0.6982 (0.8690)  loss_giou_3: 1.9063 (1.9716)  loss_ce_4: 1.3005 (1.2844)  loss_bbox_4: 0.7181 (0.8628)  loss_giou_4: 1.8553 (1.9622)  loss_ce_unscaled: 1.3040 (1.3160)  class_error_unscaled: 47.1698 (60.5980)  loss_bbox_unscaled: 0.1624 (0.1636)  loss_giou_unscaled: 0.9188 (0.9698)  cardinality_error_unscaled: 60.0000 (44.6373)  loss_ce_0_unscaled: 1.2832 (1.3075)  loss_bbox_0_unscaled: 0.1460 (0.1629)  loss_giou_0_unscaled: 0.9276 (0.9680)  cardinality_error_0_unscaled: 60.5000 (40.8725)  loss_ce_1_unscaled: 1.3210 (1.3293)  loss_bbox_1_unscaled: 0.1459 (0.1769)  loss_giou_1_unscaled: 0.9357 (1.0222)  cardinality_error_1_unscaled: 53.5000 (44.5784)  loss_ce_2_unscaled: 1.3297 (1.3192)  loss_bbox_2_unscaled: 0.1415 (0.1727)  loss_giou_2_unscaled: 0.9276 (0.9857)  cardinality_error_2_unscaled: 67.5000 (44.2157)  loss_ce_3_unscaled: 1.3007 (1.3157)  loss_bbox_3_unscaled: 0.1396 (0.1738)  loss_giou_3_unscaled: 0.9532 (0.9858)  cardinality_error_3_unscaled: 71.5000 (50.0098)  loss_ce_4_unscaled: 1.3005 (1.2844)  loss_bbox_4_unscaled: 0.1436 (0.1726)  loss_giou_4_unscaled: 0.9277 (0.9811)  cardinality_error_4_unscaled: 38.5000 (30.6176)  time: 0.2682  data: 0.0109  max mem: 2666\n",
            "Epoch: [3]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 48.00  loss: 23.7282 (24.4898)  loss_ce: 1.1646 (1.3018)  loss_bbox: 0.7150 (0.7919)  loss_giou: 1.8202 (1.9217)  loss_ce_0: 1.1860 (1.2938)  loss_bbox_0: 0.7465 (0.8251)  loss_giou_0: 2.0035 (1.9620)  loss_ce_1: 1.2367 (1.3184)  loss_bbox_1: 0.7226 (0.8479)  loss_giou_1: 1.8279 (2.0051)  loss_ce_2: 1.2157 (1.3073)  loss_bbox_2: 0.6967 (0.8307)  loss_giou_2: 1.8328 (1.9514)  loss_ce_3: 1.2418 (1.2999)  loss_bbox_3: 0.6982 (0.8334)  loss_giou_3: 1.9321 (1.9589)  loss_ce_4: 1.1779 (1.2714)  loss_bbox_4: 0.6923 (0.8275)  loss_giou_4: 1.8497 (1.9416)  loss_ce_unscaled: 1.1646 (1.3018)  class_error_unscaled: 48.1481 (59.1850)  loss_bbox_unscaled: 0.1430 (0.1584)  loss_giou_unscaled: 0.9101 (0.9608)  cardinality_error_unscaled: 56.0000 (45.4508)  loss_ce_0_unscaled: 1.1860 (1.2938)  loss_bbox_0_unscaled: 0.1493 (0.1650)  loss_giou_0_unscaled: 1.0018 (0.9810)  cardinality_error_0_unscaled: 60.5000 (45.1721)  loss_ce_1_unscaled: 1.2367 (1.3184)  loss_bbox_1_unscaled: 0.1445 (0.1696)  loss_giou_1_unscaled: 0.9140 (1.0025)  cardinality_error_1_unscaled: 66.0000 (49.3770)  loss_ce_2_unscaled: 1.2157 (1.3073)  loss_bbox_2_unscaled: 0.1393 (0.1661)  loss_giou_2_unscaled: 0.9164 (0.9757)  cardinality_error_2_unscaled: 53.0000 (46.6803)  loss_ce_3_unscaled: 1.2418 (1.2999)  loss_bbox_3_unscaled: 0.1396 (0.1667)  loss_giou_3_unscaled: 0.9660 (0.9795)  cardinality_error_3_unscaled: 48.5000 (50.4262)  loss_ce_4_unscaled: 1.1779 (1.2714)  loss_bbox_4_unscaled: 0.1385 (0.1655)  loss_giou_4_unscaled: 0.9249 (0.9708)  cardinality_error_4_unscaled: 31.0000 (31.5410)  time: 0.2690  data: 0.0111  max mem: 2666\n",
            "Epoch: [3]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 36.36  loss: 22.7764 (24.3868)  loss_ce: 1.1620 (1.2962)  loss_bbox: 0.6894 (0.7793)  loss_giou: 1.7657 (1.9201)  loss_ce_0: 1.1860 (1.2916)  loss_bbox_0: 0.7941 (0.8267)  loss_giou_0: 2.0626 (1.9748)  loss_ce_1: 1.2319 (1.3138)  loss_bbox_1: 0.6382 (0.8285)  loss_giou_1: 1.8024 (1.9862)  loss_ce_2: 1.1945 (1.2999)  loss_bbox_2: 0.6697 (0.8145)  loss_giou_2: 1.8132 (1.9412)  loss_ce_3: 1.1442 (1.2926)  loss_bbox_3: 0.6470 (0.8198)  loss_giou_3: 1.8665 (1.9624)  loss_ce_4: 1.1570 (1.2655)  loss_bbox_4: 0.6790 (0.8201)  loss_giou_4: 1.8497 (1.9538)  loss_ce_unscaled: 1.1620 (1.2962)  class_error_unscaled: 39.1304 (55.4823)  loss_bbox_unscaled: 0.1379 (0.1559)  loss_giou_unscaled: 0.8828 (0.9600)  cardinality_error_unscaled: 56.5000 (48.9718)  loss_ce_0_unscaled: 1.1860 (1.2916)  loss_bbox_0_unscaled: 0.1588 (0.1653)  loss_giou_0_unscaled: 1.0313 (0.9874)  cardinality_error_0_unscaled: 68.0000 (48.5070)  loss_ce_1_unscaled: 1.2319 (1.3138)  loss_bbox_1_unscaled: 0.1276 (0.1657)  loss_giou_1_unscaled: 0.9012 (0.9931)  cardinality_error_1_unscaled: 76.5000 (53.2394)  loss_ce_2_unscaled: 1.1945 (1.2999)  loss_bbox_2_unscaled: 0.1339 (0.1629)  loss_giou_2_unscaled: 0.9066 (0.9706)  cardinality_error_2_unscaled: 73.0000 (50.9577)  loss_ce_3_unscaled: 1.1442 (1.2926)  loss_bbox_3_unscaled: 0.1294 (0.1640)  loss_giou_3_unscaled: 0.9332 (0.9812)  cardinality_error_3_unscaled: 67.5000 (54.1408)  loss_ce_4_unscaled: 1.1570 (1.2655)  loss_bbox_4_unscaled: 0.1358 (0.1640)  loss_giou_4_unscaled: 0.9249 (0.9769)  cardinality_error_4_unscaled: 46.5000 (36.3451)  time: 0.2801  data: 0.0117  max mem: 2666\n",
            "Epoch: [3]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 56.25  loss: 24.1652 (24.3868)  loss_ce: 1.2435 (1.3042)  loss_bbox: 0.6639 (0.7693)  loss_giou: 1.7318 (1.9001)  loss_ce_0: 1.2414 (1.2983)  loss_bbox_0: 0.8138 (0.8396)  loss_giou_0: 2.0434 (1.9815)  loss_ce_1: 1.2954 (1.3223)  loss_bbox_1: 0.7087 (0.8320)  loss_giou_1: 1.8446 (1.9806)  loss_ce_2: 1.2749 (1.3114)  loss_bbox_2: 0.7796 (0.8238)  loss_giou_2: 1.7387 (1.9251)  loss_ce_3: 1.2540 (1.3012)  loss_bbox_3: 0.7223 (0.8221)  loss_giou_3: 1.8146 (1.9543)  loss_ce_4: 1.2955 (1.2808)  loss_bbox_4: 0.6926 (0.8100)  loss_giou_4: 1.8077 (1.9300)  loss_ce_unscaled: 1.2435 (1.3042)  class_error_unscaled: 34.4828 (54.1932)  loss_bbox_unscaled: 0.1328 (0.1539)  loss_giou_unscaled: 0.8659 (0.9501)  cardinality_error_unscaled: 70.5000 (50.5741)  loss_ce_0_unscaled: 1.2414 (1.2983)  loss_bbox_0_unscaled: 0.1628 (0.1679)  loss_giou_0_unscaled: 1.0217 (0.9908)  cardinality_error_0_unscaled: 65.0000 (48.4506)  loss_ce_1_unscaled: 1.2954 (1.3223)  loss_bbox_1_unscaled: 0.1417 (0.1664)  loss_giou_1_unscaled: 0.9223 (0.9903)  cardinality_error_1_unscaled: 74.0000 (53.1790)  loss_ce_2_unscaled: 1.2749 (1.3114)  loss_bbox_2_unscaled: 0.1559 (0.1648)  loss_giou_2_unscaled: 0.8694 (0.9625)  cardinality_error_2_unscaled: 74.5000 (51.5926)  loss_ce_3_unscaled: 1.2540 (1.3012)  loss_bbox_3_unscaled: 0.1445 (0.1644)  loss_giou_3_unscaled: 0.9073 (0.9772)  cardinality_error_3_unscaled: 75.5000 (55.0432)  loss_ce_4_unscaled: 1.2955 (1.2808)  loss_bbox_4_unscaled: 0.1385 (0.1620)  loss_giou_4_unscaled: 0.9039 (0.9650)  cardinality_error_4_unscaled: 65.5000 (39.9877)  time: 0.2763  data: 0.0115  max mem: 2666\n",
            "Epoch: [3]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 65.22  loss: 24.3738 (24.3801)  loss_ce: 1.2099 (1.3014)  loss_bbox: 0.7340 (0.7712)  loss_giou: 1.7907 (1.9022)  loss_ce_0: 1.2115 (1.2919)  loss_bbox_0: 0.8240 (0.8490)  loss_giou_0: 1.9200 (1.9752)  loss_ce_1: 1.2462 (1.3166)  loss_bbox_1: 0.8130 (0.8342)  loss_giou_1: 1.9293 (1.9778)  loss_ce_2: 1.2432 (1.3070)  loss_bbox_2: 0.8377 (0.8327)  loss_giou_2: 1.8458 (1.9254)  loss_ce_3: 1.2225 (1.2963)  loss_bbox_3: 0.8240 (0.8266)  loss_giou_3: 1.8284 (1.9504)  loss_ce_4: 1.2340 (1.2837)  loss_bbox_4: 0.7570 (0.8084)  loss_giou_4: 1.8283 (1.9302)  loss_ce_unscaled: 1.2099 (1.3014)  class_error_unscaled: 61.1111 (56.1239)  loss_bbox_unscaled: 0.1468 (0.1542)  loss_giou_unscaled: 0.8953 (0.9511)  cardinality_error_unscaled: 42.0000 (48.4725)  loss_ce_0_unscaled: 1.2115 (1.2919)  loss_bbox_0_unscaled: 0.1648 (0.1698)  loss_giou_0_unscaled: 0.9600 (0.9876)  cardinality_error_0_unscaled: 38.5000 (46.9560)  loss_ce_1_unscaled: 1.2462 (1.3166)  loss_bbox_1_unscaled: 0.1626 (0.1668)  loss_giou_1_unscaled: 0.9647 (0.9889)  cardinality_error_1_unscaled: 45.0000 (51.2747)  loss_ce_2_unscaled: 1.2432 (1.3070)  loss_bbox_2_unscaled: 0.1675 (0.1665)  loss_giou_2_unscaled: 0.9229 (0.9627)  cardinality_error_2_unscaled: 44.5000 (49.6374)  loss_ce_3_unscaled: 1.2225 (1.2963)  loss_bbox_3_unscaled: 0.1648 (0.1653)  loss_giou_3_unscaled: 0.9142 (0.9752)  cardinality_error_3_unscaled: 44.0000 (52.4396)  loss_ce_4_unscaled: 1.2340 (1.2837)  loss_bbox_4_unscaled: 0.1514 (0.1617)  loss_giou_4_unscaled: 0.9142 (0.9651)  cardinality_error_4_unscaled: 52.0000 (38.8571)  time: 0.2607  data: 0.0111  max mem: 2666\n",
            "Epoch: [3]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 55.10  loss: 24.2744 (24.4076)  loss_ce: 1.2156 (1.3040)  loss_bbox: 0.7939 (0.7782)  loss_giou: 1.8561 (1.8979)  loss_ce_0: 1.2382 (1.2969)  loss_bbox_0: 0.9303 (0.8652)  loss_giou_0: 1.9371 (1.9756)  loss_ce_1: 1.2471 (1.3205)  loss_bbox_1: 0.8142 (0.8345)  loss_giou_1: 1.9293 (1.9665)  loss_ce_2: 1.2432 (1.3092)  loss_bbox_2: 0.8500 (0.8445)  loss_giou_2: 1.8971 (1.9286)  loss_ce_3: 1.2225 (1.2991)  loss_bbox_3: 0.8292 (0.8280)  loss_giou_3: 1.8284 (1.9397)  loss_ce_4: 1.2220 (1.2865)  loss_bbox_4: 0.7926 (0.8109)  loss_giou_4: 1.8665 (1.9218)  loss_ce_unscaled: 1.2156 (1.3040)  class_error_unscaled: 57.8947 (54.3735)  loss_bbox_unscaled: 0.1588 (0.1556)  loss_giou_unscaled: 0.9280 (0.9490)  cardinality_error_unscaled: 62.5000 (51.0850)  loss_ce_0_unscaled: 1.2382 (1.2969)  loss_bbox_0_unscaled: 0.1861 (0.1730)  loss_giou_0_unscaled: 0.9686 (0.9878)  cardinality_error_0_unscaled: 55.0000 (49.4950)  loss_ce_1_unscaled: 1.2471 (1.3205)  loss_bbox_1_unscaled: 0.1628 (0.1669)  loss_giou_1_unscaled: 0.9647 (0.9833)  cardinality_error_1_unscaled: 67.0000 (53.8650)  loss_ce_2_unscaled: 1.2432 (1.3092)  loss_bbox_2_unscaled: 0.1700 (0.1689)  loss_giou_2_unscaled: 0.9486 (0.9643)  cardinality_error_2_unscaled: 44.5000 (51.3250)  loss_ce_3_unscaled: 1.2225 (1.2991)  loss_bbox_3_unscaled: 0.1658 (0.1656)  loss_giou_3_unscaled: 0.9142 (0.9698)  cardinality_error_3_unscaled: 67.0000 (54.6100)  loss_ce_4_unscaled: 1.2220 (1.2865)  loss_bbox_4_unscaled: 0.1585 (0.1622)  loss_giou_4_unscaled: 0.9333 (0.9609)  cardinality_error_4_unscaled: 33.0000 (40.1450)  time: 0.2690  data: 0.0109  max mem: 2666\n",
            "Epoch: [3] Total time: 0:00:27 (0.2769 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 55.10  loss: 24.2744 (24.4076)  loss_ce: 1.2156 (1.3040)  loss_bbox: 0.7939 (0.7782)  loss_giou: 1.8561 (1.8979)  loss_ce_0: 1.2382 (1.2969)  loss_bbox_0: 0.9303 (0.8652)  loss_giou_0: 1.9371 (1.9756)  loss_ce_1: 1.2471 (1.3205)  loss_bbox_1: 0.8142 (0.8345)  loss_giou_1: 1.9293 (1.9665)  loss_ce_2: 1.2432 (1.3092)  loss_bbox_2: 0.8500 (0.8445)  loss_giou_2: 1.8971 (1.9286)  loss_ce_3: 1.2225 (1.2991)  loss_bbox_3: 0.8292 (0.8280)  loss_giou_3: 1.8284 (1.9397)  loss_ce_4: 1.2220 (1.2865)  loss_bbox_4: 0.7926 (0.8109)  loss_giou_4: 1.8665 (1.9218)  loss_ce_unscaled: 1.2156 (1.3040)  class_error_unscaled: 57.8947 (54.3735)  loss_bbox_unscaled: 0.1588 (0.1556)  loss_giou_unscaled: 0.9280 (0.9490)  cardinality_error_unscaled: 62.5000 (51.0850)  loss_ce_0_unscaled: 1.2382 (1.2969)  loss_bbox_0_unscaled: 0.1861 (0.1730)  loss_giou_0_unscaled: 0.9686 (0.9878)  cardinality_error_0_unscaled: 55.0000 (49.4950)  loss_ce_1_unscaled: 1.2471 (1.3205)  loss_bbox_1_unscaled: 0.1628 (0.1669)  loss_giou_1_unscaled: 0.9647 (0.9833)  cardinality_error_1_unscaled: 67.0000 (53.8650)  loss_ce_2_unscaled: 1.2432 (1.3092)  loss_bbox_2_unscaled: 0.1700 (0.1689)  loss_giou_2_unscaled: 0.9486 (0.9643)  cardinality_error_2_unscaled: 44.5000 (51.3250)  loss_ce_3_unscaled: 1.2225 (1.2991)  loss_bbox_3_unscaled: 0.1658 (0.1656)  loss_giou_3_unscaled: 0.9142 (0.9698)  cardinality_error_3_unscaled: 67.0000 (54.6100)  loss_ce_4_unscaled: 1.2220 (1.2865)  loss_bbox_4_unscaled: 0.1585 (0.1622)  loss_giou_4_unscaled: 0.9333 (0.9609)  cardinality_error_4_unscaled: 33.0000 (40.1450)\n",
            "Test:  [ 0/15]  eta: 0:00:06  class_error: 86.36  loss: 41.8674 (41.8674)  loss_ce: 1.8066 (1.8066)  loss_bbox: 2.1871 (2.1871)  loss_giou: 3.3035 (3.3035)  loss_ce_0: 1.7391 (1.7391)  loss_bbox_0: 2.2189 (2.2189)  loss_giou_0: 3.3246 (3.3246)  loss_ce_1: 1.8148 (1.8148)  loss_bbox_1: 1.8252 (1.8252)  loss_giou_1: 3.4893 (3.4893)  loss_ce_2: 1.7869 (1.7869)  loss_bbox_2: 1.8650 (1.8650)  loss_giou_2: 3.2242 (3.2242)  loss_ce_3: 1.7403 (1.7403)  loss_bbox_3: 1.6788 (1.6788)  loss_giou_3: 3.3113 (3.3113)  loss_ce_4: 1.7463 (1.7463)  loss_bbox_4: 1.5919 (1.5919)  loss_giou_4: 3.2133 (3.2133)  loss_ce_unscaled: 1.8066 (1.8066)  class_error_unscaled: 86.3636 (86.3636)  loss_bbox_unscaled: 0.4374 (0.4374)  loss_giou_unscaled: 1.6518 (1.6518)  cardinality_error_unscaled: 78.0000 (78.0000)  loss_ce_0_unscaled: 1.7391 (1.7391)  loss_bbox_0_unscaled: 0.4438 (0.4438)  loss_giou_0_unscaled: 1.6623 (1.6623)  cardinality_error_0_unscaled: 22.0000 (22.0000)  loss_ce_1_unscaled: 1.8148 (1.8148)  loss_bbox_1_unscaled: 0.3650 (0.3650)  loss_giou_1_unscaled: 1.7447 (1.7447)  cardinality_error_1_unscaled: 78.0000 (78.0000)  loss_ce_2_unscaled: 1.7869 (1.7869)  loss_bbox_2_unscaled: 0.3730 (0.3730)  loss_giou_2_unscaled: 1.6121 (1.6121)  cardinality_error_2_unscaled: 22.0000 (22.0000)  loss_ce_3_unscaled: 1.7403 (1.7403)  loss_bbox_3_unscaled: 0.3358 (0.3358)  loss_giou_3_unscaled: 1.6557 (1.6557)  cardinality_error_3_unscaled: 78.0000 (78.0000)  loss_ce_4_unscaled: 1.7463 (1.7463)  loss_bbox_4_unscaled: 0.3184 (0.3184)  loss_giou_4_unscaled: 1.6067 (1.6067)  cardinality_error_4_unscaled: 78.0000 (78.0000)  time: 0.4179  data: 0.2264  max mem: 2666\n",
            "Test:  [10/15]  eta: 0:00:01  class_error: 25.71  loss: 39.5976 (39.8351)  loss_ce: 1.3572 (1.4264)  loss_bbox: 1.8831 (1.8916)  loss_giou: 3.3035 (3.3151)  loss_ce_0: 1.4083 (1.4265)  loss_bbox_0: 1.8756 (1.9050)  loss_giou_0: 3.3246 (3.3290)  loss_ce_1: 1.3801 (1.4388)  loss_bbox_1: 1.6012 (1.6458)  loss_giou_1: 3.2026 (3.2650)  loss_ce_2: 1.4134 (1.4410)  loss_bbox_2: 2.4672 (2.4702)  loss_giou_2: 3.3632 (3.4081)  loss_ce_3: 1.3602 (1.4151)  loss_bbox_3: 1.6955 (1.6959)  loss_giou_3: 3.2344 (3.2245)  loss_ce_4: 1.3649 (1.4175)  loss_bbox_4: 1.8995 (1.8447)  loss_giou_4: 3.2865 (3.2747)  loss_ce_unscaled: 1.3572 (1.4264)  class_error_unscaled: 32.6531 (45.1222)  loss_bbox_unscaled: 0.3766 (0.3783)  loss_giou_unscaled: 1.6518 (1.6576)  cardinality_error_unscaled: 82.5000 (81.2727)  loss_ce_0_unscaled: 1.4083 (1.4265)  loss_bbox_0_unscaled: 0.3751 (0.3810)  loss_giou_0_unscaled: 1.6623 (1.6645)  cardinality_error_0_unscaled: 17.5000 (18.7273)  loss_ce_1_unscaled: 1.3801 (1.4388)  loss_bbox_1_unscaled: 0.3202 (0.3292)  loss_giou_1_unscaled: 1.6013 (1.6325)  cardinality_error_1_unscaled: 82.5000 (81.2727)  loss_ce_2_unscaled: 1.4134 (1.4410)  loss_bbox_2_unscaled: 0.4934 (0.4940)  loss_giou_2_unscaled: 1.6816 (1.7040)  cardinality_error_2_unscaled: 17.5000 (18.7273)  loss_ce_3_unscaled: 1.3602 (1.4151)  loss_bbox_3_unscaled: 0.3391 (0.3392)  loss_giou_3_unscaled: 1.6172 (1.6123)  cardinality_error_3_unscaled: 82.5000 (81.2727)  loss_ce_4_unscaled: 1.3649 (1.4175)  loss_bbox_4_unscaled: 0.3799 (0.3689)  loss_giou_4_unscaled: 1.6433 (1.6373)  cardinality_error_4_unscaled: 82.5000 (81.2727)  time: 0.2123  data: 0.0313  max mem: 2666\n",
            "Test:  [14/15]  eta: 0:00:00  class_error: 72.86  loss: 39.5976 (39.5024)  loss_ce: 1.3572 (1.4384)  loss_bbox: 1.7091 (1.8149)  loss_giou: 3.2630 (3.2790)  loss_ce_0: 1.4083 (1.4407)  loss_bbox_0: 1.8224 (1.8236)  loss_giou_0: 3.2822 (3.2877)  loss_ce_1: 1.3801 (1.4511)  loss_bbox_1: 1.4972 (1.5722)  loss_giou_1: 3.2026 (3.2332)  loss_ce_2: 1.4134 (1.4590)  loss_bbox_2: 2.4118 (2.4706)  loss_giou_2: 3.4497 (3.4242)  loss_ce_3: 1.3602 (1.4270)  loss_bbox_3: 1.6788 (1.6406)  loss_giou_3: 3.2492 (3.2078)  loss_ce_4: 1.3649 (1.4306)  loss_bbox_4: 1.8922 (1.8137)  loss_giou_4: 3.3124 (3.2880)  loss_ce_unscaled: 1.3572 (1.4384)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.3418 (0.3630)  loss_giou_unscaled: 1.6315 (1.6395)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.4083 (1.4407)  loss_bbox_0_unscaled: 0.3645 (0.3647)  loss_giou_0_unscaled: 1.6411 (1.6439)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.3801 (1.4511)  loss_bbox_1_unscaled: 0.2994 (0.3144)  loss_giou_1_unscaled: 1.6013 (1.6166)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.4134 (1.4590)  loss_bbox_2_unscaled: 0.4824 (0.4941)  loss_giou_2_unscaled: 1.7249 (1.7121)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.3602 (1.4270)  loss_bbox_3_unscaled: 0.3358 (0.3281)  loss_giou_3_unscaled: 1.6246 (1.6039)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.3649 (1.4306)  loss_bbox_4_unscaled: 0.3784 (0.3627)  loss_giou_4_unscaled: 1.6562 (1.6440)  cardinality_error_4_unscaled: 82.5000 (79.6000)  time: 0.2072  data: 0.0257  max mem: 2666\n",
            "Test: Total time: 0:00:03 (0.2109 s / it)\n",
            "Averaged stats: class_error: 72.86  loss: 39.5976 (39.5024)  loss_ce: 1.3572 (1.4384)  loss_bbox: 1.7091 (1.8149)  loss_giou: 3.2630 (3.2790)  loss_ce_0: 1.4083 (1.4407)  loss_bbox_0: 1.8224 (1.8236)  loss_giou_0: 3.2822 (3.2877)  loss_ce_1: 1.3801 (1.4511)  loss_bbox_1: 1.4972 (1.5722)  loss_giou_1: 3.2026 (3.2332)  loss_ce_2: 1.4134 (1.4590)  loss_bbox_2: 2.4118 (2.4706)  loss_giou_2: 3.4497 (3.4242)  loss_ce_3: 1.3602 (1.4270)  loss_bbox_3: 1.6788 (1.6406)  loss_giou_3: 3.2492 (3.2078)  loss_ce_4: 1.3649 (1.4306)  loss_bbox_4: 1.8922 (1.8137)  loss_giou_4: 3.3124 (3.2880)  loss_ce_unscaled: 1.3572 (1.4384)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.3418 (0.3630)  loss_giou_unscaled: 1.6315 (1.6395)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.4083 (1.4407)  loss_bbox_0_unscaled: 0.3645 (0.3647)  loss_giou_0_unscaled: 1.6411 (1.6439)  cardinality_error_0_unscaled: 17.5000 (20.4000)  loss_ce_1_unscaled: 1.3801 (1.4511)  loss_bbox_1_unscaled: 0.2994 (0.3144)  loss_giou_1_unscaled: 1.6013 (1.6166)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.4134 (1.4590)  loss_bbox_2_unscaled: 0.4824 (0.4941)  loss_giou_2_unscaled: 1.7249 (1.7121)  cardinality_error_2_unscaled: 17.5000 (20.4000)  loss_ce_3_unscaled: 1.3602 (1.4270)  loss_bbox_3_unscaled: 0.3358 (0.3281)  loss_giou_3_unscaled: 1.6246 (1.6039)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.3649 (1.4306)  loss_bbox_4_unscaled: 0.3784 (0.3627)  loss_giou_4_unscaled: 1.6562 (1.6440)  cardinality_error_4_unscaled: 82.5000 (79.6000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [4]  [  0/100]  eta: 0:01:02  lr: 0.000100  class_error: 15.15  loss: 22.4629 (22.4629)  loss_ce: 0.9360 (0.9360)  loss_bbox: 0.9474 (0.9474)  loss_giou: 1.7100 (1.7100)  loss_ce_0: 0.9503 (0.9503)  loss_bbox_0: 0.9868 (0.9868)  loss_giou_0: 1.7901 (1.7901)  loss_ce_1: 0.9232 (0.9232)  loss_bbox_1: 0.9341 (0.9341)  loss_giou_1: 1.7112 (1.7112)  loss_ce_2: 0.8548 (0.8548)  loss_bbox_2: 1.2296 (1.2296)  loss_giou_2: 2.1584 (2.1584)  loss_ce_3: 0.9069 (0.9069)  loss_bbox_3: 0.9799 (0.9799)  loss_giou_3: 1.8298 (1.8298)  loss_ce_4: 0.8403 (0.8403)  loss_bbox_4: 0.9850 (0.9850)  loss_giou_4: 1.7890 (1.7890)  loss_ce_unscaled: 0.9360 (0.9360)  class_error_unscaled: 15.1515 (15.1515)  loss_bbox_unscaled: 0.1895 (0.1895)  loss_giou_unscaled: 0.8550 (0.8550)  cardinality_error_unscaled: 65.0000 (65.0000)  loss_ce_0_unscaled: 0.9503 (0.9503)  loss_bbox_0_unscaled: 0.1974 (0.1974)  loss_giou_0_unscaled: 0.8950 (0.8950)  cardinality_error_0_unscaled: 35.0000 (35.0000)  loss_ce_1_unscaled: 0.9232 (0.9232)  loss_bbox_1_unscaled: 0.1868 (0.1868)  loss_giou_1_unscaled: 0.8556 (0.8556)  cardinality_error_1_unscaled: 67.0000 (67.0000)  loss_ce_2_unscaled: 0.8548 (0.8548)  loss_bbox_2_unscaled: 0.2459 (0.2459)  loss_giou_2_unscaled: 1.0792 (1.0792)  cardinality_error_2_unscaled: 24.0000 (24.0000)  loss_ce_3_unscaled: 0.9069 (0.9069)  loss_bbox_3_unscaled: 0.1960 (0.1960)  loss_giou_3_unscaled: 0.9149 (0.9149)  cardinality_error_3_unscaled: 56.0000 (56.0000)  loss_ce_4_unscaled: 0.8403 (0.8403)  loss_bbox_4_unscaled: 0.1970 (0.1970)  loss_giou_4_unscaled: 0.8945 (0.8945)  cardinality_error_4_unscaled: 44.5000 (44.5000)  time: 0.6245  data: 0.2215  max mem: 2666\n",
            "Epoch: [4]  [ 10/100]  eta: 0:00:28  lr: 0.000100  class_error: 42.86  loss: 23.1310 (23.2109)  loss_ce: 1.2433 (1.2781)  loss_bbox: 0.8635 (0.8481)  loss_giou: 1.8871 (1.9538)  loss_ce_0: 1.2912 (1.2776)  loss_bbox_0: 0.6932 (0.7282)  loss_giou_0: 1.7901 (1.7897)  loss_ce_1: 1.2832 (1.2810)  loss_bbox_1: 0.6647 (0.6826)  loss_giou_1: 1.7112 (1.7238)  loss_ce_2: 1.2368 (1.2444)  loss_bbox_2: 0.8009 (0.8197)  loss_giou_2: 1.8205 (1.8795)  loss_ce_3: 1.2619 (1.2742)  loss_bbox_3: 0.7394 (0.7547)  loss_giou_3: 1.7399 (1.7625)  loss_ce_4: 1.2673 (1.2925)  loss_bbox_4: 0.7836 (0.7591)  loss_giou_4: 1.8004 (1.8614)  loss_ce_unscaled: 1.2433 (1.2781)  class_error_unscaled: 32.0755 (38.8566)  loss_bbox_unscaled: 0.1727 (0.1696)  loss_giou_unscaled: 0.9435 (0.9769)  cardinality_error_unscaled: 68.0000 (70.5909)  loss_ce_0_unscaled: 1.2912 (1.2776)  loss_bbox_0_unscaled: 0.1386 (0.1456)  loss_giou_0_unscaled: 0.8950 (0.8949)  cardinality_error_0_unscaled: 32.5000 (28.6818)  loss_ce_1_unscaled: 1.2832 (1.2810)  loss_bbox_1_unscaled: 0.1329 (0.1365)  loss_giou_1_unscaled: 0.8556 (0.8619)  cardinality_error_1_unscaled: 56.0000 (56.2727)  loss_ce_2_unscaled: 1.2368 (1.2444)  loss_bbox_2_unscaled: 0.1602 (0.1639)  loss_giou_2_unscaled: 0.9103 (0.9397)  cardinality_error_2_unscaled: 26.5000 (33.6364)  loss_ce_3_unscaled: 1.2619 (1.2742)  loss_bbox_3_unscaled: 0.1479 (0.1509)  loss_giou_3_unscaled: 0.8700 (0.8813)  cardinality_error_3_unscaled: 65.5000 (66.7273)  loss_ce_4_unscaled: 1.2673 (1.2925)  loss_bbox_4_unscaled: 0.1567 (0.1518)  loss_giou_4_unscaled: 0.9002 (0.9307)  cardinality_error_4_unscaled: 69.0000 (69.0909)  time: 0.3154  data: 0.0305  max mem: 2666\n",
            "Epoch: [4]  [ 20/100]  eta: 0:00:23  lr: 0.000100  class_error: 55.17  loss: 22.0965 (22.6660)  loss_ce: 1.2018 (1.2392)  loss_bbox: 0.6890 (0.7671)  loss_giou: 1.8024 (1.8366)  loss_ce_0: 1.1651 (1.2390)  loss_bbox_0: 0.6934 (0.7472)  loss_giou_0: 1.8089 (1.8073)  loss_ce_1: 1.1960 (1.2399)  loss_bbox_1: 0.6549 (0.6895)  loss_giou_1: 1.7145 (1.7549)  loss_ce_2: 1.1449 (1.2010)  loss_bbox_2: 0.7259 (0.7790)  loss_giou_2: 1.7773 (1.8269)  loss_ce_3: 1.1788 (1.2159)  loss_bbox_3: 0.7394 (0.7496)  loss_giou_3: 1.7285 (1.8121)  loss_ce_4: 1.2373 (1.2531)  loss_bbox_4: 0.7076 (0.7200)  loss_giou_4: 1.7738 (1.7877)  loss_ce_unscaled: 1.2018 (1.2392)  class_error_unscaled: 34.4828 (37.6503)  loss_bbox_unscaled: 0.1378 (0.1534)  loss_giou_unscaled: 0.9012 (0.9183)  cardinality_error_unscaled: 66.0000 (66.3095)  loss_ce_0_unscaled: 1.1651 (1.2390)  loss_bbox_0_unscaled: 0.1387 (0.1494)  loss_giou_0_unscaled: 0.9045 (0.9036)  cardinality_error_0_unscaled: 57.0000 (53.4762)  loss_ce_1_unscaled: 1.1960 (1.2399)  loss_bbox_1_unscaled: 0.1310 (0.1379)  loss_giou_1_unscaled: 0.8573 (0.8775)  cardinality_error_1_unscaled: 65.0000 (63.5238)  loss_ce_2_unscaled: 1.1449 (1.2010)  loss_bbox_2_unscaled: 0.1452 (0.1558)  loss_giou_2_unscaled: 0.8886 (0.9135)  cardinality_error_2_unscaled: 46.0000 (42.9762)  loss_ce_3_unscaled: 1.1788 (1.2159)  loss_bbox_3_unscaled: 0.1479 (0.1499)  loss_giou_3_unscaled: 0.8643 (0.9060)  cardinality_error_3_unscaled: 64.5000 (61.2857)  loss_ce_4_unscaled: 1.2373 (1.2531)  loss_bbox_4_unscaled: 0.1415 (0.1440)  loss_giou_4_unscaled: 0.8869 (0.8938)  cardinality_error_4_unscaled: 70.0000 (67.0476)  time: 0.2834  data: 0.0112  max mem: 2666\n",
            "Epoch: [4]  [ 30/100]  eta: 0:00:20  lr: 0.000100  class_error: 97.67  loss: 22.4128 (23.1390)  loss_ce: 1.1784 (1.2640)  loss_bbox: 0.6791 (0.7512)  loss_giou: 1.7755 (1.8493)  loss_ce_0: 1.1567 (1.2672)  loss_bbox_0: 0.7534 (0.7536)  loss_giou_0: 1.8418 (1.8511)  loss_ce_1: 1.2053 (1.2673)  loss_bbox_1: 0.6673 (0.7017)  loss_giou_1: 1.8176 (1.8164)  loss_ce_2: 1.1872 (1.2362)  loss_bbox_2: 0.7483 (0.7688)  loss_giou_2: 1.7985 (1.8604)  loss_ce_3: 1.1735 (1.2366)  loss_bbox_3: 0.7873 (0.7630)  loss_giou_3: 1.8893 (1.8907)  loss_ce_4: 1.2130 (1.2723)  loss_bbox_4: 0.6858 (0.7369)  loss_giou_4: 1.7979 (1.8522)  loss_ce_unscaled: 1.1784 (1.2640)  class_error_unscaled: 34.4828 (39.4751)  loss_bbox_unscaled: 0.1358 (0.1502)  loss_giou_unscaled: 0.8877 (0.9246)  cardinality_error_unscaled: 63.0000 (66.0000)  loss_ce_0_unscaled: 1.1567 (1.2672)  loss_bbox_0_unscaled: 0.1507 (0.1507)  loss_giou_0_unscaled: 0.9209 (0.9256)  cardinality_error_0_unscaled: 82.0000 (61.9355)  loss_ce_1_unscaled: 1.2053 (1.2673)  loss_bbox_1_unscaled: 0.1335 (0.1403)  loss_giou_1_unscaled: 0.9088 (0.9082)  cardinality_error_1_unscaled: 65.0000 (62.2097)  loss_ce_2_unscaled: 1.1872 (1.2362)  loss_bbox_2_unscaled: 0.1497 (0.1538)  loss_giou_2_unscaled: 0.8992 (0.9302)  cardinality_error_2_unscaled: 56.0000 (48.2742)  loss_ce_3_unscaled: 1.1735 (1.2366)  loss_bbox_3_unscaled: 0.1575 (0.1526)  loss_giou_3_unscaled: 0.9446 (0.9454)  cardinality_error_3_unscaled: 59.5000 (60.5484)  loss_ce_4_unscaled: 1.2130 (1.2723)  loss_bbox_4_unscaled: 0.1372 (0.1474)  loss_giou_4_unscaled: 0.8989 (0.9261)  cardinality_error_4_unscaled: 52.5000 (60.3387)  time: 0.2739  data: 0.0112  max mem: 2666\n",
            "Epoch: [4]  [ 40/100]  eta: 0:00:17  lr: 0.000100  class_error: 60.71  loss: 23.8011 (23.3127)  loss_ce: 1.1784 (1.2620)  loss_bbox: 0.7512 (0.7627)  loss_giou: 1.8325 (1.8467)  loss_ce_0: 1.1524 (1.2603)  loss_bbox_0: 0.7534 (0.7889)  loss_giou_0: 1.8962 (1.8917)  loss_ce_1: 1.2200 (1.2668)  loss_bbox_1: 0.7123 (0.7313)  loss_giou_1: 1.8739 (1.8252)  loss_ce_2: 1.2339 (1.2507)  loss_bbox_2: 0.7584 (0.7806)  loss_giou_2: 1.8104 (1.8536)  loss_ce_3: 1.2112 (1.2438)  loss_bbox_3: 0.8069 (0.7874)  loss_giou_3: 1.9270 (1.8825)  loss_ce_4: 1.2265 (1.2673)  loss_bbox_4: 0.7604 (0.7575)  loss_giou_4: 1.8469 (1.8536)  loss_ce_unscaled: 1.1784 (1.2620)  class_error_unscaled: 44.0000 (41.5298)  loss_bbox_unscaled: 0.1502 (0.1525)  loss_giou_unscaled: 0.9162 (0.9233)  cardinality_error_unscaled: 63.0000 (62.7805)  loss_ce_0_unscaled: 1.1524 (1.2603)  loss_bbox_0_unscaled: 0.1507 (0.1578)  loss_giou_0_unscaled: 0.9481 (0.9459)  cardinality_error_0_unscaled: 69.5000 (54.3659)  loss_ce_1_unscaled: 1.2200 (1.2668)  loss_bbox_1_unscaled: 0.1425 (0.1463)  loss_giou_1_unscaled: 0.9369 (0.9126)  cardinality_error_1_unscaled: 45.5000 (54.7073)  loss_ce_2_unscaled: 1.2339 (1.2507)  loss_bbox_2_unscaled: 0.1517 (0.1561)  loss_giou_2_unscaled: 0.9052 (0.9268)  cardinality_error_2_unscaled: 55.0000 (48.1829)  loss_ce_3_unscaled: 1.2112 (1.2438)  loss_bbox_3_unscaled: 0.1614 (0.1575)  loss_giou_3_unscaled: 0.9635 (0.9413)  cardinality_error_3_unscaled: 53.5000 (58.0244)  loss_ce_4_unscaled: 1.2265 (1.2673)  loss_bbox_4_unscaled: 0.1521 (0.1515)  loss_giou_4_unscaled: 0.9234 (0.9268)  cardinality_error_4_unscaled: 47.0000 (55.9634)  time: 0.2670  data: 0.0115  max mem: 2666\n",
            "Epoch: [4]  [ 50/100]  eta: 0:00:14  lr: 0.000100  class_error: 14.29  loss: 24.7191 (23.5299)  loss_ce: 1.2538 (1.2686)  loss_bbox: 0.7822 (0.7759)  loss_giou: 1.8862 (1.8894)  loss_ce_0: 1.2167 (1.2674)  loss_bbox_0: 0.7203 (0.7819)  loss_giou_0: 1.9356 (1.9076)  loss_ce_1: 1.2590 (1.2763)  loss_bbox_1: 0.7466 (0.7364)  loss_giou_1: 1.7711 (1.8393)  loss_ce_2: 1.2653 (1.2650)  loss_bbox_2: 0.7297 (0.7767)  loss_giou_2: 1.8104 (1.8811)  loss_ce_3: 1.2654 (1.2604)  loss_bbox_3: 0.8072 (0.7995)  loss_giou_3: 1.9548 (1.9203)  loss_ce_4: 1.2489 (1.2737)  loss_bbox_4: 0.6997 (0.7492)  loss_giou_4: 1.7854 (1.8613)  loss_ce_unscaled: 1.2538 (1.2686)  class_error_unscaled: 53.5714 (43.2291)  loss_bbox_unscaled: 0.1564 (0.1552)  loss_giou_unscaled: 0.9431 (0.9447)  cardinality_error_unscaled: 55.0000 (61.4804)  loss_ce_0_unscaled: 1.2167 (1.2674)  loss_bbox_0_unscaled: 0.1441 (0.1564)  loss_giou_0_unscaled: 0.9678 (0.9538)  cardinality_error_0_unscaled: 16.0000 (47.0784)  loss_ce_1_unscaled: 1.2590 (1.2763)  loss_bbox_1_unscaled: 0.1493 (0.1473)  loss_giou_1_unscaled: 0.8855 (0.9197)  cardinality_error_1_unscaled: 34.5000 (50.8235)  loss_ce_2_unscaled: 1.2653 (1.2650)  loss_bbox_2_unscaled: 0.1459 (0.1553)  loss_giou_2_unscaled: 0.9052 (0.9405)  cardinality_error_2_unscaled: 45.0000 (47.5882)  loss_ce_3_unscaled: 1.2654 (1.2604)  loss_bbox_3_unscaled: 0.1614 (0.1599)  loss_giou_3_unscaled: 0.9774 (0.9601)  cardinality_error_3_unscaled: 48.0000 (56.8137)  loss_ce_4_unscaled: 1.2489 (1.2737)  loss_bbox_4_unscaled: 0.1399 (0.1498)  loss_giou_4_unscaled: 0.8927 (0.9306)  cardinality_error_4_unscaled: 41.5000 (52.8431)  time: 0.2864  data: 0.0117  max mem: 2666\n",
            "Epoch: [4]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 62.22  loss: 25.3592 (23.8648)  loss_ce: 1.2271 (1.2841)  loss_bbox: 0.8176 (0.7849)  loss_giou: 1.9347 (1.8997)  loss_ce_0: 1.2167 (1.2812)  loss_bbox_0: 0.6706 (0.7801)  loss_giou_0: 1.8976 (1.9011)  loss_ce_1: 1.2284 (1.2911)  loss_bbox_1: 0.7591 (0.7486)  loss_giou_1: 1.8355 (1.8560)  loss_ce_2: 1.2391 (1.2748)  loss_bbox_2: 0.8509 (0.8601)  loss_giou_2: 2.0833 (1.9720)  loss_ce_3: 1.1859 (1.2732)  loss_bbox_3: 0.7860 (0.8103)  loss_giou_3: 2.0265 (1.9410)  loss_ce_4: 1.2219 (1.2827)  loss_bbox_4: 0.6948 (0.7516)  loss_giou_4: 1.9150 (1.8723)  loss_ce_unscaled: 1.2271 (1.2841)  class_error_unscaled: 62.2222 (46.1762)  loss_bbox_unscaled: 0.1635 (0.1570)  loss_giou_unscaled: 0.9673 (0.9499)  cardinality_error_unscaled: 47.0000 (57.4180)  loss_ce_0_unscaled: 1.2167 (1.2812)  loss_bbox_0_unscaled: 0.1341 (0.1560)  loss_giou_0_unscaled: 0.9488 (0.9505)  cardinality_error_0_unscaled: 18.5000 (43.0738)  loss_ce_1_unscaled: 1.2284 (1.2911)  loss_bbox_1_unscaled: 0.1518 (0.1497)  loss_giou_1_unscaled: 0.9177 (0.9280)  cardinality_error_1_unscaled: 35.5000 (47.2459)  loss_ce_2_unscaled: 1.2391 (1.2748)  loss_bbox_2_unscaled: 0.1702 (0.1720)  loss_giou_2_unscaled: 1.0416 (0.9860)  cardinality_error_2_unscaled: 16.0000 (41.0984)  loss_ce_3_unscaled: 1.1859 (1.2732)  loss_bbox_3_unscaled: 0.1572 (0.1621)  loss_giou_3_unscaled: 1.0132 (0.9705)  cardinality_error_3_unscaled: 31.0000 (50.7623)  loss_ce_4_unscaled: 1.2219 (1.2827)  loss_bbox_4_unscaled: 0.1390 (0.1503)  loss_giou_4_unscaled: 0.9575 (0.9362)  cardinality_error_4_unscaled: 39.0000 (49.7131)  time: 0.2872  data: 0.0121  max mem: 2666\n",
            "Epoch: [4]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 56.25  loss: 24.6517 (24.0825)  loss_ce: 1.3512 (1.2906)  loss_bbox: 0.8179 (0.7950)  loss_giou: 1.9421 (1.9232)  loss_ce_0: 1.2927 (1.2845)  loss_bbox_0: 0.7797 (0.7970)  loss_giou_0: 1.9349 (1.9297)  loss_ce_1: 1.3188 (1.2968)  loss_bbox_1: 0.7591 (0.7657)  loss_giou_1: 1.9136 (1.8920)  loss_ce_2: 1.3223 (1.2825)  loss_bbox_2: 0.9082 (0.8553)  loss_giou_2: 2.0828 (1.9839)  loss_ce_3: 1.3131 (1.2781)  loss_bbox_3: 0.7477 (0.8130)  loss_giou_3: 2.0265 (1.9529)  loss_ce_4: 1.3211 (1.2878)  loss_bbox_4: 0.7528 (0.7598)  loss_giou_4: 1.9905 (1.8947)  loss_ce_unscaled: 1.3512 (1.2906)  class_error_unscaled: 62.0690 (49.1572)  loss_bbox_unscaled: 0.1636 (0.1590)  loss_giou_unscaled: 0.9711 (0.9616)  cardinality_error_unscaled: 37.5000 (55.3732)  loss_ce_0_unscaled: 1.2927 (1.2845)  loss_bbox_0_unscaled: 0.1559 (0.1594)  loss_giou_0_unscaled: 0.9675 (0.9649)  cardinality_error_0_unscaled: 28.5000 (41.8944)  loss_ce_1_unscaled: 1.3188 (1.2968)  loss_bbox_1_unscaled: 0.1518 (0.1531)  loss_giou_1_unscaled: 0.9568 (0.9460)  cardinality_error_1_unscaled: 35.5000 (46.1549)  loss_ce_2_unscaled: 1.3223 (1.2825)  loss_bbox_2_unscaled: 0.1816 (0.1711)  loss_giou_2_unscaled: 1.0414 (0.9919)  cardinality_error_2_unscaled: 14.5000 (42.1901)  loss_ce_3_unscaled: 1.3131 (1.2781)  loss_bbox_3_unscaled: 0.1495 (0.1626)  loss_giou_3_unscaled: 1.0132 (0.9764)  cardinality_error_3_unscaled: 23.5000 (49.6690)  loss_ce_4_unscaled: 1.3211 (1.2878)  loss_bbox_4_unscaled: 0.1506 (0.1520)  loss_giou_4_unscaled: 0.9953 (0.9474)  cardinality_error_4_unscaled: 38.5000 (48.1831)  time: 0.2758  data: 0.0119  max mem: 2666\n",
            "Epoch: [4]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 66.67  loss: 24.0401 (24.1983)  loss_ce: 1.3512 (1.2958)  loss_bbox: 0.8000 (0.8035)  loss_giou: 1.9265 (1.9245)  loss_ce_0: 1.3114 (1.2918)  loss_bbox_0: 0.8272 (0.8058)  loss_giou_0: 1.9740 (1.9263)  loss_ce_1: 1.3430 (1.3072)  loss_bbox_1: 0.7458 (0.7755)  loss_giou_1: 1.8716 (1.8915)  loss_ce_2: 1.3748 (1.2921)  loss_bbox_2: 0.7526 (0.8552)  loss_giou_2: 1.8799 (1.9769)  loss_ce_3: 1.3415 (1.2883)  loss_bbox_3: 0.7686 (0.8220)  loss_giou_3: 1.8922 (1.9508)  loss_ce_4: 1.3415 (1.2953)  loss_bbox_4: 0.8067 (0.7824)  loss_giou_4: 1.9898 (1.9136)  loss_ce_unscaled: 1.3512 (1.2958)  class_error_unscaled: 69.2308 (52.1066)  loss_bbox_unscaled: 0.1600 (0.1607)  loss_giou_unscaled: 0.9632 (0.9622)  cardinality_error_unscaled: 36.5000 (53.0000)  loss_ce_0_unscaled: 1.3114 (1.2918)  loss_bbox_0_unscaled: 0.1654 (0.1612)  loss_giou_0_unscaled: 0.9870 (0.9631)  cardinality_error_0_unscaled: 41.0000 (41.7778)  loss_ce_1_unscaled: 1.3430 (1.3072)  loss_bbox_1_unscaled: 0.1492 (0.1551)  loss_giou_1_unscaled: 0.9358 (0.9458)  cardinality_error_1_unscaled: 31.5000 (43.6543)  loss_ce_2_unscaled: 1.3748 (1.2921)  loss_bbox_2_unscaled: 0.1505 (0.1710)  loss_giou_2_unscaled: 0.9400 (0.9884)  cardinality_error_2_unscaled: 38.0000 (41.0988)  loss_ce_3_unscaled: 1.3415 (1.2883)  loss_bbox_3_unscaled: 0.1537 (0.1644)  loss_giou_3_unscaled: 0.9461 (0.9754)  cardinality_error_3_unscaled: 39.0000 (48.2284)  loss_ce_4_unscaled: 1.3415 (1.2953)  loss_bbox_4_unscaled: 0.1613 (0.1565)  loss_giou_4_unscaled: 0.9949 (0.9568)  cardinality_error_4_unscaled: 33.0000 (45.8333)  time: 0.2765  data: 0.0115  max mem: 2666\n",
            "Epoch: [4]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 51.92  loss: 23.8491 (24.1741)  loss_ce: 1.3391 (1.3026)  loss_bbox: 0.7174 (0.7971)  loss_giou: 1.8486 (1.9262)  loss_ce_0: 1.3505 (1.3014)  loss_bbox_0: 0.6963 (0.7952)  loss_giou_0: 1.7592 (1.9161)  loss_ce_1: 1.3666 (1.3148)  loss_bbox_1: 0.6758 (0.7691)  loss_giou_1: 1.7925 (1.8889)  loss_ce_2: 1.4145 (1.3076)  loss_bbox_2: 0.7395 (0.8428)  loss_giou_2: 1.8341 (1.9727)  loss_ce_3: 1.3471 (1.2990)  loss_bbox_3: 0.7553 (0.8145)  loss_giou_3: 1.8520 (1.9442)  loss_ce_4: 1.3415 (1.3018)  loss_bbox_4: 0.7915 (0.7758)  loss_giou_4: 1.9511 (1.9042)  loss_ce_unscaled: 1.3391 (1.3026)  class_error_unscaled: 67.4419 (53.1620)  loss_bbox_unscaled: 0.1435 (0.1594)  loss_giou_unscaled: 0.9243 (0.9631)  cardinality_error_unscaled: 41.5000 (53.3132)  loss_ce_0_unscaled: 1.3505 (1.3014)  loss_bbox_0_unscaled: 0.1393 (0.1590)  loss_giou_0_unscaled: 0.8796 (0.9581)  cardinality_error_0_unscaled: 43.5000 (43.4725)  loss_ce_1_unscaled: 1.3666 (1.3148)  loss_bbox_1_unscaled: 0.1352 (0.1538)  loss_giou_1_unscaled: 0.8963 (0.9445)  cardinality_error_1_unscaled: 31.5000 (44.4780)  loss_ce_2_unscaled: 1.4145 (1.3076)  loss_bbox_2_unscaled: 0.1479 (0.1686)  loss_giou_2_unscaled: 0.9170 (0.9864)  cardinality_error_2_unscaled: 45.5000 (43.3462)  loss_ce_3_unscaled: 1.3471 (1.2990)  loss_bbox_3_unscaled: 0.1511 (0.1629)  loss_giou_3_unscaled: 0.9260 (0.9721)  cardinality_error_3_unscaled: 43.0000 (49.5769)  loss_ce_4_unscaled: 1.3415 (1.3018)  loss_bbox_4_unscaled: 0.1583 (0.1552)  loss_giou_4_unscaled: 0.9756 (0.9521)  cardinality_error_4_unscaled: 33.0000 (45.1813)  time: 0.2757  data: 0.0113  max mem: 2666\n",
            "Epoch: [4]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 26.09  loss: 23.8906 (24.2730)  loss_ce: 1.2680 (1.2875)  loss_bbox: 0.9547 (0.8475)  loss_giou: 2.1606 (1.9730)  loss_ce_0: 1.3415 (1.2878)  loss_bbox_0: 0.7154 (0.8056)  loss_giou_0: 1.8610 (1.9241)  loss_ce_1: 1.3176 (1.3030)  loss_bbox_1: 0.7108 (0.7769)  loss_giou_1: 1.8912 (1.8986)  loss_ce_2: 1.3530 (1.2968)  loss_bbox_2: 0.6868 (0.8434)  loss_giou_2: 1.9053 (1.9706)  loss_ce_3: 1.3177 (1.2882)  loss_bbox_3: 0.7470 (0.8210)  loss_giou_3: 1.9532 (1.9434)  loss_ce_4: 1.2814 (1.2882)  loss_bbox_4: 0.7940 (0.7967)  loss_giou_4: 1.9511 (1.9208)  loss_ce_unscaled: 1.2680 (1.2875)  class_error_unscaled: 50.0000 (50.9289)  loss_bbox_unscaled: 0.1909 (0.1695)  loss_giou_unscaled: 1.0803 (0.9865)  cardinality_error_unscaled: 59.5000 (54.4600)  loss_ce_0_unscaled: 1.3415 (1.2878)  loss_bbox_0_unscaled: 0.1431 (0.1611)  loss_giou_0_unscaled: 0.9305 (0.9620)  cardinality_error_0_unscaled: 67.0000 (45.8550)  loss_ce_1_unscaled: 1.3176 (1.3030)  loss_bbox_1_unscaled: 0.1422 (0.1554)  loss_giou_1_unscaled: 0.9456 (0.9493)  cardinality_error_1_unscaled: 58.0000 (46.2950)  loss_ce_2_unscaled: 1.3530 (1.2968)  loss_bbox_2_unscaled: 0.1374 (0.1687)  loss_giou_2_unscaled: 0.9527 (0.9853)  cardinality_error_2_unscaled: 57.5000 (44.9900)  loss_ce_3_unscaled: 1.3177 (1.2882)  loss_bbox_3_unscaled: 0.1494 (0.1642)  loss_giou_3_unscaled: 0.9766 (0.9717)  cardinality_error_3_unscaled: 65.5000 (51.3100)  loss_ce_4_unscaled: 1.2814 (1.2882)  loss_bbox_4_unscaled: 0.1588 (0.1593)  loss_giou_4_unscaled: 0.9756 (0.9604)  cardinality_error_4_unscaled: 42.5000 (45.6550)  time: 0.2728  data: 0.0108  max mem: 2666\n",
            "Epoch: [4] Total time: 0:00:28 (0.2818 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 26.09  loss: 23.8906 (24.2730)  loss_ce: 1.2680 (1.2875)  loss_bbox: 0.9547 (0.8475)  loss_giou: 2.1606 (1.9730)  loss_ce_0: 1.3415 (1.2878)  loss_bbox_0: 0.7154 (0.8056)  loss_giou_0: 1.8610 (1.9241)  loss_ce_1: 1.3176 (1.3030)  loss_bbox_1: 0.7108 (0.7769)  loss_giou_1: 1.8912 (1.8986)  loss_ce_2: 1.3530 (1.2968)  loss_bbox_2: 0.6868 (0.8434)  loss_giou_2: 1.9053 (1.9706)  loss_ce_3: 1.3177 (1.2882)  loss_bbox_3: 0.7470 (0.8210)  loss_giou_3: 1.9532 (1.9434)  loss_ce_4: 1.2814 (1.2882)  loss_bbox_4: 0.7940 (0.7967)  loss_giou_4: 1.9511 (1.9208)  loss_ce_unscaled: 1.2680 (1.2875)  class_error_unscaled: 50.0000 (50.9289)  loss_bbox_unscaled: 0.1909 (0.1695)  loss_giou_unscaled: 1.0803 (0.9865)  cardinality_error_unscaled: 59.5000 (54.4600)  loss_ce_0_unscaled: 1.3415 (1.2878)  loss_bbox_0_unscaled: 0.1431 (0.1611)  loss_giou_0_unscaled: 0.9305 (0.9620)  cardinality_error_0_unscaled: 67.0000 (45.8550)  loss_ce_1_unscaled: 1.3176 (1.3030)  loss_bbox_1_unscaled: 0.1422 (0.1554)  loss_giou_1_unscaled: 0.9456 (0.9493)  cardinality_error_1_unscaled: 58.0000 (46.2950)  loss_ce_2_unscaled: 1.3530 (1.2968)  loss_bbox_2_unscaled: 0.1374 (0.1687)  loss_giou_2_unscaled: 0.9527 (0.9853)  cardinality_error_2_unscaled: 57.5000 (44.9900)  loss_ce_3_unscaled: 1.3177 (1.2882)  loss_bbox_3_unscaled: 0.1494 (0.1642)  loss_giou_3_unscaled: 0.9766 (0.9717)  cardinality_error_3_unscaled: 65.5000 (51.3100)  loss_ce_4_unscaled: 1.2814 (1.2882)  loss_bbox_4_unscaled: 0.1588 (0.1593)  loss_giou_4_unscaled: 0.9756 (0.9604)  cardinality_error_4_unscaled: 42.5000 (45.6550)\n",
            "Test:  [ 0/15]  eta: 0:00:06  class_error: 86.36  loss: 42.3545 (42.3545)  loss_ce: 1.8219 (1.8219)  loss_bbox: 2.2964 (2.2964)  loss_giou: 3.2100 (3.2100)  loss_ce_0: 1.8096 (1.8096)  loss_bbox_0: 1.7373 (1.7373)  loss_giou_0: 3.5182 (3.5182)  loss_ce_1: 1.8299 (1.8299)  loss_bbox_1: 1.5664 (1.5664)  loss_giou_1: 3.3436 (3.3436)  loss_ce_2: 1.8180 (1.8180)  loss_bbox_2: 1.7823 (1.7823)  loss_giou_2: 3.5134 (3.5134)  loss_ce_3: 1.8253 (1.8253)  loss_bbox_3: 1.7735 (1.7735)  loss_giou_3: 3.4845 (3.4845)  loss_ce_4: 1.8012 (1.8012)  loss_bbox_4: 1.9530 (1.9530)  loss_giou_4: 3.2701 (3.2701)  loss_ce_unscaled: 1.8219 (1.8219)  class_error_unscaled: 86.3636 (86.3636)  loss_bbox_unscaled: 0.4593 (0.4593)  loss_giou_unscaled: 1.6050 (1.6050)  cardinality_error_unscaled: 78.0000 (78.0000)  loss_ce_0_unscaled: 1.8096 (1.8096)  loss_bbox_0_unscaled: 0.3475 (0.3475)  loss_giou_0_unscaled: 1.7591 (1.7591)  cardinality_error_0_unscaled: 78.0000 (78.0000)  loss_ce_1_unscaled: 1.8299 (1.8299)  loss_bbox_1_unscaled: 0.3133 (0.3133)  loss_giou_1_unscaled: 1.6718 (1.6718)  cardinality_error_1_unscaled: 78.0000 (78.0000)  loss_ce_2_unscaled: 1.8180 (1.8180)  loss_bbox_2_unscaled: 0.3565 (0.3565)  loss_giou_2_unscaled: 1.7567 (1.7567)  cardinality_error_2_unscaled: 78.0000 (78.0000)  loss_ce_3_unscaled: 1.8253 (1.8253)  loss_bbox_3_unscaled: 0.3547 (0.3547)  loss_giou_3_unscaled: 1.7423 (1.7423)  cardinality_error_3_unscaled: 78.0000 (78.0000)  loss_ce_4_unscaled: 1.8012 (1.8012)  loss_bbox_4_unscaled: 0.3906 (0.3906)  loss_giou_4_unscaled: 1.6350 (1.6350)  cardinality_error_4_unscaled: 78.0000 (78.0000)  time: 0.4071  data: 0.2238  max mem: 2666\n",
            "Test:  [10/15]  eta: 0:00:01  class_error: 25.71  loss: 40.6821 (41.0113)  loss_ce: 1.3555 (1.4284)  loss_bbox: 3.0543 (2.9813)  loss_giou: 3.4271 (3.4320)  loss_ce_0: 1.3929 (1.4367)  loss_bbox_0: 1.5823 (1.6252)  loss_giou_0: 3.3337 (3.3197)  loss_ce_1: 1.3698 (1.4345)  loss_bbox_1: 1.9179 (1.8371)  loss_giou_1: 3.3436 (3.3819)  loss_ce_2: 1.3765 (1.4340)  loss_bbox_2: 1.5954 (1.6336)  loss_giou_2: 3.2956 (3.2929)  loss_ce_3: 1.3949 (1.4362)  loss_bbox_3: 1.6115 (1.6426)  loss_giou_3: 3.2689 (3.2689)  loss_ce_4: 1.3732 (1.4248)  loss_bbox_4: 2.5657 (2.5587)  loss_giou_4: 3.4059 (3.4429)  loss_ce_unscaled: 1.3555 (1.4284)  class_error_unscaled: 32.6531 (45.1222)  loss_bbox_unscaled: 0.6109 (0.5963)  loss_giou_unscaled: 1.7135 (1.7160)  cardinality_error_unscaled: 82.5000 (81.2727)  loss_ce_0_unscaled: 1.3929 (1.4367)  loss_bbox_0_unscaled: 0.3165 (0.3250)  loss_giou_0_unscaled: 1.6668 (1.6598)  cardinality_error_0_unscaled: 82.5000 (81.2727)  loss_ce_1_unscaled: 1.3698 (1.4345)  loss_bbox_1_unscaled: 0.3836 (0.3674)  loss_giou_1_unscaled: 1.6718 (1.6910)  cardinality_error_1_unscaled: 82.5000 (81.2727)  loss_ce_2_unscaled: 1.3765 (1.4340)  loss_bbox_2_unscaled: 0.3191 (0.3267)  loss_giou_2_unscaled: 1.6478 (1.6465)  cardinality_error_2_unscaled: 82.5000 (81.2727)  loss_ce_3_unscaled: 1.3949 (1.4362)  loss_bbox_3_unscaled: 0.3223 (0.3285)  loss_giou_3_unscaled: 1.6345 (1.6344)  cardinality_error_3_unscaled: 82.5000 (81.2727)  loss_ce_4_unscaled: 1.3732 (1.4248)  loss_bbox_4_unscaled: 0.5131 (0.5117)  loss_giou_4_unscaled: 1.7030 (1.7215)  cardinality_error_4_unscaled: 82.5000 (81.2727)  time: 0.2093  data: 0.0302  max mem: 2666\n",
            "Test:  [14/15]  eta: 0:00:00  class_error: 72.86  loss: 40.8005 (40.8202)  loss_ce: 1.3555 (1.4401)  loss_bbox: 3.0143 (3.0009)  loss_giou: 3.4902 (3.4551)  loss_ce_0: 1.3929 (1.4441)  loss_bbox_0: 1.5424 (1.5562)  loss_giou_0: 3.3150 (3.2962)  loss_ce_1: 1.3698 (1.4450)  loss_bbox_1: 1.8916 (1.8059)  loss_giou_1: 3.4526 (3.3934)  loss_ce_2: 1.3765 (1.4422)  loss_bbox_2: 1.5263 (1.5621)  loss_giou_2: 3.2578 (3.2618)  loss_ce_3: 1.3949 (1.4446)  loss_bbox_3: 1.5537 (1.5728)  loss_giou_3: 3.2478 (3.2384)  loss_ce_4: 1.3732 (1.4365)  loss_bbox_4: 2.5046 (2.5640)  loss_giou_4: 3.4565 (3.4608)  loss_ce_unscaled: 1.3555 (1.4401)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.6029 (0.6002)  loss_giou_unscaled: 1.7451 (1.7276)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.3929 (1.4441)  loss_bbox_0_unscaled: 0.3085 (0.3112)  loss_giou_0_unscaled: 1.6575 (1.6481)  cardinality_error_0_unscaled: 82.5000 (79.6000)  loss_ce_1_unscaled: 1.3698 (1.4450)  loss_bbox_1_unscaled: 0.3783 (0.3612)  loss_giou_1_unscaled: 1.7263 (1.6967)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.3765 (1.4422)  loss_bbox_2_unscaled: 0.3053 (0.3124)  loss_giou_2_unscaled: 1.6289 (1.6309)  cardinality_error_2_unscaled: 82.5000 (79.6000)  loss_ce_3_unscaled: 1.3949 (1.4446)  loss_bbox_3_unscaled: 0.3107 (0.3146)  loss_giou_3_unscaled: 1.6239 (1.6192)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.3732 (1.4365)  loss_bbox_4_unscaled: 0.5009 (0.5128)  loss_giou_4_unscaled: 1.7282 (1.7304)  cardinality_error_4_unscaled: 82.5000 (79.6000)  time: 0.2037  data: 0.0248  max mem: 2666\n",
            "Test: Total time: 0:00:03 (0.2076 s / it)\n",
            "Averaged stats: class_error: 72.86  loss: 40.8005 (40.8202)  loss_ce: 1.3555 (1.4401)  loss_bbox: 3.0143 (3.0009)  loss_giou: 3.4902 (3.4551)  loss_ce_0: 1.3929 (1.4441)  loss_bbox_0: 1.5424 (1.5562)  loss_giou_0: 3.3150 (3.2962)  loss_ce_1: 1.3698 (1.4450)  loss_bbox_1: 1.8916 (1.8059)  loss_giou_1: 3.4526 (3.3934)  loss_ce_2: 1.3765 (1.4422)  loss_bbox_2: 1.5263 (1.5621)  loss_giou_2: 3.2578 (3.2618)  loss_ce_3: 1.3949 (1.4446)  loss_bbox_3: 1.5537 (1.5728)  loss_giou_3: 3.2478 (3.2384)  loss_ce_4: 1.3732 (1.4365)  loss_bbox_4: 2.5046 (2.5640)  loss_giou_4: 3.4565 (3.4608)  loss_ce_unscaled: 1.3555 (1.4401)  class_error_unscaled: 32.6531 (44.6545)  loss_bbox_unscaled: 0.6029 (0.6002)  loss_giou_unscaled: 1.7451 (1.7276)  cardinality_error_unscaled: 82.5000 (79.6000)  loss_ce_0_unscaled: 1.3929 (1.4441)  loss_bbox_0_unscaled: 0.3085 (0.3112)  loss_giou_0_unscaled: 1.6575 (1.6481)  cardinality_error_0_unscaled: 82.5000 (79.6000)  loss_ce_1_unscaled: 1.3698 (1.4450)  loss_bbox_1_unscaled: 0.3783 (0.3612)  loss_giou_1_unscaled: 1.7263 (1.6967)  cardinality_error_1_unscaled: 82.5000 (79.6000)  loss_ce_2_unscaled: 1.3765 (1.4422)  loss_bbox_2_unscaled: 0.3053 (0.3124)  loss_giou_2_unscaled: 1.6289 (1.6309)  cardinality_error_2_unscaled: 82.5000 (79.6000)  loss_ce_3_unscaled: 1.3949 (1.4446)  loss_bbox_3_unscaled: 0.3107 (0.3146)  loss_giou_3_unscaled: 1.6239 (1.6192)  cardinality_error_3_unscaled: 82.5000 (79.6000)  loss_ce_4_unscaled: 1.3732 (1.4365)  loss_bbox_4_unscaled: 0.5009 (0.5128)  loss_giou_4_unscaled: 1.7282 (1.7304)  cardinality_error_4_unscaled: 82.5000 (79.6000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Epoch: [5]  [  0/100]  eta: 0:00:54  lr: 0.000100  class_error: 24.00  loss: 26.0803 (26.0803)  loss_ce: 1.1722 (1.1722)  loss_bbox: 1.2311 (1.2311)  loss_giou: 2.4842 (2.4842)  loss_ce_0: 1.1605 (1.1605)  loss_bbox_0: 0.8745 (0.8745)  loss_giou_0: 2.2508 (2.2508)  loss_ce_1: 1.1640 (1.1640)  loss_bbox_1: 0.8616 (0.8616)  loss_giou_1: 2.1691 (2.1691)  loss_ce_2: 1.1670 (1.1670)  loss_bbox_2: 0.8755 (0.8755)  loss_giou_2: 2.1397 (2.1397)  loss_ce_3: 1.1969 (1.1969)  loss_bbox_3: 0.9143 (0.9143)  loss_giou_3: 2.2087 (2.2087)  loss_ce_4: 1.1538 (1.1538)  loss_bbox_4: 0.9744 (0.9744)  loss_giou_4: 2.0820 (2.0820)  loss_ce_unscaled: 1.1722 (1.1722)  class_error_unscaled: 24.0000 (24.0000)  loss_bbox_unscaled: 0.2462 (0.2462)  loss_giou_unscaled: 1.2421 (1.2421)  cardinality_error_unscaled: 78.5000 (78.5000)  loss_ce_0_unscaled: 1.1605 (1.1605)  loss_bbox_0_unscaled: 0.1749 (0.1749)  loss_giou_0_unscaled: 1.1254 (1.1254)  cardinality_error_0_unscaled: 79.5000 (79.5000)  loss_ce_1_unscaled: 1.1640 (1.1640)  loss_bbox_1_unscaled: 0.1723 (0.1723)  loss_giou_1_unscaled: 1.0846 (1.0846)  cardinality_error_1_unscaled: 84.5000 (84.5000)  loss_ce_2_unscaled: 1.1670 (1.1670)  loss_bbox_2_unscaled: 0.1751 (0.1751)  loss_giou_2_unscaled: 1.0699 (1.0699)  cardinality_error_2_unscaled: 73.0000 (73.0000)  loss_ce_3_unscaled: 1.1969 (1.1969)  loss_bbox_3_unscaled: 0.1829 (0.1829)  loss_giou_3_unscaled: 1.1043 (1.1043)  cardinality_error_3_unscaled: 84.0000 (84.0000)  loss_ce_4_unscaled: 1.1538 (1.1538)  loss_bbox_4_unscaled: 0.1949 (0.1949)  loss_giou_4_unscaled: 1.0410 (1.0410)  cardinality_error_4_unscaled: 70.5000 (70.5000)  time: 0.5403  data: 0.2189  max mem: 2666\n",
            "Epoch: [5]  [ 10/100]  eta: 0:00:28  lr: 0.000100  class_error: 56.86  loss: 24.8937 (24.9554)  loss_ce: 1.3278 (1.2886)  loss_bbox: 0.9084 (1.1149)  loss_giou: 1.9751 (2.1169)  loss_ce_0: 1.2659 (1.2991)  loss_bbox_0: 0.7474 (0.8354)  loss_giou_0: 1.7866 (1.8715)  loss_ce_1: 1.2445 (1.2894)  loss_bbox_1: 0.8616 (0.9364)  loss_giou_1: 2.0636 (2.0508)  loss_ce_2: 1.3086 (1.3143)  loss_bbox_2: 0.7586 (0.8618)  loss_giou_2: 1.6773 (1.8446)  loss_ce_3: 1.3303 (1.3189)  loss_bbox_3: 0.7739 (0.8754)  loss_giou_3: 1.7079 (1.8627)  loss_ce_4: 1.3324 (1.3037)  loss_bbox_4: 0.7795 (0.8954)  loss_giou_4: 1.8508 (1.8758)  loss_ce_unscaled: 1.3278 (1.2886)  class_error_unscaled: 38.0952 (39.3785)  loss_bbox_unscaled: 0.1817 (0.2230)  loss_giou_unscaled: 0.9875 (1.0584)  cardinality_error_unscaled: 70.0000 (68.6818)  loss_ce_0_unscaled: 1.2659 (1.2991)  loss_bbox_0_unscaled: 0.1495 (0.1671)  loss_giou_0_unscaled: 0.8933 (0.9357)  cardinality_error_0_unscaled: 62.5000 (60.3182)  loss_ce_1_unscaled: 1.2445 (1.2894)  loss_bbox_1_unscaled: 0.1723 (0.1873)  loss_giou_1_unscaled: 1.0318 (1.0254)  cardinality_error_1_unscaled: 65.0000 (62.8182)  loss_ce_2_unscaled: 1.3086 (1.3143)  loss_bbox_2_unscaled: 0.1517 (0.1724)  loss_giou_2_unscaled: 0.8386 (0.9223)  cardinality_error_2_unscaled: 68.0000 (69.0455)  loss_ce_3_unscaled: 1.3303 (1.3189)  loss_bbox_3_unscaled: 0.1548 (0.1751)  loss_giou_3_unscaled: 0.8539 (0.9313)  cardinality_error_3_unscaled: 77.0000 (77.2727)  loss_ce_4_unscaled: 1.3324 (1.3037)  loss_bbox_4_unscaled: 0.1559 (0.1791)  loss_giou_4_unscaled: 0.9254 (0.9379)  cardinality_error_4_unscaled: 67.5000 (65.9545)  time: 0.3118  data: 0.0298  max mem: 2666\n",
            "Epoch: [5]  [ 20/100]  eta: 0:00:23  lr: 0.000100  class_error: 28.00  loss: 21.7063 (22.9589)  loss_ce: 1.1996 (1.2088)  loss_bbox: 0.6778 (0.8658)  loss_giou: 1.7849 (1.9057)  loss_ce_0: 1.1598 (1.2181)  loss_bbox_0: 0.6454 (0.7492)  loss_giou_0: 1.7108 (1.7928)  loss_ce_1: 1.1703 (1.2078)  loss_bbox_1: 0.6841 (0.7972)  loss_giou_1: 1.7495 (1.8852)  loss_ce_2: 1.1753 (1.2292)  loss_bbox_2: 0.6310 (0.7474)  loss_giou_2: 1.6978 (1.8058)  loss_ce_3: 1.2092 (1.2255)  loss_bbox_3: 0.6333 (0.7473)  loss_giou_3: 1.6998 (1.8031)  loss_ce_4: 1.1980 (1.2129)  loss_bbox_4: 0.5932 (0.7476)  loss_giou_4: 1.7585 (1.8096)  loss_ce_unscaled: 1.1996 (1.2088)  class_error_unscaled: 48.2759 (43.8816)  loss_bbox_unscaled: 0.1356 (0.1732)  loss_giou_unscaled: 0.8924 (0.9529)  cardinality_error_unscaled: 58.0000 (54.7857)  loss_ce_0_unscaled: 1.1598 (1.2181)  loss_bbox_0_unscaled: 0.1291 (0.1498)  loss_giou_0_unscaled: 0.8554 (0.8964)  cardinality_error_0_unscaled: 49.0000 (52.5000)  loss_ce_1_unscaled: 1.1703 (1.2078)  loss_bbox_1_unscaled: 0.1368 (0.1594)  loss_giou_1_unscaled: 0.8748 (0.9426)  cardinality_error_1_unscaled: 56.5000 (57.8333)  loss_ce_2_unscaled: 1.1753 (1.2292)  loss_bbox_2_unscaled: 0.1262 (0.1495)  loss_giou_2_unscaled: 0.8489 (0.9029)  cardinality_error_2_unscaled: 63.0000 (58.4048)  loss_ce_3_unscaled: 1.2092 (1.2255)  loss_bbox_3_unscaled: 0.1267 (0.1495)  loss_giou_3_unscaled: 0.8499 (0.9015)  cardinality_error_3_unscaled: 66.5000 (60.1905)  loss_ce_4_unscaled: 1.1980 (1.2129)  loss_bbox_4_unscaled: 0.1186 (0.1495)  loss_giou_4_unscaled: 0.8793 (0.9048)  cardinality_error_4_unscaled: 47.0000 (48.9286)  time: 0.2784  data: 0.0113  max mem: 2666\n",
            "Epoch: [5]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 46.00  loss: 22.1943 (23.5729)  loss_ce: 1.1855 (1.2734)  loss_bbox: 0.6270 (0.8094)  loss_giou: 1.7291 (1.8892)  loss_ce_0: 1.1458 (1.2734)  loss_bbox_0: 0.6454 (0.7355)  loss_giou_0: 1.7108 (1.8197)  loss_ce_1: 1.1703 (1.2719)  loss_bbox_1: 0.6309 (0.7679)  loss_giou_1: 1.6861 (1.8730)  loss_ce_2: 1.1647 (1.2770)  loss_bbox_2: 0.6310 (0.7421)  loss_giou_2: 1.7759 (1.8472)  loss_ce_3: 1.1670 (1.2745)  loss_bbox_3: 0.6952 (0.7903)  loss_giou_3: 1.8216 (1.9186)  loss_ce_4: 1.1692 (1.2616)  loss_bbox_4: 0.7163 (0.8147)  loss_giou_4: 1.8412 (1.9337)  loss_ce_unscaled: 1.1855 (1.2734)  class_error_unscaled: 48.2759 (47.6706)  loss_bbox_unscaled: 0.1254 (0.1619)  loss_giou_unscaled: 0.8645 (0.9446)  cardinality_error_unscaled: 49.0000 (54.6935)  loss_ce_0_unscaled: 1.1458 (1.2734)  loss_bbox_0_unscaled: 0.1291 (0.1471)  loss_giou_0_unscaled: 0.8554 (0.9098)  cardinality_error_0_unscaled: 46.5000 (53.4355)  loss_ce_1_unscaled: 1.1703 (1.2719)  loss_bbox_1_unscaled: 0.1262 (0.1536)  loss_giou_1_unscaled: 0.8431 (0.9365)  cardinality_error_1_unscaled: 60.0000 (60.5484)  loss_ce_2_unscaled: 1.1647 (1.2770)  loss_bbox_2_unscaled: 0.1262 (0.1484)  loss_giou_2_unscaled: 0.8879 (0.9236)  cardinality_error_2_unscaled: 58.5000 (59.4516)  loss_ce_3_unscaled: 1.1670 (1.2745)  loss_bbox_3_unscaled: 0.1390 (0.1581)  loss_giou_3_unscaled: 0.9108 (0.9593)  cardinality_error_3_unscaled: 65.5000 (62.5323)  loss_ce_4_unscaled: 1.1692 (1.2616)  loss_bbox_4_unscaled: 0.1433 (0.1629)  loss_giou_4_unscaled: 0.9206 (0.9668)  cardinality_error_4_unscaled: 30.0000 (45.2581)  time: 0.2677  data: 0.0114  max mem: 2666\n",
            "Epoch: [5]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 70.27  loss: 23.6867 (23.9184)  loss_ce: 1.2595 (1.2862)  loss_bbox: 0.8080 (0.8345)  loss_giou: 1.8871 (1.8989)  loss_ce_0: 1.2405 (1.2727)  loss_bbox_0: 0.8104 (0.7769)  loss_giou_0: 1.9455 (1.8467)  loss_ce_1: 1.2570 (1.2777)  loss_bbox_1: 0.8366 (0.8079)  loss_giou_1: 1.9230 (1.8901)  loss_ce_2: 1.2377 (1.2741)  loss_bbox_2: 0.8500 (0.7916)  loss_giou_2: 1.9541 (1.8820)  loss_ce_3: 1.2091 (1.2726)  loss_bbox_3: 0.8908 (0.8244)  loss_giou_3: 1.9710 (1.9262)  loss_ce_4: 1.2105 (1.2621)  loss_bbox_4: 0.8660 (0.8395)  loss_giou_4: 2.0202 (1.9543)  loss_ce_unscaled: 1.2595 (1.2862)  class_error_unscaled: 50.0000 (49.7920)  loss_bbox_unscaled: 0.1616 (0.1669)  loss_giou_unscaled: 0.9436 (0.9494)  cardinality_error_unscaled: 58.0000 (55.8537)  loss_ce_0_unscaled: 1.2405 (1.2727)  loss_bbox_0_unscaled: 0.1621 (0.1554)  loss_giou_0_unscaled: 0.9727 (0.9234)  cardinality_error_0_unscaled: 46.5000 (48.8415)  loss_ce_1_unscaled: 1.2570 (1.2777)  loss_bbox_1_unscaled: 0.1673 (0.1616)  loss_giou_1_unscaled: 0.9615 (0.9451)  cardinality_error_1_unscaled: 60.5000 (59.1341)  loss_ce_2_unscaled: 1.2377 (1.2741)  loss_bbox_2_unscaled: 0.1700 (0.1583)  loss_giou_2_unscaled: 0.9771 (0.9410)  cardinality_error_2_unscaled: 56.5000 (55.0488)  loss_ce_3_unscaled: 1.2091 (1.2726)  loss_bbox_3_unscaled: 0.1782 (0.1649)  loss_giou_3_unscaled: 0.9855 (0.9631)  cardinality_error_3_unscaled: 67.5000 (61.8171)  loss_ce_4_unscaled: 1.2105 (1.2621)  loss_bbox_4_unscaled: 0.1732 (0.1679)  loss_giou_4_unscaled: 1.0101 (0.9771)  cardinality_error_4_unscaled: 39.5000 (44.9268)  time: 0.2686  data: 0.0111  max mem: 2666\n",
            "Epoch: [5]  [ 50/100]  eta: 0:00:14  lr: 0.000100  class_error: 64.71  loss: 23.5846 (23.7683)  loss_ce: 1.2496 (1.2885)  loss_bbox: 0.8390 (0.8328)  loss_giou: 1.7253 (1.8716)  loss_ce_0: 1.1659 (1.2692)  loss_bbox_0: 0.8536 (0.7828)  loss_giou_0: 1.8557 (1.8266)  loss_ce_1: 1.2095 (1.2774)  loss_bbox_1: 0.8927 (0.8113)  loss_giou_1: 1.8584 (1.8536)  loss_ce_2: 1.1669 (1.2686)  loss_bbox_2: 0.8792 (0.7975)  loss_giou_2: 1.8713 (1.8545)  loss_ce_3: 1.1712 (1.2725)  loss_bbox_3: 0.8615 (0.8289)  loss_giou_3: 1.7778 (1.8912)  loss_ce_4: 1.2009 (1.2703)  loss_bbox_4: 0.8343 (0.8440)  loss_giou_4: 1.8630 (1.9269)  loss_ce_unscaled: 1.2496 (1.2885)  class_error_unscaled: 62.0690 (51.2736)  loss_bbox_unscaled: 0.1678 (0.1666)  loss_giou_unscaled: 0.8626 (0.9358)  cardinality_error_unscaled: 58.5000 (54.9216)  loss_ce_0_unscaled: 1.1659 (1.2692)  loss_bbox_0_unscaled: 0.1707 (0.1566)  loss_giou_0_unscaled: 0.9278 (0.9133)  cardinality_error_0_unscaled: 26.5000 (43.7549)  loss_ce_1_unscaled: 1.2095 (1.2774)  loss_bbox_1_unscaled: 0.1785 (0.1623)  loss_giou_1_unscaled: 0.9292 (0.9268)  cardinality_error_1_unscaled: 47.5000 (55.8824)  loss_ce_2_unscaled: 1.1669 (1.2686)  loss_bbox_2_unscaled: 0.1758 (0.1595)  loss_giou_2_unscaled: 0.9357 (0.9273)  cardinality_error_2_unscaled: 24.0000 (48.5784)  loss_ce_3_unscaled: 1.1712 (1.2725)  loss_bbox_3_unscaled: 0.1723 (0.1658)  loss_giou_3_unscaled: 0.8889 (0.9456)  cardinality_error_3_unscaled: 62.0000 (59.6765)  loss_ce_4_unscaled: 1.2009 (1.2703)  loss_bbox_4_unscaled: 0.1669 (0.1688)  loss_giou_4_unscaled: 0.9315 (0.9634)  cardinality_error_4_unscaled: 50.5000 (46.5588)  time: 0.2871  data: 0.0121  max mem: 2666\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cba15418f2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-bd4c5f137ef8>\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    177\u001b[0m         train_stats = train_one_epoch(\n\u001b[1;32m    178\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             args.clip_max_norm)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-480e6558bf84>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}